### Lecture: 7. Recurrent Neural Networks
#### Date: Mar 28
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2122/slides/?07
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2122/slides.pdf/npfl114-07.pdf,PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-07-czech.mp4, CZ Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-07-english.mp4, EN Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-07-czech.practicals.mp4, CZ Practicals
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-07-english.practicals.mp4, EN Practicals
#### Questions: #lecture_7_questions
#### Lecture assignment: sequence_classification
#### Lecture assignment: tagger_we
#### Lecture assignment: tagger_cle
#### Lecture assignment: tagger_competition

- Sequence modelling using Recurrent Neural Networks (RNN) [Chapter 10 until Section 10.2.1 (excluding) of DLB]
- The challenge of long-term dependencies [Section 10.7 of DLB]
- Long Short-Term Memory (LSTM) [Section 10.10.1 of DLB, *[Sepp Hochreiter, Jürgen Schmidhuber (1997): Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf), [Felix A. Gers, Jürgen Schmidhuber, Fred Cummins (2000): Learning to Forget: Continual Prediction with LSTM](ftp://ftp.idsia.ch/pub/juergen/FgGates-NC.pdf)*]
- Gated Recurrent Unit (GRU) [Section 10.10.2 of DLB, *[Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)*]
- Highway Networks [[Training Very Deep Networks](https://arxiv.org/abs/1507.06228)]
- RNN Regularization
  - Variational Dropout [[A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)]
  - Layer Normalization [[Layer Normalization](https://arxiv.org/abs/1607.06450)]
- Bidirectional RNN [Section 10.3 of DLB]
- Word Embeddings [Section 14.2.4 of DLB]
- Character-level embeddings using Recurrent neural networks [C2W model from [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](http://arxiv.org/abs/1508.02096)]
- **Character-level embeddings using Convolutional neural networks [CharCNN from [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)]**
