### Lecture: 3. Training Neural Networks II
#### Date: Feb 27
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2223/slides/?03
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2223/slides.pdf/npfl114-2223-03.pdf, PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2223/npfl114-2223-03-czech.mp4, CZ Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2223/npfl114-2223-03-english.mp4, EN Lecture
#### Questions: #lecture_3_questions
#### Lecture assignment: mnist_regularization
#### Lecture assignment: mnist_ensemble
#### Lecture assignment: uppercase

- Softmax with NLL (negative log likelihood) as a loss function [Section 6.2.2.3 of DLB, notably equation (6.30); plus slides 10-12]
- Regularization [Chapter 7 until Section 7.1 of DLB]
  - Early stopping [Section 7.8 of DLB, without the _How early stopping acts as a regularizer_ part]
  - L2 and L1 regularization [Sections 7.1 and 5.6.1 of DLB; plus slides 17-18]
  - Dataset augmentation [Section 7.4 of DLB]
  - Ensembling [Section 7.11 of DLB]
  - Dropout [Section 7.12 of DLB]
  - Label smoothing [Section 7.5.1 of DLB]
- Saturating non-linearities [Section 6.3.2 and second half of Section 6.2.2.2 of DLB]
- Parameter initialization strategies [Section 8.4 of DLB]
- Gradient clipping [Section 10.11.1 of DLB]
