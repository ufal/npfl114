### Lecture: 3. Training Neural Networks II
#### Date: Mar 9
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/1920/slides/?03
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/1920/slides.pdf/npfl114-03.pdf,PDF Slides
#### Video: https://slideslive.com/38906435/deep-learning-lecture-3-training-neural-networks-ii,2018 Video
#### Questions: #lecture_3_questions
#### Lecture assignment: explore_examples
#### Lecture assignment: mnist_regularization
#### Lecture assignment: mnist_ensemble
#### Lecture assignment: uppercase

- *Training neural network with a single hidden layer*
- Softmax with NLL (negative log likelihood) as a loss function [Section 6.2.2.3 of DLB, notably equation (6.30); plus slides 10-12]
- Regularization [Chapter 7 until Section 7.1 of DLB]
  - Early stopping [Section 7.8 of DLB, without the *How early stopping acts as a regularizer* part]
  - L2 and L1 regularization [Sections 7.1 and 5.6.1 of DLB; plus slides 17-18]
  - Dataset augmentation [Section 7.4 of DLB]
  - Ensembling [Section 7.11 of DLB]
  - Dropout [Section 7.12 of DLB]
  - Label smoothing [Section 7.5.1 of DLB]
- Saturating non-linearities [Section 6.3.2 and second half of Section 6.2.2.2 of DLB]
- Parameter initialization strategies [Section 8.4 of DLB]
- Gradient clipping [Section 10.11.1 of DLB]
