### Lecture: 10. Seq2seq, NMT, Transformer
#### Date: Apr 17
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2223/slides/?10
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2223/slides.pdf/npfl114-2223-10.pdf, PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2223/npfl114-2223-10-czech.mp4, CZ Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2223/npfl114-2223-10-english.mp4, EN Lecture
#### Questions: #lecture_10_questions
#### Lecture assignment: lemmatizer_noattn
#### Lecture assignment: lemmatizer_attn
#### Lecture assignment: lemmatizer_competition

- Neural Machine Translation using Encoder-Decoder or Sequence-to-Sequence architecture [Section 12.5.4 of DLB, [Ilya Sutskever, Oriol Vinyals, Quoc V. Le: Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) and [Kyunghyun Cho et al.: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)]
- Using Attention mechanism in Neural Machine Translation [Section 12.4.5.1 of DLB, [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)]
- Translating Subword Units [[Rico Sennrich, Barry Haddow, Alexandra Birch: Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)]
- _Google NMT [[Yonghui Wu et al.: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)]_
- Transformer architecture [[Attention Is All You Need](https://arxiv.org/abs/1706.03762)]
