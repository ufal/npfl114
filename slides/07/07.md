title: NPFL114, Lecture 7
class: title, langtech, cc-by-nc-sa

# Recurrent Neural Networks

## Milan Straka

### April 14, 2020

---
section: RNN
class: middle, center
# Recurrent Neural Networks

# Recurrent Neural Networks

---
# Recurrent Neural Networks

## Single RNN cell

![w=17%,h=center](rnn_cell.pdf)

~~~

## Unrolled RNN cells

![w=60%,h=center](rnn_cell_unrolled.pdf)

---
# Basic RNN Cell

![w=50%,h=center,mh=55%](rnn_cell_basic.pdf)

~~~
Given an input $→x^{(t)}$ and previous state $→s^{(t-1)}$, the new state is computed as
$$→s^{(t)} = f(→s^{(t-1)}, →x^{(t)}; →θ).$$

~~~
One of the simplest possibilities (called `SimpleRNN` in TensorFlow) is
$$→s^{(t)} = \tanh(⇉U→s^{(t-1)} + ⇉V→x^{(t)} + →b).$$

---
# Basic RNN Cell

Basic RNN cells suffer a lot from vanishing/exploding gradients (_the challenge
of long-term dependencies_).

~~~

If we simplify the recurrence of states to
$$→s^{(t)} = ⇉U→s^{(t-1)},$$
we get
$$→s^{(t)} = ⇉U^t→s^{(0)}.$$

~~~

If $U$ has eigenvalue decomposition of $⇉U = ⇉Q ⇉Λ ⇉Q^{-1}$, we get
$$→s^{(t)} = ⇉Q ⇉Λ^t ⇉Q^{-1} →s^{(0)}.$$
The main problem is that the _same_ function is iteratively applied many times.

~~~
Several more complex RNN cell variants have been proposed, which alleviate
this issue to some degree, namely **LSTM** and **GRU**.

---
section: LSTM
# Long Short-Term Memory

Hochreiter & Schmidhuber (1997) suggested that to enforce
_constant error flow_, we would like
$$f' = →1.$$

~~~
They propose to achieve that by a _constant error carrousel_.

![w=60%,h=center](lstm_cec_idea.pdf)

~~~ ~~
They propose to achieve that by a _constant error carrousel_.

![w=60%,h=center](lstm_cec.pdf)

---
# Long Short-Term Memory

They also propose an _input_ and _output_ gates which control the flow
of information into and out of the carrousel (_memory cell_ $→c_t$).

![w=40%,f=right](lstm_input_output_gates.pdf)

$$\begin{aligned}
  →i_t & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  →o_t & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  →c_t & ← →c_{t-1} + →i_t \cdot \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  →h_t & ← →o_t \cdot \tanh(→c_t)
\end{aligned}$$

---
# Long Short-Term Memory

Later in Gers, Schmidhuber & Cummins (1999) a possibility to _forget_
information from memory cell $→c_t$ was added.

![w=40%,f=right](lstm_input_output_forget_gates.pdf)

$$\begin{aligned}
  →i_t & ← σ(⇉W^i →x_t + ⇉V^i →h_{t-1} + →b^i) \\
  →f_t & ← σ(⇉W^f →x_t + ⇉V^f →h_{t-1} + →b^f) \\
  →o_t & ← σ(⇉W^o →x_t + ⇉V^o →h_{t-1} + →b^o) \\
  →c_t & ← →f_t \cdot →c_{t-1} + →i_t \cdot \tanh(⇉W^y →x_t + ⇉V^y →h_{t-1} + →b^y) \\
  →h_t & ← →o_t \cdot \tanh(→c_t)
\end{aligned}$$

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-SimpleRNN.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-chain.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-C-line.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-i.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-f.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-C.png)

---
# Long Short-Term Memory
![w=100%,v=middle](LSTM3-focus-o.png)

---
section: GRU
# Gated Recurrent Unit

_Gated recurrent unit (GRU)_ was proposed by Cho et al. (2014) as
a simplification of LSTM. The main differences are

![w=45%,f=right](gru.pdf)

- no memory cell
- forgetting and updating tied together

~~~
$$\begin{aligned}
  →r_t & ← σ(⇉W^r →x_t + ⇉V^r →h_{t-1} + →b^r) \\
  →u_t & ← σ(⇉W^u →x_t + ⇉V^u →h_{t-1} + →b^u) \\
  →ĥ_t & ← \tanh(⇉W^h →x_t + ⇉V^h (→r_t \cdot →h_{t-1}) + →b^h) \\
  →h_t & ← →u_t \cdot →h_{t-1} + (1 - →u_t) \cdot →ĥ_t
\end{aligned}$$

---
# Gated Recurrent Unit
![w=100%,v=middle](LSTM3-var-GRU.png)

---
section: HighwayNetworks
class: middle, center
# Highway Networks

# Highway Networks

---
# Highway Networks

For input $→x$, fully connected layer computes
$$→y ← H(→x, ⇉W_H).$$

~~~
Highway networks add residual connection with gating:
$$→y ← H(→x, ⇉W_H) \cdot T(→x, ⇉W_T) + →x \cdot (1 - T(→x, ⇉W_T)).$$

~~~
Usually, the gating is defined as
$$T(→x, ⇉W_T) ← σ(⇉W_T →x + →b_T).$$

~~~
Note that the resulting update is very similar to a GRU cell with $→h_t$ removed; for a
fully connected layer $H(→x, ⇉W_H) = \tanh(⇉W_H →x + →b_H)$ it is exactly it.

---
# Highway Networks

![w=100%](highway_training.pdf)

---
# Highway Networks

![w=90%,h=center](highway_activations.jpg)

---
# Highway Networks

![w=95%,h=center](highway_leisoning.pdf)

---
section: RNNRegularization
# Regularizing RNNs

## Dropout

- Using dropout on hidden states interferes with long-term dependencies.

~~~

- However, using dropout on the inputs and outputs works well and is used
frequently.
~~~
    - In case residual connections are present, the output dropout needs to be
      applied before adding the residual connection.

~~~
- Several techniques were designed to allow using dropout on hidden states.
    - Variational Dropout
    - Recurrent Dropout
    - Zoneout

---
# Regularizing RNNs

## Variational Dropout

![w=75%,h=center](variational_rnn.pdf)

~~~
To implement variational dropout on inputs in TensorFlow, use `noise_shape` of
`tf.keras.layers.Dropout` to force the same mask across time-steps.
The variational dropout on the hidden states can be implemented using
`recurrent_dropout` argument of `tf.keras.layers.{LSTM,GRU,SimpleRNN}{,Cell}`.

---
# Regularizing RNNs

## Recurrent Dropout

Dropout only candidate states (i.e., values added to the memory cell in LSTM and previous state in GRU).

~~~
## Zoneout

Randomly preserve hidden activations instead of dropping them.

~~~
## Batch Normalization

![w=42%,f=right](recurrent_batch_normalization.pdf)

Very fragile and sensitive to proper initialization – there were papers with
negative results (_Dario Amodei et al, 2015: Deep Speech 2_ or _Cesar Laurent et al,
2016: Batch Normalized Recurrent Neural Networks_) until people managed to make
it work (_Tim Cooijmans et al, 2016: Recurrent Batch Normalization_;
specifically, initializing $γ=0.1$ did the trick).

---
# Regularizing RNNs

## Layer Normalization

Much more stable than batch normalization.

![w=100%](layer_norm.pdf)

---
# Layer Normalization

In an important recent architecture (namely Transformer), many fully
connected layers are used, with a residual connection and a layer normalization.

![w=45%,h=center](layer_norm_residual.pdf)

~~~
This could be considered an alternative to highway networks, i.e., a suitable
residual connection for fully connected layers.
~~~
Note the architecture can be considered as a variant of a mobile inverted
bottleneck $1×1$ convolution block.

---
section: RNNApplications
# Basic RNN Applications

## Sequence Element Representation

Create output for individual elements, for example for classification of the
individual elements.

![w=70%,h=center](rnn_cell_unrolled.pdf)

~~~
## Sequence Representation

Generate a single output for the whole sequence (either the last output of the
last state).

---
# Basic RNN Applications

## Sequence Prediction

During training, predict next sequence element.

![w=75%,h=center](sequence_prediction_training.pdf)

~~~
During inference, use predicted elements as further inputs.

![w=75%,h=center](sequence_prediction_inference.pdf)

---
# Multilayer RNNs

We might stack several layers of recurrent neural networks. Usually using two or
three layers gives better results than just one.

![w=75%,h=center](multilayer_rnn.pdf)

---
# Multilayer RNNs

In case of multiple layers, residual connections usually improve results.
Because dimensionality has to be the same, they are usually applied from the
second layer.

![w=75%,h=center](multilayer_rnn_residual.pdf)

---
# Bidirectional RNN

To consider both the left and right contexts, a _bidirectional_ RNN can be used,
which consists of parallel application of a _forward_ RNN and a _backward_ RNN.

![w=80%,h=center](bidirectional_rnn.pdf)

~~~
The outputs of both directions can be either _added_ or _concatenated_. Even
if adding them does not seem very intuitive, it does not increase
dimensionality and therefore allows residual connections to be used in case
of multilayer bidirectional RNN.

---
section: WordEmbeddings
# Word Embeddings

We might represent _words_ using one-hot encoding, considering all words to be
independent of each other.

~~~
However, words are not independent – some are more similar than others.

~~~
Ideally, we would like some kind of similarity in the space of the word
representations.

~~~
## Distributed Representation
The idea behind distributed representation is that objects can
be represented using a set of common underlying factors.

~~~
We therefore represent words as fixed-size _embeddings_ into $ℝ^d$ space,
with the vector elements playing role of the common underlying factors.

~~~
These embeddings are initialized randomly and trained together with the rest of
the network.

---
# Word Embeddings

The word embedding layer is in fact just a fully connected layer on top of
one-hot encoding. However, it is not implemented in that way.

~~~
Instead, a so-called _embedding_ layer is used, which is much more efficient.
When a matrix is multiplied by an one-hot encoded vector (all but one zeros
and exactly one 1), the row corresponding to that 1 is selected, so the
embedding layer can be implemented only as a simple lookup.

~~~
In TensorFlow, the embedding layer is available as
```python
tf.keras.layers.Embedding(input_dim, output_dim)
```

---
# Word Embeddings

Even if the embedding layer is just a fully connected layer on top of one-hot
encoding, it is important that this layer is _shared_ across
the whole network.

~~~
![w=37%](words_onehot.pdf)
~~~
![w=60%](words_embeddings.pdf)
