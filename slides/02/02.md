title: NPFL114, Lecture 2
class: title, langtech, cc-by-nc-sa
# Training Neural Networks

## Milan Straka

### March 11, 2019

---
section: ML Basics
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the main goal of _machine learning_ is to perform well on _previously
unseen_ data, so called **generalization error** or **test error**. We typically
estimate the generalization error using a **test set** of examples independent
of the training set.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.pdf)

---
# Machine Learning Basics

We control whether a model underfits or overfits by modifying its _capacity_.
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.pdf)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than any other.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||â†’Î¸||^2$).

![w=70%,h=center](regularization.pdf)

---
# Machine Learning Basics

_Hyperparameters_ are not adapted by learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
section: Loss
# Loss Function

A model is usually trained in order to minimize the _loss_ on the training data.

~~~

Assuming that a model computes $f(â†’x;â†’Î¸)$ using parameters $â†’Î¸$,
the _mean square error_ is computed as
$$âˆ‘_i \left(f(â†’x^{(i)}; â†’Î¸) - y^{(i)}\right)^2.$$

~~~

A common principle used to design loss functions is the _maximum likelihood
principle_.

---
# Maximum Likelihood Estimation

Let $ğ• = \\{â†’x^{(1)}, â†’x^{(2)}, \ldots, â†’x^{(m)}\\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $\hat p_\textrm{data}$.

Let $p_\textrm{model}(â†’x; â†’Î¸)$ be a family of distributions. The
*maximum likelihood estimation* of $â†’Î¸$ is:

$$\begin{aligned}
â†’Î¸_\mathrm{ML} &= \argmax_â†’Î¸ p_\textrm{model}(\mathbb X; â†’Î¸) \\
               &= \argmax_â†’Î¸ âˆ\nolimits_{i=1}^m p_\textrm{model}(â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ âˆ‘\nolimits_{i=1}^m -\log p_\textrm{model}(â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(â†’x; â†’Î¸)] \\
               &= \argmin_â†’Î¸ H(pÌ‚_\textrm{data}, p_\textrm{model}(â†’x; â†’Î¸)) \\
               &= \argmin_â†’Î¸ D_\textrm{KL}(pÌ‚_\textrm{data}||p_\textrm{model}(â†’x; â†’Î¸)) \color{gray} + H(pÌ‚_\textrm{data})
\end{aligned}$$

---
# Maximum Likelihood Estimation

Easily generalized to situations where our goal is predict $y$ given $â†’x$.
$$\begin{aligned}
â†’Î¸_\mathrm{ML} &= \argmax_â†’Î¸ p_\textrm{model}(\mathbb Y | \mathbb X; â†’Î¸) \\
               &= \argmax_â†’Î¸ âˆ\nolimits_{i=1}^m p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ âˆ‘\nolimits_{i=1}^m -\log p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
\end{aligned}$$

The resulting _loss function_ is called _negative log likelihood_, or
_cross-entropy_ or _Kullback-Leibler divegence_.

---
# Mean Square Error as MLE

Assume our goal is to perform a regression, i.e., to predict $p(y | â†’x)$ for $y âˆˆ â„$.

Let $yÌ‚(â†’x; â†’Î¸)$ give a prediction of mean of $y$.

We define $p(y | â†’x)$ as $\N(y; yÌ‚(â†’x; â†’Î¸), Ïƒ^2)$ for a given fixed $Ïƒ$.
Then:
$$\begin{aligned}
\argmax_â†’Î¸ p(y | â†’x; â†’Î¸) =& \argmin_â†’Î¸ âˆ‘_{i=1}^m -\log p(y^{(i)} | â†’x^{(i)} ; â†’Î¸) \\
                          =& -\argmin_â†’Î¸ âˆ‘_{i=1}^m \log \sqrt{\frac{1}{2Ï€Ïƒ^2}}
                            e ^ {\normalsize -\frac{(y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2}} \\
                          =& -m \log Ïƒ^{-1} -m \log (2Ï€)^{-1/2} - \argmin_â†’Î¸ âˆ‘_{i=1}^m -\frac{(y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2} \\
                          =& \argmin_â†’Î¸ âˆ‘_{i=1}^m \frac{||y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸)||^2}{2Ïƒ^2}
\end{aligned}$$
