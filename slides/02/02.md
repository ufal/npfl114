title: NPFL114, Lecture 2
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Training Neural Networks

## Milan Straka

### February 20, 2023

---
# Refresh â€“ Neural Networks

- Neural network describes a computation, which gets an input tensor and
  produces an output.

~~~
  - For the time being, the input tensor has a fixed size.
~~~
  - The input tensor is usually a vector, but it can be 2D/3D/4D tensor.
    - images, video, time sequences like speech, â€¦
~~~
  - The output usually describes a distribution.
    - normal distribution for regression
    - Bernoulli for binary classification
    - categorical for multiclass classification

~~~
- The basic units are **nodes**, composed in an acyclic graph.

~~~
- The edges have weights, nodes have activation functions.

~~~
- Nodes of neural networks are usually composed in layers.

---
section: ML Basics
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data-generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data-generating distribution.

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than others.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~ ~
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_ (the model is â€œtoo weakâ€, bad performance even on training set)
- _overfitting_ (the model is â€œtoo strongâ€, learned rules are too specific and do
  not generalize)
![w=83%,h=center](underfitting_overfitting.svgz)

---
# Machine Learning Basics

We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- _representational capacity_ (what the model could represent, depends on the
  model size)

~~~
- _effective capacity_ (what the model actually learns, depends on training,
  regularization, â€¦)

~~~
![w=81%,h=center](generalization_error.svgz)

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ (but not necessarily its training error) is called
**regularization**.

~~~

**$L^2$ regularization** (also called **weight decay**) penalizes models
with large weights (using aÂ penalty of $\frac{1}{2}\|â†’Î¸\|^2$).

![w=70%,h=center](regularization.svgz)

---
# Machine Learning Basics

**Hyperparameters** are not adapted by a learning algorithm itself,
while the model **parameters** (weights, biases) are adapted by it.

~~~
Usually a **development set**, also called a **validation set**, is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
section: MLE
# Loss Function

A model is usually trained in order to minimize the **loss** on the training data.

~~~

Assuming that a model computes $f(â†’x;â†’Î¸)$ using parameters $â†’Î¸$,
the **mean square error** of given $N$ examples $\big(â†’x^{(1)}, y^{(1)}\big),
\big(â†’x^{(2)}, y^{(2)}\big), â€¦, \big(â†’x^{(N)}, y^{(N)}\big)$ is computed as
$$\frac{1}{N} âˆ‘_{i=1}^N \Big(f(â†’x^{(i)}; â†’Î¸) - y^{(i)}\Big)^2.$$

~~~
A common principle used to design loss functions is the **maximum likelihood
principle**.

---
# Maximum Likelihood Estimation

Let $ğ• = \{â†’x^{(1)}, â†’x^{(2)}, â€¦, â†’x^{(N)}\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$.

~~~
We denote the **empirical data distribution** as $pÌ‚_\textrm{data}$, where
$$pÌ‚_\textrm{data}(â†’x) â‰ \frac{\big|\{i: â†’x^{(i)} = â†’x\}\big|}{N}.$$

~~~
Let $p_\textrm{model}(â‡â†’x; â†’Î¸)$ be a family of distributions.
~~~
- If the weights are fixed, $p_\textrm{model}(â‡â†’x{\color{lightgray}; â†’Î¸})$ is a probability distribution.
~~~
- If we instead consider the fixed training data $ğ•$, then
  $$L(â†’Î¸) = p_\textrm{model}(ğ•; â†’Î¸) = âˆ\nolimits_{i=1}^N p_\textrm{model}(â†’x^{(i)}; â†’Î¸)$$
  is called the **likelihood**.
~~~
  Note that even if the value of the likelihood is in range $[0, 1]$, it is not
  a probability, because the likelihood is not a probability distribution.

---
# Maximum Likelihood Estimation

Let $ğ• = \{â†’x^{(1)}, â†’x^{(2)}, â€¦, â†’x^{(N)}\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $pÌ‚_\textrm{data}$ and let
$p_\textrm{model}(â‡â†’x; â†’Î¸)$ be a family of distributions.

The **maximum likelihood estimation** of $â†’Î¸$ is:

$\displaystyle \kern8em\mathllap{â†’Î¸_\mathrm{MLE}} = \argmax_{â†’Î¸} p_\textrm{model}(ğ•; â†’Î¸) = \argmax_{â†’Î¸} âˆ\nolimits_{i=1}^N p_\textrm{model}(â†’x^{(i)}; â†’Î¸)$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’Î¸} âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(â†’x^{(i)}; â†’Î¸)$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’Î¸} ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(â†’x; â†’Î¸)]$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’Î¸} H(pÌ‚_\textrm{data}(â‡â†’x), p_\textrm{model}(â‡â†’x; â†’Î¸))$

~~~
$\displaystyle \kern8em{} = \argmin_{â†’Î¸} D_\textrm{KL}(pÌ‚_\textrm{data}(â‡â†’x)\|p_\textrm{model}(â‡â†’x; â†’Î¸)) \color{gray} + H(pÌ‚_\textrm{data}(â‡â†’x))$

---
style: .katex-display { margin: .6em 0 }
# Maximum Likelihood Estimation

MLE can be easily generalized to the conditional case, where our goal is to predict $y$ given $â†’x$:

~~~
$$\begin{aligned}
â†’Î¸_\mathrm{MLE} &= \argmax_{â†’Î¸} p_\textrm{model}(ğ• | ğ•; â†’Î¸) = \argmin_{â†’Î¸} âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
                &= \argmin_{â†’Î¸} âˆ‘\nolimits_{i=1}^N -\log p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
                &= \argmin_{â†’Î¸} ğ”¼_{(â‡â†’x, â‡y) âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(y | â†’x; â†’Î¸)] \\
                &= \argmin_{â†’Î¸} H(pÌ‚_\textrm{data}(â‡y | â‡â†’x), p_\textrm{model}(â‡y | â‡â†’x; â†’Î¸)) \\
                &= \argmin_{â†’Î¸} D_\textrm{KL}(pÌ‚_\textrm{data}(â‡y | â‡â†’x)\|p_\textrm{model}(â‡y | â‡â†’x; â†’Î¸)) \color{gray} + H(pÌ‚_\textrm{data}(â‡y | â‡â†’x))
\end{aligned}$$

~~~
where the conditional entropy is defined as
$H(pÌ‚_\textrm{data}) = ğ”¼_{(â‡â†’x, â‡y) âˆ¼ pÌ‚_\textrm{data}} [-\log (pÌ‚_\textrm{data}(y | â†’x; â†’Î¸))]$
and the conditional cross-entropy as
$H(pÌ‚_\textrm{data}, p_\textrm{model}) = ğ”¼_{(â‡â†’x, â‡y) âˆ¼ pÌ‚_\textrm{data}} [-\log (p_\textrm{model}(y | â†’x; â†’Î¸))]$.

~~~
The resulting _loss function_ is called **negative log-likelihood** (**NLL**), or
**cross-entropy**, or **Kullback-Leibler divergence**.

---
# Estimators and Bias

An **estimator** is a rule for computing an estimate of a given value, often an
expectation of some random value(s).
For example, we might estimate _mean_ of a random variable by sampling a value
according to its probability distribution.

~~~
The **bias** of an estimator is the difference of the expected value of the estimator
and the true value being estimated.
~~~
If the bias is zero, we call the estimator **unbiased**, otherwise **biased**.

~~~
If we have a sequence of estimates, it might also happen that the bias converges
to zero. Consider the well-known sample estimate of variance. Given independent
and identically distributed random variables $â‡x_1, \ldots, â‡x_N$, we might
estimate the mean and the variance as
$$Î¼Ì‚ = \frac{1}{N} âˆ‘\nolimits_i x_i,~~~ÏƒÌ‚^2 = \frac{1}{N} âˆ‘\nolimits_i (x_i - Î¼Ì‚)^2.$$
~~~
Such a mean estimate is unbiased, but the estimate of the variance is biased,
because $ğ”¼[ÏƒÌ‚^2] = (1 - \frac{1}{N})Ïƒ^2$; however, the bias of this estimate
converges to zero for increasing $N$.

~~~
Also, an unbiased estimator does not necessarily have a small variance â€“ in some
cases, it can have a large variance, so a biased estimator with a smaller variance
might be preferred.

---
# Properties of Maximum Likelihood Estimation

Assume that the true data-generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(â€¢; â†’Î¸)$, and assume there exists a unique
$â†’Î¸_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(â€¢; â†’Î¸_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $â†’Î¸_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the
  data-generating distribution, then $â†’Î¸_m$ converges in probability to
  $â†’Î¸_{p_\textrm{data}}$.

  Formally, for any $Îµ > 0$, $P(\|â†’Î¸_m - â†’Î¸_{p_\textrm{data}}\| > Îµ) â†’ 0$
  as $m â†’ âˆ$.

~~~
- MLE is in a sense the _most statistically efficient_. For any consistent estimator,
  let us consider the average distance of $â†’Î¸_m$ and $â†’Î¸_{p_\textrm{data}}$:
  $ğ”¼_{â‡â†’x_1, â€¦, â‡â†’x_m âˆ¼ p_\textrm{data}} \big[\|â†’Î¸_m - â†’Î¸_{p_\textrm{data}}\|^2\big]$. \
  It can be shown (Rao 1945, CramÃ©r 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $Ïƒ^2$ â€“ the most general
such a distribution is the normal distribution.

![w=50%,h=center](constant_variance.svgz)

---
# Mean Square Error as MLE

Let $f(â†’x; â†’Î¸)$ be the output of our model, which we assume to be the mean of $y$.

~~~
We define $p(y | â†’x)$ as $ğ“(y; f(â†’x; â†’Î¸), Ïƒ^2)$ for some fixed $Ïƒ^2$.
~~~
The MLE then results in

![w=18%,f=right;position:absolute;right:.5em](normal_paranormal.png)

$\displaystyle \kern8em\mathllap{\argmax_{â†’Î¸} p(ğ• | ğ•; â†’Î¸)} = \argmin_{â†’Î¸} âˆ‘_{i=1}^N -\log p(y^{(i)} | â†’x^{(i)} ; â†’Î¸)$

~~~
$\displaystyle \kern1.5em{} = \argmin_{â†’Î¸} -âˆ‘_{i=1}^N \log \sqrt{\frac{1}{2Ï€Ïƒ^2}} e ^ {\normalsize -\frac{(y^{(i)} - f(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2}}$

~~~
$\displaystyle \kern1.5em{} = \argmin_{â†’Î¸} {\color{gray} -N \log (2Ï€Ïƒ^2)^{-1/2}} - âˆ‘_{i=1}^N -\frac{\big(y^{(i)} - f(â†’x^{(i)}; â†’Î¸)\big)^2}{2Ïƒ^2}$

~~~
$\displaystyle \kern1.5em{} = \argmin_{â†’Î¸} âˆ‘_{i=1}^N \frac{\big(y^{(i)} - f(â†’x^{(i)}; â†’Î¸)\big)^2}{2Ïƒ^2} = \argmin_{â†’Î¸} \frac{1}{N} âˆ‘_{i=1}^N \big(f(â†’x^{(i)}; â†’Î¸) - y^{(i)}\big)^2.$

---
section: Gradient Descent
# Gradient Descent

Let a model compute $f(â†’x;â†’Î¸)$ using parameters $â†’Î¸$, and for a given loss
function $L$ denote
$$E(â†’Î¸) = ğ”¼_{(â‡â†’x, â‡y)âˆ¼pÌ‚_\textrm{data}} L\big(f(â†’x; â†’Î¸), y\big).$$
~~~
![w=53%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_{â†’Î¸} E(â†’Î¸),$$
we may use _gradient descent_:
$$â†’Î¸ â† â†’Î¸ - Î± âˆ‡_{â†’Î¸} E(â†’Î¸).$$

~~~
The constant $Î±$ is called a **learning rate** and specifies the â€œlengthâ€
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

The gradient of the error function $E(â†’Î¸)$ can be computed as
$$âˆ‡_{â†’Î¸} E(â†’Î¸) = ğ”¼_{(â‡â†’x, â‡y)âˆ¼pÌ‚_\textrm{data}} âˆ‡_{â†’Î¸} L\big(f(â†’x; â†’Î¸), y\big).$$

~~~
- **(Standard/Batch) Gradient Descent**: We use all training data to compute $âˆ‡_{â†’Î¸} E(â†’Î¸)$.

~~~
- **Stochastic (or Online) Gradient Descent**: We estimate $âˆ‡_{â†’Î¸} E(â†’Î¸)$ using
  a single random example from the training data. Such an estimate is unbiased,
  but very noisy.

$$âˆ‡_{â†’Î¸} E(â†’Î¸) â‰ˆ âˆ‡_{â†’Î¸} L\big(f(â†’x; â†’Î¸), y\big)\textrm{~~for a randomly chosen~~}(â†’x, y)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

~~~
- **Minibatch SGD**: Trade-off between gradient descent and SGD â€“ the
  expectation in $âˆ‡_{â†’Î¸} E(â†’Î¸)$ is estimated using $m$ random independent
  examples from the training data.

$$âˆ‡_{â†’Î¸} E(â†’Î¸) â‰ˆ \frac{1}{m} âˆ‘_{i=1}^m âˆ‡_{â†’Î¸} L\big(f(â†’x^{(i)}; â†’Î¸), y^{(i)}\big)
  \textrm{~~for randomly chosen~~}(â†’x^{(i)}, y^{(i)})\textrm{~~from~~}pÌ‚_\textrm{data}.$$

---
# Stochastic Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $Î±_i$, and using a noisy estimate $J(â†’Î¸)$ of the real
gradient $âˆ‡_{â†’Î¸} E(â†’Î¸)$:
$$â†’Î¸_{i+1} â† â†’Î¸_i - Î±_i J(â†’Î¸_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro
algorithm, 1951) that if the loss function is convex and continuous, then SGD
converges to the unique optimum almost surely if the sequence of learning rates
$Î±_i$ fulfills the following conditions:
$$âˆ€i: Î±_i > 0,~~~âˆ‘_i Î±_i = âˆ,~~~âˆ‘_i Î±_i^2 < âˆ.$$

~~~
Note that the third condition implies that $Î±_i â†’ 0$.

~~~
For nonconvex loss functions, we can get guarantees of converging to a _local_
optimum only. However, note that finding the global minimum of an arbitrary
function is _at least NP-hard_.

---
# Stochastic Gradient Descent Convergence

Convex functions mentioned on the previous slide are such that for $â†’u, â†’v$
and real $0 â‰¤ t â‰¤ 1$,
$$f(tâ†’u + (1-t)â†’v) â‰¤ tf(â†’u) + (1-t)f(â†’v).$$

![w=88%,mw=50%,h=center](convex_2d.svgz)![w=66.5%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function of a single variable is convex iff its second
derivative is always nonnegative. (For functions of multiple variables,
the Hessian must be positive semi-definite.)

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$, $-\log x$, MSE,
$Ïƒ$+NLL, $\softmax$+NLL.

---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-56 (0.85 million parameters)
with/without skip connections:
![w=100%](nn_loss.jpg)

---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-110 without skip connections and DenseNet-121:
![w=100%](nn_loss_densenet.jpg)

---
section: Backpropagation
# Backpropagation

Assume we want to compute partial derivatives of a given loss function $L$.

![w=90%,mw=49%,h=center](node_forward.svgz)
~~~
![w=90%,mw=49%,h=center](node_backward.svgz)

The gradient computation is based on the chain rule of derivatives: $\displaystyle \frac{âˆ‚L}{âˆ‚x_i} = \frac{âˆ‚L}{âˆ‚y} \frac{âˆ‚y}{âˆ‚x_i}$.

~~~
![w=90%,mw=49%,h=center](fork_forward.svgz)
~~~
![w=90%,mw=49%,h=center](fork_backward.svgz)

---
# Backpropagation Algorithm

#### Forward Propagation
<div class="algorithm">

**Input**: Network with nodes $u^{(1)}, u^{(2)}, â€¦, u^{(n)}$ numbered in
topological order.  
Each node's value is computed as $u^{(i)} = f^{(i)}(A^{(i)})$
for $A^{(i)}$ being a set of values of the predecessors $P(u^{(i)})$ of
$u^{(i)}$. <br>
**Output**: Value of $u^{(n)}$.

~~~
- For $i = 1, â€¦, n$:
~~~
    - $A^{(i)} â† \big\lbrace u^{(j)} | j âˆˆ P(u^{(i)})\big\rbrace$
~~~
    - $u^{(i)} â† f^{(i)}(A^{(i)})$
~~~
- Return $u^{(n)}$
</div>

---
# Backpropagation Algorithm

#### Simple Variant of Backpropagation
<div class="algorithm">

**Input**: The network as in the Forward propagation algorithm.<br>
**Output**: Partial derivatives $g^{(i)} = \frac{âˆ‚u^{(n)}}{âˆ‚u^{(i)}}$ of $u^{(n)}$ with respect to all $u^{(i)}$.

~~~
- Run forward propagation to compute all $u^{(i)}$
~~~
- $g^{(n)} = 1$
~~~
- For $i = n-1, â€¦, 1$:
~~~
    - $g^{(i)} â† âˆ‘_{j:iâˆˆP(u^{(j)})} g^{(j)} \frac{âˆ‚u^{(j)}}{âˆ‚u^{(i)}}$
~~~
- Return $\big(g^{(1)}, g^{(2)}, â€¦, g^{(n)}\big)$
</div>

~~~
In practice, we do not usually represent networks as collections of scalar
nodes; instead we represent them as collections of tensor functions â€“ most
usually functions $f: â„^n â†’ â„^m$. Then $\frac{âˆ‚f(â†’x)}{âˆ‚â†’x}$ is a Jacobian
matrix. However, the backpropagation algorithm is analogous.

---
# Neural Network Activation Functions

## Hidden Layers Derivatives
- $Ïƒ$:
  $$\frac{âˆ‚Ïƒ(x)}{âˆ‚x} = Ïƒ(x) â‹… \big(1-Ïƒ(x)\big)$$
~~~
- $\tanh$:
  $$\frac{âˆ‚\tanh(x)}{âˆ‚x} = 1 - \tanh(x)^2$$
~~~
- ReLU:
  $$ \frac{âˆ‚\ReLU(x)}{âˆ‚x} = \begin{Bmatrix} 1 &\textrm{if } x > 0 \\ \textrm{NaN} &\textrm{if }x = 0 \\ 0 &\textrm{if } x < 0 \end{Bmatrix} \xlongequal{\substack{\textrm{assuming }\frac{âˆ‚\ReLU(x)}{âˆ‚x}(0) = 0}} \big[x > 0\big] = \big[\ReLU(x) > 0\big]$$

---
section: SGDs
# Stochastic Gradient Descent

#### Stochastic Gradient Descent (SGD) Algorithm
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$.  
**Output**: Updated parameters $â†’Î¸$.

- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L\big(f(â†’x^{(i)}; â†’Î¸), y^{(i)}\big)$
    - $â†’Î¸ â† â†’Î¸ - Î±â†’g$
</div>

---
# SGD With Momentum

#### SGD With Momentum
<div class="algorithm">
![w=40%,f=right](momentum.svgz)

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, momentum $Î²$.  
**Output**: Updated parameters $â†’Î¸$.

- $â†’v â† â†’0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L\big(f(â†’x^{(i)}; â†’Î¸), y^{(i)}\big)$
    - $â†’v â† Î²â†’v - Î±â†’g$
    - $â†’Î¸ â† â†’Î¸ + â†’v$
</div>

A nice writeup about momentum can be found on https://distill.pub/2017/momentum/.

---
# SGD With Nesterov Momentum

#### SGD With Nesterov Momentum
<div class="algorithm">
![w=55%,f=right](nesterov.jpg)

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.<br>
**Input**: Learning rate $Î±$, momentum $Î²$.<br>
**Output**: Updated parameters $â†’Î¸$.
- $â†’v â† â†’0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’Î¸ â† â†’Î¸ + Î²â†’v$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’v â† Î²â†’v - Î±â†’g$
    - $â†’Î¸ â† â†’Î¸ - Î±â†’g$
</div>

---
section: Adaptive LR
# Algorithms with Adaptive Learning Rates

#### AdaGrad (2011)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, constant $Îµ$ (usually $10^{-7}$).  
**Output**: Updated parameters $â†’Î¸$.

- $â†’r â† â†’0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’r â† â†’r + â†’g^2$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’r} + Îµ}â†’g$
</div>

- The $â†’g^2$ and $\frac{Î±}{\sqrt{â†’r} + Îµ}â†’g$ are computed element-wise.
- The $â†’g^2$ is sometimes also written as $â†’gâŠ™â†’g$.

---
# Algorithms with Adaptive Learning Rates

AdaGrad has favourable convergence properties (being faster than regular SGD)
for convex loss landscapes. In this settings, gradients converge to zero
reasonably fast.

~~~
However, for nonconvex losses, gradients can stay quite large for a long time.
In that case, the algorithm behaves as if decreasing learning rate by a factor
of $1/\sqrt{t}$, because if each
$$â†’g â‰ˆ â†’g_0,$$
then after $t$ steps
$$â†’r â‰ˆ t â‹… â†’g_0^2,$$

~~~
and therefore
$$\frac{Î±}{\sqrt{â†’r} + Îµ} â‰ˆ \frac{Î± / \sqrt{t}}{\sqrt{â†’g_0^2} + Îµ/\sqrt t}.$$

---
style: .katex-display {margin:0 0} div.algorithm {padding-top:.1em; padding-bottom:.1em}
# Algorithms with Adaptive Learning Rates

#### RMSProp (2012)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, momentum $Î²$ (usually $0.9$), constant $Îµ$ (usually $10^{-7}$).  
**Output**: Updated parameters $â†’Î¸$.

- $â†’r â† â†’0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’r â† Î²â†’r + (1-Î²)â†’g^2$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’r} + Îµ}â†’g$
</div>

~~~
However, after first step, $â†’r = (1-Î²)â†’g^2$, which for default $Î²=0.9$ is
$$â†’r = 0.1 â†’g^2,$$
so $â†’r$ is a biased estimate of $ğ”¼[â†’g^2]$ (but the bias converges to zero exponentially fast).

---
style: div.algorithm {padding-top:0.1em}
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$ (default 0.001), constant $Îµ$ (usually $10^{-7}$).  
**Input**: Momentum $Î²_1$ (default 0.9), momentum $Î²_2$ (default 0.999).  
**Output**: Updated parameters $â†’Î¸$.

- $â†’s â† â†’0$, $â†’r â† â†’0$, $t â† 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $t â† t + 1$
    - $â†’s â† Î²_1â†’s + (1-Î²_1)â†’g$Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â _(biased first moment estimate)_
    - $â†’r â† Î²_2â†’r + (1-Î²_2)â†’g^2$Â Â Â Â Â Â Â Â Â Â Â Â Â Â _(biased second moment estimate)_
    - $â†’sÌ‚ â† â†’s / (1 - Î²_1^t)$, $â†’rÌ‚ â† â†’r / (1 - Î²_2^t)$Â Â Â Â _(unbiased estimates of the moments)_
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’rÌ‚} + Îµ}â†’sÌ‚$
</div>

---
# Adam Bias Correction

To allow analysis, we add indices to the update
$$â†’s_{t} â† Î²_1â†’s_{t-1} + (1 - Î²_1)â†’g_t,$$
with $â†’s_0 â† â†’0$.

~~~ ~
# Adam Bias Correction

To allow analysis, we add indices to the update
$$â†’s_{t} â† Î²_1â†’s_{t-1} + (1 - Î²_1)â†’g_t,$$

![w=50%,f=right](ema_all.svgz)

with $â†’s_0 â† â†’0$.

After $t$ steps, we have

$$â†’s_t = (1 - Î²_1) âˆ‘_{i=1}^t Î²_1^{t-i}â†’g_i.$$

~~~
Because $âˆ‘_{i=0}^âˆ Î²_1^i = \frac{1}{1 - Î²_1}$, $â†’s_âˆ$ is computed as a weighted
average of infinitely many elements.

---
# Adam Bias Correction

![w=50%,f=right](ema_truncated.svgz)

However, for $t < âˆ$, the sum of weights in the computation of $â†’s_t$ does not
sum to one.

~~~
To obtain an unbiased estimate, we therefore need to account for the â€œmissingâ€
elements; in other words, we need to scale the weights so that they sum
to one.

~~~
The sum of weights after $t$ steps is
$$(1 - Î²_1) âˆ‘_{i=1}^t Î²_1^{t-i} = âˆ‘_{i=1}^t Î²_1^{t-i} - âˆ‘_{i=0}^{t-1} Î²_1^{t-i} = 1 - Î²_1^t,$$

~~~
so we obtain an unbiased estimate by dividing $â†’s_t$ with $(1 - Î²_1^t)$, and
analogously for the correction of $â†’r$.

---
# Adaptive Optimizers Animations

![w=50%,h=center](optimizers-1.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-2.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-3.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-4.gif)

---
section: LR Schedules
# Learning Rate Schedules

Even if RMSProp and Adam are adaptive, they still usually require carefully tuned
decreasing learning rate for top-notch performance.

~~~
![w=32%,f=right](decay_linear.svgz)

- **Polynomial decay**: learning rate is multiplied by some polynomial of the
  current update number $t$.
  - **Linear decay** uses $Î±_t = Î±_\textrm{initial} â‹… \big(1 - \frac{t}{\textrm{max~steps}}\big)$ and has
    theoretical guarantees of convergence, but is usually too fast for deep
    neural networks.
~~~
  - **Inverse square root decay** uses $Î±_t = Î±_\textrm{initial} â‹… \frac{1}{\sqrt{t}}$
    and is currently used by best machine translation models.

~~~
![w=32%,f=right](decay_exponential.svgz)

- **Exponential decay**: learning rate is multiplied by a constant each
  minibatch/epoch/several epochs.

  - $Î±_t = Î±_\textrm{initial} â‹… c^t$
  - Often used for convolutional networks (image recognition etc.).

---
# Learning Rate Schedules

![w=32%,f=right](decay_cosine.svgz)

- **Cosine decay**: The cosine decay has became quite popular in the past years,
  both for training and finetuning.

  $$Î±_t = Î±_\textrm{initial} â‹… \frac{1}{2}\bigg(1 + \cos\Big(Ï€ â‹… \frac{t}{\textrm{max~steps}}\Big)\bigg)$$

~~~
- Cyclic restarts, warmup, â€¦

~~~
The `tf.optimizers.schedules` offers several such learning rate schedules,
which can be passed to any Keras optimizer directly as a learning rate.
- `tf.optimizers.schedules.PiecewiseConstantDecay`
- `tf.optimizers.schedules.PolynomialDecay`
- `tf.optimizers.schedules.ExponentialDecay`
- `tf.optimizers.schedules.CosineDecay`

---
section: DoubleD
# Why do Neural Networks Generalize so Well â€“ Double Descent

![w=100%,v=middle](double_descent.svgz)

---
# Deep Double Descent

![w=100%,v=middle](deep_double_descent.svgz)

---
# Deep Double Descent â€“ Effective Model Complexity

The authors define the **Effective Model Complexity** (**EMC**) of a training procedure $ğ“£$ with respect to distribution $ğ““$ and parameter $Îµ>0$
as
$$\operatorname{EMC}_{ğ““,Îµ}(ğ“£) â‰  \max \big\{n \,\big|\, ğ”¼_{â‡Sâˆ¼ğ““^n}[\mathrm{Error}_S(ğ“£(S))] â‰¤ Îµ\big\},$$
where $\mathrm{Error}_S(M)$ is the mean error of a model $M$ on the train samples $S$.

~~~
**Hypothesis:** For any natural data distribution $ğ““$, neural-network-based
training procedure $ğ“£$, and small $Îµ>0$, if we consider the task of predicting
labels based on $n$ samples from $ğ““$, then:

- **Under-parametrized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£)$ is
  sufficiently smaller than $n$, any perturbation of $ğ“£$ that increases its
  effective complexity will decrease the test error.

~~~
- **Over-parametrized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£)$ is
  sufficiently larger than $n$, any perturbation of $ğ“£$ that increases its
  effective complexity will decrease the test error.

~~~
- **Critically parametrized regime.** If $\operatorname{EMC}_{ğ““,Îµ}(ğ“£) â‰ˆ n$,
  then a perturbation of $ğ“£$ that increases its effective complexity might
  decrease **or increase** the test error.

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent_size_epochs.svgz)

---
# Why do Neural Networks Generalize so Well

![w=90%,h=center](deep_double_descent_width.svgz)

