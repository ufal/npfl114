title: NPFL114, Lecture 2
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Training Neural Networks

## Milan Straka

### March 2, 2020

---
section: ML Basics
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
_Bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated.

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

~~~
If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $â‡x_1,
\ldots, â‡x_n$ idenpendent and identically distributed random variables, we might
estimate mean and variance as
$$Î¼Ì‚ = \frac{1}{n} âˆ‘\nolimits_i x_i,~~~ÏƒÌ‚_2 = \frac{1}{n} âˆ‘\nolimits_i (x_i - Î¼Ì‚)^2.$$
~~~
Such estimate is biased, because $ğ”¼[ÏƒÌ‚^2] = (1 - \frac{1}{n})Ïƒ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance â€“ in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the main goal of _machine learning_ is to perform well on _previously
unseen_ data, so called **generalization error** or **test error**. We typically
estimate the generalization error using a **test set** of examples independent
of the training set, but generated by the same data generating distribution.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.pdf)

---
# Machine Learning Basics

We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.pdf)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than others.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||â†’Î¸||^2$).

![w=70%,h=center](regularization.pdf)

---
# Machine Learning Basics

_Hyperparameters_ are not adapted by learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](double_descent.pdf)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent.pdf)

---
# Why do Neural Networks Generalize so Well

![w=90%,h=center](deep_double_descent_width.pdf)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent_size_epochs.pdf)

---
section: Loss
# Loss Function

A model is usually trained in order to minimize the _loss_ on the training data.

~~~

Assuming that a model computes $f(â†’x;â†’Î¸)$ using parameters $â†’Î¸$,
the _mean square error_ is computed as
$$\frac{1}{m} âˆ‘_{i=1}^m \left(f(â†’x^{(i)}; â†’Î¸) - y^{(i)}\right)^2.$$

~~~

A common principle used to design loss functions is the _maximum likelihood
principle_.

---
# Maximum Likelihood Estimation

Let $ğ• = \{â†’x^{(1)}, â†’x^{(2)}, \ldots, â†’x^{(m)}\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $pÌ‚_\textrm{data}$.

Let $p_\textrm{model}(â†’x; â†’Î¸)$ be a family of distributions. The
*maximum likelihood estimation* of $â†’Î¸$ is:

$$\begin{aligned}
â†’Î¸_\mathrm{ML} &= \argmax_â†’Î¸ p_\textrm{model}(\mathbb X; â†’Î¸) \\
               &= \argmax_â†’Î¸ âˆ\nolimits_{i=1}^m p_\textrm{model}(â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ âˆ‘\nolimits_{i=1}^m -\log p_\textrm{model}(â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ ğ”¼_{â‡â†’x âˆ¼ pÌ‚_\textrm{data}} [-\log p_\textrm{model}(â†’x; â†’Î¸)] \\
               &= \argmin_â†’Î¸ H(pÌ‚_\textrm{data}, p_\textrm{model}(â†’x; â†’Î¸)) \\
               &= \argmin_â†’Î¸ D_\textrm{KL}(pÌ‚_\textrm{data}||p_\textrm{model}(â†’x; â†’Î¸)) \color{gray} + H(pÌ‚_\textrm{data})
\end{aligned}$$

---
# Maximum Likelihood Estimation

MLE can be easily generalized to a conditional case, where our goal is to predict $y$ given $â†’x$:
$$\begin{aligned}
â†’Î¸_\mathrm{ML} &= \argmax_â†’Î¸ p_\textrm{model}(\mathbb Y | \mathbb X; â†’Î¸) \\
               &= \argmax_â†’Î¸ âˆ\nolimits_{i=1}^m p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
               &= \argmin_â†’Î¸ âˆ‘\nolimits_{i=1}^m -\log p_\textrm{model}(y^{(i)} | â†’x^{(i)}; â†’Î¸) \\
\end{aligned}$$

~~~
The resulting _loss function_ is called _negative log likelihood_, or
_cross-entropy_ or _Kullback-Leibler divergence_.

---
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(â‹…; â†’Î¸)$, and assume there exists a unique
$â†’Î¸_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(â‹…; â†’Î¸_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $â†’Î¸_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $â†’Î¸_m$ converges in probability to
  $â†’Î¸_{p_\textrm{data}}$.

  Formally, for any $Îµ > 0$, $P(||â†’Î¸_m - â†’Î¸_{p_\textrm{data}}|| > Îµ) â†’ 0$
  as $m â†’ âˆ$.

~~~
- MLE is in a sense most _statistic efficient_. For any consistent estimator, we
  might consider the average distance of $â†’Î¸_m$ and $â†’Î¸_{p_\textrm{data}}$,
  formally $ğ”¼_{â‡â†’x_1, \ldots, â‡â†’x_m âˆ¼ p_\textrm{data}} [||â†’Î¸_m - â†’Î¸_{p_\textrm{data}}||_2^2]$.
  It can be shown (Rao 1945, CramÃ©r 1946) that no consistent estimator has
  a lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
# Mean Square Error as MLE

Assume our goal is to perform a regression, i.e., to predict $p(y | â†’x)$ for $y âˆˆ â„$.

~~~
Let $yÌ‚(â†’x; â†’Î¸)$ give a prediction of mean of $y$.

~~~
We define $p(y | â†’x)$ as $\N(y; yÌ‚(â†’x; â†’Î¸), Ïƒ^2)$ for a given fixed $Ïƒ$.
~~~
Then:
$$\begin{aligned}
\argmax_â†’Î¸ p(y | â†’x; â†’Î¸) =& \argmin_â†’Î¸ âˆ‘_{i=1}^m -\log p(y^{(i)} | â†’x^{(i)} ; â†’Î¸) \\
                         =& -\argmin_â†’Î¸ âˆ‘_{i=1}^m \log \sqrt{\frac{1}{2Ï€Ïƒ^2}}
                            e ^ {\normalsize -\frac{(y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2}} \\
                         =& -\argmin_â†’Î¸ {\color{gray} m \log (2Ï€Ïƒ^2)^{-1/2} +}
                            âˆ‘_{i=1}^m -\frac{(y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2} \\
                         =& \argmin_â†’Î¸ âˆ‘_{i=1}^m \frac{(y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2}{2Ïƒ^2} = \argmin_â†’Î¸ âˆ‘_{i=1}^m (y^{(i)} - yÌ‚(â†’x^{(i)}; â†’Î¸))^2.
\end{aligned}$$

---
section: Gradient Descent
# Gradient Descent

Let a model compute $f(â†’x;â†’Î¸)$ using parameters $â†’Î¸$, and for a given loss
function $L$ denote
$$J(â†’Î¸) = ğ”¼_{(â†’x, y)âˆ¼pÌ‚_\textrm{data}} L\big(f(â†’x; â†’Î¸), y\big).$$
~~~
![w=53%,f=right](gradient_descent.pdf)

Assuming we are minimizing an error function
$$\argmin_â†’Î¸ J(â†’Î¸)$$
we may use _gradient descent_:
$$â†’Î¸ â† â†’Î¸ - Î±âˆ‡_â†’Î¸J(â†’Î¸)$$

~~~
The constant $Î±$ is called a _learning rate_ and specifies the â€œlengthâ€
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

## (Regular) Gradient Descent

We use all training data to compute $J(â†’Î¸)$.

~~~
## Online (or Stochastic) Gradient Descent

We estimate the expectation in $J(â†’Î¸)$ using a single randomly sampled example
from the training data. Such an estimate is unbiased, but very noisy.

$$J(â†’Î¸) â‰ˆ L\big(f(â†’x; â†’Î¸), y\big)\textrm{~~for randomly chosen~~}(â†’x, y)\textrm{~~from~~}pÌ‚_\textrm{data}.$$

~~~
## Minibatch SGD

The minibatch SGD is a trade-off between gradient descent and SGD â€“ the
expectation in $J(â†’Î¸)$ is estimated using $m$ random independent examples from
the training data.

$$J(â†’Î¸) â‰ˆ \frac{1}{m} âˆ‘_{i=1}^m L\big(f(â†’x^{(i)}; â†’Î¸), y^{(i)}\big)
          \textrm{~~for randomly chosen~~}(â†’x^{(i)}, y^{(i)})\textrm{~~from~~}pÌ‚_\textrm{data}.$$

---
# Stochastic Gradient Descent Convergence

It can be proven (under some reasonable conditions; see Robbins and Monro
algorithm, 1951) that if the loss function is convex and continuous, then SGD
converges to the unique optimum almost surely if the sequence of learning rates
$Î±_i$ fulfills the following conditions:
$$âˆ‘_i Î±_i = âˆ,~~~âˆ‘_i Î±_i^2 < âˆ.$$

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only.

~~~
Note that finding a global minimum of an arbitrary function is _at least NP-hard_.

---
# Stochastic Gradient Descent Convergence

Convex functions mentioned on a previous slide are such that for $x_1, x_2$
and real $0 â‰¤ t â‰¤ 1$,
$$f(tx_1 + (1-t)x_2) â‰¤ tf(x_1) + (1-t)f(x_2).$$

![w=90%,mw=50%,h=center](convex_2d.pdf)![w=68%,mw=50%,h=center](convex_3d.pdf)

~~~
A twice-differentiable function is convex iff its second derivative is always
non-negative.

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$ and $-\log x$.

---
# Stochastic Gradient Descent Convergence

In 2018, there have been several improvements:
- Under some models with high capacity, it can be proven that SGD will reach
  global optimum by showing it will reach zero training error.
~~~

- Neural networks can be easily modified so that the augmented version has
  no local minimums. Therefore, if such a network converges, it converged
  to a global minimum. However, the training process can still fail to converge
  by increasing the size of the parameters $||â†’Î¸||$ beyond any limit.


---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-56 (0.85 million parameters)
with/without skip connections:
![w=100%](nn_loss.jpg)

---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-110 without skip connections and DenseNet-121.
![w=100%](nn_loss_densenet.jpg)

---
section: Backpropagation
# Backpropagation

Assume we want to compute partial derivatives of a given loss function $J$ and
let $\frac{âˆ‚J}{âˆ‚z}$ be known.

![w=30%,h=center](chain_rule.pdf)

$$\begin{gathered}
\frac{âˆ‚J}{âˆ‚y_i} = \frac{âˆ‚J}{âˆ‚z} \frac{âˆ‚z}{âˆ‚y_i} = \frac{âˆ‚J}{âˆ‚z} \frac{âˆ‚g(â†’y)}{âˆ‚y_i} \\
\frac{âˆ‚J}{âˆ‚â†’x_i} = \frac{âˆ‚J}{âˆ‚z} \frac{âˆ‚z}{âˆ‚y_i} \frac{âˆ‚y_i}{âˆ‚â†’x_i} = \frac{âˆ‚J}{âˆ‚z} \frac{âˆ‚g(â†’y)}{âˆ‚y_i} \frac{âˆ‚f(â†’x_i)}{âˆ‚â†’x_i}
\end{gathered}$$

---
# Backpropagation Example

![w=75%,h=center,v=middle](net.pdf)

---
# Backpropagation Example

![w=75%,h=center,v=middle](net-forward.pdf)

---
class: wide, center
# Backpropagation Example

![w=95%,mh=90%,v=bottom](net-backward.pdf)

~~~
This is meant to be frightening â€“ you do **not** do this manually when training.

---
class: middle
# Backpropagation Algorithm

#### Forward Propagation
<div class="algorithm">

**Input**: Network with nodes $u^{(1)}, u^{(2)}, \ldots, u^{(n)}$ numbered in
topological order.  
Each node's value is computed as $u^{(i)} = f^{(i)}(\mathbb A^{(i)})$
for $\mathbb A^{(i)}$ being a set of values of the predecessors $P(u^{(i)})$ of
$u^{(i)}$. <br>
**Output**: Value of $u^{(n)}$.
- For $i = 1, \ldots, n$:
    - $\mathbb A^{(i)} â† \lbrace u^{(j)} | j âˆˆ P(u^{(i)})\rbrace$
    - $u^{(i)} â† f^{(i)}(\mathbb A^{(i)})$
- Return $u^{(n)}$
</div>

---
class: middle
# Backpropagation Algorithm

#### Simple Variant of Backpropagation
<div class="algorithm">

**Input**: The network as in the Forward propagation algorithm.<br>
**Output**: Partial derivatives $g^{(i)} = \frac{âˆ‚u^{(n)}}{âˆ‚u^{(i)}}$ of $u^{(n)}$ with respect to all $u^{(i)}$.
- Run forward propagation to compute all $u^{(i)}$
- $g^{(n)} = 1$
- For $i = n-1, \ldots, 1$:
    - $g^{(i)} â† âˆ‘_{j:iâˆˆP(u^{(j)})} g^{(j)} \frac{âˆ‚u^{(j)}}{âˆ‚u^{(i)}}$
- Return $â†’g$
</div>

In practice, we do not usually represent networks as collections of scalar
nodes; instead we represent them as collections of tensor functions â€“ most
usually functions $f: â„^n â†’ â„^m$. Then $\frac{âˆ‚f(â†’x)}{âˆ‚â†’x}$ is a Jacobian.
However, the backpropagation algorithm is analogous.

---
section: NN Training
# Neural Network Architecture Ã  la '80s

![w=45%,h=center](../01/neural_network.svg)

---
# Neural Network Architecture Ã  la '80s
There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(âˆ‘_j w_{i,j} x_j + b_i\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$â†’h = f\left(â‡‰W â†’x + â†’b\right)$$

---
# Neural Networks and Biases

![w=55%,h=center](mlp_bias.pdf)

---
# Neural Network Activation Functions

## Hidden Layers Derivatives
- $Ïƒ$:
  $$\frac{dÏƒ(x)}{dx} = Ïƒ(x) â‹… (1-Ïƒ(x))$$
~~~
- $\tanh$:
  $$\frac{d\tanh(x)}{dx} = 1 - \tanh(x)^2$$
~~~
- ReLU:
  $$ \frac{d\ReLU(x)}{dx} = \begin{cases} 1 &\text{if } x > 0 \\ \textrm{NaN} &\text{if }x = 0 \\ 0 &\text{if } x < 0 \end{cases}$$

---
section: SGDs
class: middle
# Stochastic Gradient Descent

#### Stochastic Gradient Descent (SGD) Algorithm
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$.  
**Output**: Updated parameters $â†’Î¸$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’Î¸ â† â†’Î¸ - Î±â†’g$
</div>

---
# SGD With Momentum

#### SGD With Momentum
<div class="algorithm">
![w=40%,f=right](momentum.pdf)

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, momentum $Î²$.  
**Output**: Updated parameters $â†’Î¸$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’v â† Î²â†’v - Î±â†’g$
    - $â†’Î¸ â† â†’Î¸ + â†’v$
</div>

---
# SGD With Nesterov Momentum

#### SGD With Nesterov Momentum
<div class="algorithm">
![w=55%,f=right](nesterov.jpg)

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.<br>
**Input**: Learning rate $Î±$, momentum $Î²$.<br>
**Output**: Updated parameters $â†’Î¸$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’Î¸ â† â†’Î¸ + Î²â†’v$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’v â† Î²â†’v - Î±â†’g$
    - $â†’Î¸ â† â†’Î¸ - Î±â†’g$
</div>

---
section: Adaptive LR
class: middle
# Algorithms with Adaptive Learning Rates

#### AdaGrad (2011)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, constant $Îµ$ (usually $10^{-8}$).  
**Output**: Updated parameters $â†’Î¸$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’r â† â†’r + â†’g^2$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’r + Îµ}}â†’g$
</div>

---
# Algorithms with Adaptive Learning Rates

AdaGrad has favourable convergence properties (being faster than regular SGD)
for convex loss landscapes. In this settings, gradients converge to zero
reasonably fast.

~~~
However, for non-convex losses, gradients can stay quite large for a long time.
In that case, the algorithm behaves as if decreasing learning rate by a factor
of $1/\sqrt{t}$, because if each
$$â†’g â‰ˆ â†’g_0,$$
then after $t$ steps
$$â†’r â‰ˆ t â‹… â†’g_0^2$$
and therefore
$$\frac{Î±}{\sqrt{â†’r + Îµ}} â‰ˆ \frac{Î± / \sqrt{t}}{\sqrt{â†’g_0^2 + Îµ/t}}.$$

---
# Algorithms with Adaptive Learning Rates

#### RMSProp (2012)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$, momentum $Î²$ (usually $0.9$), constant $Îµ$ (usually $10^{-8}$).  
**Output**: Updated parameters $â†’Î¸$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $â†’r â† Î²â†’r + (1-Î²)â†’g^2$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’r + Îµ}}â†’g$
</div>

~~~
However, after first step, $â†’r = (1-Î²)â†’g^2$, which for default $Î²=0.9$ is
$$â†’r = 0.1 â†’g^2,$$
a biased estimate (but the bias converges to zero exponentially fast).

---
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$ (default 0.001), constant $Îµ$ (usually $10^{-8}$).  
**Input**: Momentum $Î²_1$ (default 0.9), momentum $Î²_2$ (default 0.999).  
**Output**: Updated parameters $â†’Î¸$.
- $â†’s â† 0$, $â†’r â† 0$, $t â† 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $t â† t + 1$
    - $â†’s â† Î²_1â†’s + (1-Î²_1)â†’g$   Â Â  _(biased first moment estimate)_
    - $â†’r â† Î²_2â†’r + (1-Î²_2)â†’g^2$ Â Â _(biased second moment estimate)_
    - $â†’sÌ‚ â† â†’s / (1 - Î²_1^t)$, $â†’rÌ‚ â† â†’r / (1 - Î²_2^t)$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±}{\sqrt{â†’rÌ‚ + Îµ}}â†’sÌ‚$
</div>

---
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
<div class="algorithm">

**Input**: NN computing function $f(â†’x; â†’Î¸)$ with initial value of parameters $â†’Î¸$.  
**Input**: Learning rate $Î±$ (default 0.001), constant $Îµ$ (usually $10^{-8}$).  
**Input**: Momentum $Î²_1$ (default 0.9), momentum $Î²_2$ (default 0.999).  
**Output**: Updated parameters $â†’Î¸$.
- $â†’s â† 0$, $â†’r â† 0$, $t â† 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‡_{â†’Î¸} âˆ‘_i L(f(â†’x^{(i)}; â†’Î¸), y^{(i)})$
    - $t â† t + 1$
    - $â†’s â† Î²_1â†’s + (1-Î²_1)â†’g$   Â Â  _(biased first moment estimate)_
    - $â†’r â† Î²_2â†’r + (1-Î²_2)â†’g^2$ Â Â _(biased second moment estimate)_
    - $Î±_t â† Î± \sqrt{1 - Î²_2^t} / (1-Î²_1^t)$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±_t}{\sqrt{â†’r + Îµ}}â†’s$
</div>

---
# Adam Bias Correction

After $t$ steps, we have
$$â†’r_t = (1 - Î²_2) âˆ‘_{i=1}^t Î²_2^{t-i}â†’g_i^2.$$

Assuming that the second moment $ğ”¼[â†’g_i^2]$ is stationary, we have
$$\begin{aligned}
ğ”¼[â†’r_t] &=  ğ”¼\left[(1 - Î²_2) âˆ‘_{i=1}^t Î²_2^{t-i}â†’g_i^2\right] \\
        &=  ğ”¼[â†’g_t^2] â‹… (1 - Î²_2) âˆ‘_{i=1}^t Î²_2^{t-i} \\
        &=  ğ”¼[â†’g_t^2] â‹… (1 - Î²_2^t)
\end{aligned}$$

and analogously for correction of $â†’s$.

---
# Adaptive Optimizers Animations

![w=50%,h=center](optimizers-1.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-2.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-3.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-4.gif)

---
section: LR Schedules
# Learning Rate Schedules

Even if RMSProp and Adam are adaptive, they still usually require carefully tuned
decreasing learning rate for top-notch performance.

~~~
- _Exponential decay_: learning rate is multiplied by a constant each
  batch/epoch/several epochs.

  - $Î± = Î±_\textrm{initial} â‹… c^t$
  - Often used for convolutional networks (image recognition etc.).

~~~
- _Polynomial decay_: learning rate is multiplied by some polynomial of $t$.
~~~
  - _Inverse time decay_ uses $Î± = Î±_\textrm{initial} â‹… \frac{1}{t}$ and has
    theoretical guarantees of convergence, but is usually too fast for deep
    neural networks.
~~~
  - _Inverse-square decay_ uses $Î± = Î±_\textrm{initial} â‹… \frac{1}{\sqrt{t}}$
    and is currently used by best machine translation models.

~~~
- Cosine decay, restarts, warmup, â€¦

~~~
The `tf.optimizers.schedules` offers several such learning rate schedules,
which can be passed to any Keras optimizer directly as a learning rate.
