title: NPFL114, Lecture 5
class: title, langtech, cc-by-nc-sa

# Convolutional Neural Networks II

## Milan Straka

### April 01, 2019

---
section: Howto
# Designing and Training Neural Networks

Designing and training a neural network is not a one-shot action,
but instead an iterative procedure.

~~~
- When choosing hyperparameters, it is important to verify that the model
  does not underfit and does not overfit.

~~~
- Underfitting can be checked by increasing model capacity or training longer.

~~~
- Overfitting can be tested by observing train/dev difference and by trying
  stronger regularization.

~~~
Specifically, this implies that:
- We need to set number of training epochs so that training loss/performance
  no longer increases at the end of training.

~~~
- Generally, we want to use a large batchsize that does not slow us down too
  much (GPUs sometimes allow larger batches without slowing down training).
  However, with increasing batch size we need to increase learning rate, which
  is possible only to some extent. Also, small batch size sometimes work
  as regularization (especially for vanilla SGD algorithm).

---
# Loading and Saving Models

- Using `tf.keras.Model.save`, both the architecture and model weights are
  saved. But saving the architecture is currently quite brittle:

  - `tf.keras.layers.InputLayer` does not work correctly
  - object losses (inherited from `tf.losses.Loss`) cannot be loaded
  - TensorFlow specific functions (not in `tf.keras.layers`) works only
    sometimes
  - …

~~~
  Of course, the bugs are being fixed.

~~~
- Using `tf.keras.Model.save_weights`, only the weights of the model are
  saved. If the model is constructed again by the script (which usually
  required specifying the same hyperparameters as during model training),
  weights can be loaded using `tf.keras.Model.load_weights`.

---
# Main Takeaways From Previous Lecture

- Convolutions can provide

  - local interactions in spacial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3×3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double number of channels.

~~~
- Final fully connected layers are not needed, global average pooling
  is usually enough.

~~~
- Batch normalization is a great regularization method for CNNs.
---
section: ResNet
# ResNet – 2015 (3.6% error)

![w=95%,h=center](../04/resnet_depth_effect.pdf)

---
# ResNet – 2015 (3.6% error)

![w=90%,h=center](../04/resnet_block.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%](../04/resnet_block_reduced.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%](../04/resnet_architecture.pdf)

---
# ResNet – 2015 (3.6% error)

![w=42%,mw=50%,h=center,f=left](../04/resnet_overall.pdf)

~~~
The residual connections cannot be applied directly when
number of channels increase.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1×1$ convolution is used on the projections to match the
required number of channels.

---
# ResNet – 2015 (3.6% error)

![w=100%,v=middle](../04/resnet_residuals.pdf)

---
# ResNet – 2015 (3.6% error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
class: middle
# ResNet – 2015 (3.6% error)

![w=49%](../04/resnet_validation.pdf)
![w=49%](../04/resnet_testing.pdf)

---
section: ResNet Modifications
# WideNet

![w=100%,v=middle](widenet_block.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors do not consider bottleneck blocks. Instead, they experiment with
  different _block types_, e.g., $B(1, 3, 1)$ of $B(3, 3)$

![w=50%](widenet_ablation_blocks.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors evaluate various _widening factors_ $k$

![w=50%](widenet_ablation_width.pdf)

---
# WideNet

![w=40%,f=right](widenet_architecture.pdf)

- Authors measure the effect of _dropping out_ inside the residual block
  (but not the residual connection itself)

![w=50%](widenet_ablation_dropout.pdf)
![w=70%,h=center](widenet_curves.pdf)

---
# WideNet – CIFAR Results

![w=70%,h=center](widenet_cifar.pdf)

---
# DenseNet

![w=100%](densenet_overview_2.pdf)
![w=45%,h=center](densenet_overview.pdf)

---
# DenseNet – Architecture

![w=100%](densenet_architecture.pdf)

---
class: middle
# DenseNet – Results

![w=60%](densenet_results.pdf)![w=40%](densenet_comparison.pdf)

---
# PyramidNet

![w=100%,v=middle](pyramidnet_blocks.pdf)

---
# PyramidNet – Growth Rate

![w=70%,h=center](pyramidnet_growth_rate.pdf)

In architectures up until now, number of filters doubled when spacial
resolution was halved.

~~~
Such exponential growth would suggest gradual widening rule
$D_k = \lfloor D_{k-1} ⋅ α^{1/N}\rfloor$.

~~~
However, the authors employ a linear widening rule
$D_k = \lfloor D_{k-1} + α/N\rfloor$, where $D_k$ is number of filters
in the $k$-th out of $N$ convolutional block and $α$ is number of filters
to add in total.

---
# PyramidNet – Residual Connections

No residual connection can be a real identity – the authors propose
to zero-pad missing channels, where the zero-pad channels correspond
to newly computed features.


![w=85%,h=center](pyramidnet_residuals.pdf)

---
class: middle
# PyramidNet – CIFAR Results

![w=70%](pyramidnet_cifar.pdf)![w=30%](pyramidnet_architecture.pdf)

---
# ResNeXt

![w=80%,h=center](resnext_block.pdf)

---
# ResNeXt

![w=50%,h=center](resnext_architecture.pdf)

---
# ResNeXt

![w=100%,v=middle](resnext_training.pdf)

---
section: CNN Regularization
# Deep Networks with Stochastic Depth

![w=70%,h=center](stochastic_depth_illustration.pdf)

We drop a whole block (but not the residual connection) with probability $1-p_l$.
During inference, we multiply the block output by $p_l$ to compensate.

~~~
All $p_l$ can be set to a constant, but more effective is to use a simple linear
decay $p_l = 1 - l/L(1-p_L)$ where $p_L$ is the final probability of the last layer,
motivated by the intuition that the initial blocks extract low-level features
utilized by the later layers and should therefore be present.

---
# Deep Networks with Stochastic Depth

![w=100%,v=middle](stochastic_depth_ablations.pdf)

---
# Deep Networks with Stochastic Depth

![w=100%,v=middle](stochastic_depth_cifar.pdf)

---
# Cutout

![w=60%,h=center](cutout_examples.pdf)

Drop $16×16$ square in the input image, with randomly chosen center.
The pixels are replaced by a their mean value from the dataset.

---
# Cutout

![w=80%,h=center](cutout_ablations.pdf)
![w=80%,h=center](cutout_results.pdf)

---
# DropBlock

![w=100%,v=middle](dropblock_motivation.pdf)

---
# DropBlock

![w=70%,h=center](dropblock_algorithm.pdf)

---
# DropBlock

![w=100%,v=middle](dropblock_imagenet.pdf)

---
section: Image Detection
class: middle, center
# Beyond Image Classification

# Beyond Image Classification

---
# Beyond Image Classification

![w=70%,f=right](../01/object_detection.pdf)

- Object detection (including location)
<br clear="both">
![w=70%,f=right](../01/image_segmentation.pdf)

- Image segmentation
<br clear="both">
![w=70%,f=right](../01/human_pose_estimation.pdf)

- Human pose estimation

---
# Fast R-CNN

- Start with a network pre-trained on ImageNet (VGG-16 is used in the original
  paper).

## RoI Pooling
- Crucial for fast performance.
- The last max-pool layer ($14×14 → 7×7$ in VGG) is replaced by a RoI pooling
  layer, producing output of the same size. For each output sub-window we
  max-pool the corresponding values in the output layer.
- Two sibling layers are added, one predicting $K+1$ categories and the other
  one predicting 4 bounding box parameters for each of $K$ categories.

---
# Fast R-CNN

![w=100%,v=middle](fast_rcnn.jpg)

---
# Fast R-CNN

The bounding box is parametrized as follows. Let $x_r, y_r, w_r, h_r$ be
center coordinates and width and height of the RoI, and let $x, y, w, h$ be
parameters of the bounding box. We represent them as follows:
$$\begin{aligned}
t_x &= (x - x_r)/w_r, & t_y &= (y - y_r)/h_r \\
t_w &= \log (w/w_r), & t_h &= \log (h/h_r)
\end{aligned}$$

~~~
Usually a $\textrm{smooth}_{L_1}$ loss, or _Huber loss_, is employed for bounding box parameters
$$\textrm{smooth}_{L_1}(x) = \begin{cases}
  0.5x^2    & \textrm{if }|x| < 1 \\
  |x| - 0.5 & \textrm{otherwise}
\end{cases}$$

~~~
The complete loss is then
$$L(\hat c, \hat t,c, t) = L_\textrm{cls}(\hat c, c) + λ[c ≥ 1]
  ∑_{i ∈ \lbrace \mathrm{x, y, w, h}\rbrace} \textrm{smooth}_{L_1}(\hat t_i - t_i).$$

---
# Fast R-CNN

## Intersection over union
For two bounding boxes (or two masks) the _intersection over union_ (_IoU_)
is a ration of the intersection of the boxes (or masks) and the union
of the boxes (or masks).

~~~
## Choosing RoIs for training
During training, we use $2$ images with $64$ RoIs each. The RoIs are selected
so that $25\%$ have intersection over union (IoU) overlap with ground-truth
boxes at least 0.5; the others are chosen to have the IoU in range $[0.1, 0.5)$.

~~~
## Choosing RoIs during inference
Single object can be found in multiple RoIs. To choose the most salient one,
we perform _non-maximum suppression_ -- we ignore RoIs which have an overlap
with a higher scoring RoI of the same type, where the IoU is larger than a given
threshold (usually, 0.3 is used).

---
# Object Detection Evaluation

## Average Precision
Evaluation is performed using _Average Precision_ (_AP_).

We assume all bounding boxes (or masks) produced by a system have confidence
values which can be used to rank them. Then, for a single class, we take the
boxes (or masks) in the order of the ranks and generate precision/recall curve,
considering a bounding box correct if it has IoU at least 0.5 with any
ground-truth box.
We define _AP_ as an average of precisions for recall levels $0, 0.1, 0.2,
\ldots, 1$.

~~~
![w=55%,mw=50%,h=center](precision_recall_person.pdf)![w=55%,mw=50%,h=center](precision_recall_bottle.pdf)

---
# Faster R-CNN

For Fast R-CNN, the most time consuming part is generating the RoIs.

Therefore, Faster R-CNN jointly generates _regions of interest_ using
a _region proposal network_ and performs object detection.

~~~
![w=39%,h=center](faster_rcnn_architecture.jpg)

---
# Faster R-CNN

The region proposals are generated using a $3×3$ sliding window, with
3 different scales ($128^2$, $256^2$ and $512^2$) and 3
aspect ratios ($1:1$, $1:2$, $2:1$).

![w=70%,h=center](faster_rcnn_rpn.pdf)

---
# Faster R-CNN

![w=94%,h=center](faster_rcnn_performance.pdf)

---
section: Segmentation
# Mask R-CNN

"Straightforward" extension of Faster R-CNN able to produce image segmentation
(i.e., masks for every object).

![w=100%,mh=80%,v=middle](../01/image_segmentation.pdf)

---
# Mask R-CNN

![w=100%,v=middle](mask_rcnn_architecture.jpg)

---
# Mask R-CNN

## RoIAlign

More precise alignment is required for the RoI in order to predict the masks.
Therefore, instead of max-pooling used in the RoI pooling, RoIAlign with
bilinear interpolation is used.

![w=38%,h=center](mask_rcnn_roialign.pdf)

---
# Mask R-CNN

Masks are predicted in a third branch of the object detector.

- Usually higher resolution is needed ($14×14$ instead of $7×7$).
- The masks are predicted for each class separately.
- The masks are predicted using convolutions instead of fully connected layers.

![w=100%](mask_rcnn_heads.pdf)

---
# Mask R-CNN

![w=100%,v=middle](mask_rcnn_ablation.pdf)

---
# Mask R-CNN – Human Pose Estimation

![w=80%,h=center](../01/human_pose_estimation.pdf)

~~~
- Testing applicability of Mask R-CNN architecture.

- Keypoints (e.g., left shoulder, right elbow, …) are detected
  as independent one-hot masks of size $56×56$ with $\softmax$ output function.

~~~
![w=70%,h=center](mask_rcnn_hpe_performance.pdf)

---
section: GroupNorm
# Normalization

## Batch Normalization

Neuron value is normalized across the minibatch, and in case of CNN also across
all positions.

~~~
## Layer Normalization

Neuron value is normalized across the layer.

~~~
![w=100%](normalizations.pdf)

---
# Group Normalization

Group Normalization is analogous to Layer normalization, but the channels are
normalized in groups (by default, $G=32$).

![w=40%,h=center](normalizations.pdf)

~~~
![w=40%,h=center](group_norm.pdf)

---
# Group Normalization

![w=78%,h=center](group_norm_vs_batch_norm.pdf)

---
# Group Normalization

![w=65%,h=center](group_norm_coco.pdf)
