![:wip](Under construction)

class: title
## NPFL114, Lecture 05

# Convolutional Networks II

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Convolution Layer

The $K$ is usually called a _kernel_ or a _filter_, and we generally apply
several of them at the same time.

Consider an input image with $C$ channels. The convolution layer with
$F$ filters of width $W$, height $H$ and stride $S$ produces an output with $F$ channels
kernels of total size $W √ó H √ó C √ó F$ and is computed as
$$(‚áâI \* ‚áâK)\_{i, j, k} = ‚àë\_{m, n, o} ‚áâI\_{i\cdot S + m, j\cdot S + n, o} ‚áâK\_{m, n, o, k}.$$

--

There are multiple padding schemes, most common are:
- `valid`: we only use valid pixels, which causes the result to me smaller
- `same`: we pad original image with zero pixels so that the result is exactly
  the size of the input

---
# Convolution Layer

There are two prevalent image formats:
- `NHWC` or `channels_last`: The dimensions of the 4-dimensional image tensor
  are batch, height, width, and channels.

  The original TensorFlow format, faster on CPU.

--

- `NCHW` or `channels_first`: The dimensions of the 4-dimensional image tensor
  are batch, channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling: minor translation invariance
- Average pooling

![:pdf 80%,center](pooling.pdf)

---
class: center
# VGG ‚Äì 2014 (6.8% error)

![:pdf 62%](vgg_architecture.pdf)
![:pdf 45%](vgg_parameters.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_block.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_block_reduction.pdf)

---
class: middle
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 100%](inception_architecture.pdf)

---
# Inception (GoogLeNet) ‚Äì 2014 (6.7% error)

![:pdf 14.5%,center](inception_graph.pdf)

Also note the two auxiliary classifiers (they have weight 0.3).

---
# Batch Normalization

_Internal covariate shift_ refers to the change in the distributions
of hidden node activations due to the updates of network parameters
during training.

Let $‚Üíx = (x\_1, \ldots, x\_d)$ be $d$-dimensional input. We would like to
normalize each dimension as $$\hat x\_i = \frac{x\_i - ùîº[x\_i]}{\sqrt{\Var[x\_i]}}.$$
Furthermore, it may be advantageous to learn suitable scale $Œ≥\_i$ and shift $Œ≤\_i$ to
produce normalized value $$y\_i = Œ≥\_i\hat x\_i + Œ≤\_i.$$

---
# Batch Normalization

Consider a mini-batch of $m$ examples $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$.

_Batch normalizing transform_ of the mini-batch is the following transformation.

.algorithm[
**Inputs**: Mini-batch $(‚Üíx^{(1)}, \ldots, ‚Üíx^{(m)})$, $Œµ ‚àà ‚Ñù$<br>
**Outputs**: Normalized batch $(‚Üíy^{(1)}, \ldots, ‚Üíy^{(m)})$
- $‚ÜíŒº ‚Üê \frac{1}{m} ‚àë\_{i = 1}^m ‚Üíx^{(i)}$
- $‚ÜíœÉ^2 ‚Üê \frac{1}{m} ‚àë\_{i = 1}^m (‚Üíx^{(i)} - Œº)^2$
- $\hat‚Üíx^{(i)} ‚Üê (‚Üíx^{(i)} - ‚ÜíŒº) / \sqrt{œÉ^2 + Œµ}$
- $‚Üíy^{(i)} ‚Üê ‚ÜíŒ≥ \hat‚Üíx^{(i)} + ‚ÜíŒ≤$
]

--

Batch normalization is commonly added just before a nonlinearity. Therefore, we
replace $f(‚áâW‚Üíx + ‚Üíb)$ by $f(\textit{BN}(‚áâW‚Üíx))$.

--

During inference, $‚ÜíŒº$ and $‚ÜíœÉ^2$ are fixed. They are either precomputed
after training on the whole training data, or an exponential moving average is
updated during training.

---
class: middle
# Inception with BatchNorm (4.8% error)

![:pdf 100%](inception_batchnorm.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:image 55%](inception3_conv5.png)
![:image 35%](inception3_conv3.png)

---
class: middle
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 32%](inception3_inception_a.pdf)
![:pdf 32%](inception3_inception_b.pdf)
![:pdf 32%](inception3_inception_c.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 70%](inception3_architecture.pdf)

---
class: middle, center
# Inception v2 and v3 ‚Äì 2015 (3.6% error)

![:pdf 70%](inception3_ablation.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_depth_effect.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_block.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_block_reduced.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_architecture.pdf)

---
class: center
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 30%](resnet_overall.pdf)

---
class: middle
# ResNet ‚Äì 2015 (3.6% error)

![:pdf 100%](resnet_residuals.pdf)

---
class: middle, full
# ResNet ‚Äì 2015 (3.6% error)

![:image 100%](../02/nn_loss.jpg)

---
class: middle, full
# WideNet ‚Äì 2016

![:pdf 100%](widenet_block.pdf)

---
class: middle
# ResNeXt ‚Äì 2016

![:pdf 100%](resnext_block.pdf)

---
class: center, middle
# ResNeXt ‚Äì 2016

![:pdf 70%](resnext_architecture.pdf)

---
class: middle, full
# ResNeXt ‚Äì 2016

![:pdf 100%](resnext_training.pdf)

---
class: center, middle
# NasNet ‚Äì 2017

![:pdf 47%](nasnet_overall.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_rnn_controller.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_blocks.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](nasnet_performance.pdf)

---
class: middle
# NasNet ‚Äì 2017

![:pdf 100%](../01/nas_net.pdf)

---
class: middle, center
# Beyond Image Classification

# Beyond Image Classification

---
# Beyond Image Classification

- Transfer learning

--

- Convolutions can be applied to other dimensions than 2:

--

    - 1D: speech, text (e.g., text classification)

--
    - 3D: the dimensions can be either spacial (e.g., 3D object recognition)
      or temporal-spacial (e.g., videos)

---
# Beyond Image Classification

- Object detection (including location)
![:pdf 100%](object_detection.pdf)

--
- Image segmentation
![:pdf 100%](image_segmentation.pdf)

--
- Human pose estimation
![:pdf 100%](human_pose_estimation.pdf)

---
class: middle
# Fast R-CNN

![:image 100%](fast_rcnn.jpg)

---
class: center
# Faster R-CNN

![:image 65%](faster_rcnn_architecture.jpg)

---
class: middle
# Faster R-CNN

![:pdf 100%](faster_rcnn_rpn.pdf)

---
class: middle
# Mask R-CNN

![:image 100%](mask_rcnn_architecture.jpg)

---
# Mask R-CNN

![:pdf 40%,center](mask_rcnn_roialign.pdf)

---
class: middle
# Mask R-CNN

![:pdf 100%](mask_rcnn_heads.pdf)

---
class: middle, full
# Mask R-CNN

![:pdf 100%](mask_rcnn_ablation.pdf)

---
# Mask R-CNN ‚Äì Human Pose Estimation

![:pdf 100%](human_pose_estimation.pdf)

--

- Testing applicability of Mask R-CNN architecture.

- Keypoints (e.g., left shoulder, right elbow, ‚Ä¶) are detected
  as independent one-hot masks of size $56√ó56$ with $\softmax$ output function.

--

![:pdf 100%](mask_rcnn_hpe_performance.pdf)
