class: title
## NPFL114, Lecture 08

# Recurrent Neural Networks II,<br>Word Embeddings

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Recurrent Neural Networks

## Single RNN cell

![:pdf 25%,center](../07/rnn_cell.pdf)

## Unrolled RNN cells

![:pdf 90%,center](../07/rnn_cell_unrolled.pdf)

---
# Challenge of Long-term Dependencies

Consider a RNN cell which given an input $â†’x^{(t)}$ and previous state
$â†’s^{(t-1)}$ computes the new state as
$$â†’s^{(t)} = f(â†’s^{(t-1)}, â†’x^{(t)}; â†’Î¸).$$

--

RNN cells generally suffer a lot from vanishing/exploding gradients (_the
challenge of long-term dependencies_), a problem attributed to the fact
that _same_ function is iteratively applied many times.

---
# Long Short-Term Memory

Hochreiter & Schmidhuber (1997) suggested that to enforce
_constant error flow_, we would like
$$f' = â†’1.$$

They propose to achieve that by a _constant error carrousel_.

![:pdf 90%,center](lstm_cec.pdf)

---
# Long Short-Term Memory

They also propose an _input_ and _output_ gates which control the flow
of information into and out of the carrousel (_memory cell_ $â†’c\_t$).

![:pdf 70%,center](lstm_input_output_gates.pdf)

$$\begin{aligned}
  â†’i\_t & â† Ïƒ(â‡‰W^i â†’x\_t + â‡‰V^i â†’h\_{t-1} + â†’b^i) \\\
  â†’o\_t & â† Ïƒ(â‡‰W^o â†’x\_t + â‡‰V^o â†’h\_{t-1} + â†’b^o) \\\
  â†’c\_t & â† â†’c\_{t-1} + â†’i\_t \cdot \tanh(â‡‰W^y â†’x\_t + â‡‰V^y â†’h\_{t-1} + â†’b^y) \\\
  â†’h\_t & â† â†’o\_t \cdot \tanh(â†’c\_t)
\end{aligned}$$

---
# Long Short-Term Memory

Later in Gers, Schmidhuber & Cummins (1999) a possibility to _forget_
information from memory cell $â†’c\_t$ was added.

![:pdf 50%,center](lstm_input_output_forget_gates.pdf)

$$\begin{aligned}
  â†’i\_t & â† Ïƒ(â‡‰W^i â†’x\_t + â‡‰V^i â†’h\_{t-1} + â†’b^i) \\\
  â†’f\_t & â† Ïƒ(â‡‰W^f â†’x\_t + â‡‰V^f â†’h\_{t-1} + â†’b^f) \\\
  â†’o\_t & â† Ïƒ(â‡‰W^o â†’x\_t + â‡‰V^o â†’h\_{t-1} + â†’b^o) \\\
  â†’c\_t & â† â†’f\_t \cdot â†’c\_{t-1} + â†’i\_t \cdot \tanh(â‡‰W^y â†’x\_t + â‡‰V^y â†’h\_{t-1} + â†’b^y) \\\
  â†’h\_t & â† â†’o\_t \cdot \tanh(â†’c\_t)
\end{aligned}$$

---
# Gated Recurrent Unit

_Gated recurrent unit (GRU)_ was proposed by Cho et al. (2014) as
a simplification of LSTM. The main differences are
- no memory cell
- forgetting and updating tied together

![:pdf 70%,center](gru.pdf)

---
# Gated Recurrent Unit

![:pdf 70%,center](gru.pdf)

$$\begin{aligned}
  â†’r\_t & â† Ïƒ(â‡‰W^r â†’x\_t + â‡‰V^r â†’h\_{t-1} + â†’b^r) \\\
  â†’u\_t & â† Ïƒ(â‡‰W^u â†’x\_t + â‡‰V^u â†’h\_{t-1} + â†’b^u) \\\
  \hatâ†’h\_t & â† \tanh(â‡‰W^h â†’x\_t + â‡‰V^h (â†’r\_t \cdot â†’h\_{t-1}) + â†’b^h) \\\
  â†’h\_t & â† â†’u\_t \cdot â†’h\_{t-1} + (1 - â†’u\_t) \cdot \hatâ†’h\_t
\end{aligned}$$

---
class: middle, center
# Word Embeddings

# Word Embeddings

---
# Word Embeddings

One-hot encoding considers all words to be independent of each other.

However, words are not independent â€“ some are more similar than others.

Ideally, we would like some kind of similarity in the space of the word
representations.

--

## Distributed Representation
The idea behind distributed representation is that objects can
be represented using a set of common underlying factors.

--

We therefore represent words as fixed-size _embeddings_ into $â„^d$ space,
with the vector elements playing role of the common underlying factors.

---
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

--

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

**Words that are used in the same contexts tend to have similar meanings**.

The distributional hypothesis is usually attributed to Firth (1957).

---
# Word2Vec

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

![:pdf 70%,center](word2vec.pdf)

---
class: middle
# Word2Vec

![:pdf 100%](word2vec_composability.pdf)

---
# Word2Vec â€“ SkipGram Model

![:pdf 70%,center,margin:30px 0](word2vec.pdf)

Considering input word $w\_i$ and output $w\_o$, the Skip-gram model defines
$$p(w\_o | w\_i) â‰ \frac{e^{â‡‰W\_{w\_o}^\top â‡‰V\_{w\_i}}}{âˆ‘\_w e^{â‡‰W\_w^\top â‡‰V\_{w\_i}}}.$$

---
# Word2Vec â€“ Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n\_1, n\_2, \ldots, n\_L$, we define
$$p\_\textrm{HS}(w | w\_i) â‰ âˆ\_{j=1}^{L-1} Ïƒ(\textrm{[+1 if }n\_{j+1}\textrm{  is right child else -1]} \cdot â‡‰W\_{n\_j}^\top â‡‰V\_{w\_i}).$$

---
# Word2Vec â€“ Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

We could also only sample the _negative examples_ instead of training all of
them.

This gives rise to the following _negative sampling_ objective:
$$l\_\textrm{NEG}(w\_o, w\_i) â‰ \log Ïƒ(â‡‰W\_{w\_o}^\top â‡‰V\_{w\_i}) + âˆ‘\_{j=1}^k ğ”¼\_{w\_j âˆ¼ P(w)} \log Ïƒ(-â‡‰W\_{w\_j}^\top â‡‰V\_{w\_i}).$$

--

For $P(w)$, both uniform and unigram distribution $U(w)$ work, but $$U(w)^{3/4}$$
outperforms them significantly.

---
class: middle, center
# Basic NLP Processing

# Basic NLP Processing

---
class: center
# Bidirectional RNN

![:pdf 80%](bidirectional_rnn.pdf)

---
class: middle, center
# Word Embeddings for Unknown Words

# Word Embeddings for Unknown Words

---
class: center
# Recurrent Character-level WEs

![:pdf 62%](cle_rnn.pdf)

---
class: middle
# Recurrent Character-level WEs

![:pdf 100%](cle_rnn_examples.pdf)

---
class: middle
# Recurrent Character-level WEs

![:image 100%](cle_rnn_gru.jpg)

---
class: center
# Convolutional Character-level WEs

![:pdf 75%](cle_cnn.pdf)

---
class: middle
# Convolutional Character-level WEs

![:pdf 100%](cle_cnn_examples.pdf)

---
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

--

The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K âˆ¼ 10^6$ is used)

---
class: middle
# Charagram WEs

![:pdf 100%](cle_charagram_examples.pdf)

---
class: center
# Charagram WEs

![:pdf 66%](cle_charagram_ngrams.pdf)

---
# Character-level WE Implementation

## Training

- Generate unique words per batch.

- Process the unique words in the batch.

- Copy the resulting embeddings suitably in the batch.

--

## Inference

- We can cache character-level word embeddings during inference.

---
# TF â€“ RNN and Variable Length Sequences

## tf.nn.dynamic\_rnn
```python
outputs, state = tf.nn.dynamic_rnn(cell,
                                   inputs,
                                   dtype=None,
                                   initial_state=None,
*                                  sequence_length=None,
                                   ...)
```

--

The `sequence_length` parameter allows sequences of different length. If used,
the `outputs` past the sequence lengths are zeros and the `state` is the state
after processing last element of each sequence.

---
# TF â€“ RNN and Variable Length Sequences

## Losses
```python
outputs, state = tf.nn.sparse_softmax_cross_entropy(
  labels,
  logits,
* weights=1.0,
  ...)
```
--

The `weights` can be used to mask some of the labels and logits.

## Generating the weights
```python
tf.sequence_masks(lengths, maxlen=None, dtype=tf.bool)

weights = tf.sequence_masks(lengths, dtype=tf.float32)
```

---
# TF â€“ Bidirectional RNNs

## tf.nn.bidirectional\_dynamic\_rnn
```python
(outputs_fwd, outputs_bwd), (state_fwd, state_bwd) = \
  tf.nn.bidirectional_dynamic_rnn(cell_fwd,
                                  cell_bwd,
                                  inputs,
                                  dtype=None,
                                  initial_state_fwd=None,
                                  initial_state_bwd=None,
                                  sequence_length=None,
                                  time_major=False,
                                  ...)
```
