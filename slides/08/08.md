title: NPFL114, Lecture 8
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }

# CRF, CTC, Word2Vec

## Milan Straka

### April 4, 2022

---
section: CRF
class: middle, center
# Structured Prediction

# Structured Prediction

---
# Structured Prediction

Consider generating a sequence of $y_1, \ldots, y_N âˆˆ Y^N$ given input
$â†’x_1, \ldots, â†’x_N$.

~~~
Predicting each sequence element independently models the distribution $P(y_i | â‡‰X)$.

![w=40%,h=center](labeling_independent.svgz)

~~~
However, there may be dependencies among the $y_i$ themselves, which
is difficult to capture by independent element classification.

---
# Maximum Entropy Markov Models

We might model the dependencies by assuming that the output sequence is
a Markov chain, and model it as
$$P(y_i | â‡‰X, y_{i-1}).$$

~~~
Each label would be predicted by a softmax from the hidden state and the
_previous label_.
![w=40%,h=center](labeling_memm.svgz)

~~~
The decoding can be then performed by a dynamic programming algorithm.

---
# Maximum Entropy Markov Models

However, MEMMs suffer from a so-called **label bias** problem. Because the
probability is factorized, each $P(y_i | â‡‰X, y_{i-1})$ is a distribution
and **must sum to one**.

~~~
Imagine there was a label error during prediction. In the next step, the model
might â€œrealizeâ€ that the previous label has very low probability of being
followed by any label â€“ however, it cannot express this by setting the
probability of all following labels to a low value, it has to â€œconserve the massâ€.

---
# Conditional Random Fields

Let $G = (V, E)$ be a graph such that $â†’y$ is indexed by vertices of $G$.
Then $(â‡‰X, â†’y)$ is a **conditional random field**, if the random variables $â†’y$
conditioned on $â‡‰X$ obey the Markov property with respect to the graph, i.e.,
$$P\big(y_i | â‡‰X, \{y_j \,|\, âˆ€j â‰  i\}\big) = P\big(y_i | â‡‰X, \{y_j \,|\, âˆ€j: (i, j) âˆˆ E\}\big).$$

~~~
By a _fundamental theorem of random fields (the Hammersleyâ€“Clifford theorem)_, the
density of a conditional random field can be factorized over the _cliques_
(complete subgraphs) of the graph $G$:
$$P(â†’y | â‡‰X) = âˆ_{\textrm{clique~}C\textrm{~of~}G} P(â†’y_C|â‡‰X).$$

---
# Linear-Chain Conditional Random Fields (CRF)

Most often, we assume that dependencies of $â†’y$, conditioned on $â‡‰X$, form a chain.

![w=40%,h=center](labeling_crf.svgz)

~~~
Then, the cliques are _nodes_ and _edges_, and we can factorize the
probability as:
$$P(â†’y | â‡‰X) âˆ \exp\bigg(âˆ‘_{i=1}^N \log P(y_i | â‡‰X) + âˆ‘_{i=2}^N \log P(y_i, y_{i-1})\bigg).$$

---
# Linear-Chain Conditional Random Fields (CRF)

Linear-chain Conditional Random Field, usually abbreviated only to CRF, acts as
an output layer. It can be considered an extension of softmax â€“ instead of
a sequence of independent softmaxes, it is a sentence-level softmax, with
additional weights for neighboring sequence elements.

We start by defining a score of a label sequence $â†’y$ as
$$s(â‡‰X, â†’y; â†’Î¸, â‡‰A) = f_{â†’Î¸}(y_1 | â‡‰X) + âˆ‘\nolimits_{i=2}^N \big(â‡‰A_{y_{i-1}, y_i} + f_{â†’Î¸}(y_i | â‡‰X)\big)$$
~~~
and define the probability of a label sequence $â†’y$ using $\softmax$:
$$p(â†’y | â‡‰X) = \softmax_{â†’z âˆˆ Y^N}\big(s(â‡‰X, â†’z)\big)_{â†’y}.$$

~~~
For cross-entropy (and also to avoid underflow), we need a logarithm of the probability:
$$\begin{gathered}
  \log p(â†’y | â‡‰X) = s(â‡‰X, â†’y) - \operatorname{logsumexp}_{â†’z âˆˆ Y^N}\big(s(â‡‰X, â†’z)\big),~\textrm{where} \\
  \textstyle\operatorname{logsumexp}_x\big(f(x)\big) = \log\big(âˆ‘\nolimits_x e^{f(x)}\big).
\end{gathered}$$

---
# Linear-Chain Conditional Random Fields (CRF)

## Computation

We can compute $p(â†’y | â‡‰X)$ efficiently using dynamic programming. We denote
$Î±_t(k)$ the logarithmic probability of all $t$-element sequences with the
last label $y$ being $k$.

~~~
The core idea is the following:

![w=40%,h=center](crf_composability.svgz)

$$Î±_t(k) = f_{â†’Î¸}(y_t=k | â‡‰X) + \operatorname{logsumexp}_{jâˆˆY} \big(Î±_{t-1}(j) + â‡‰A_{j, k}\big).$$

~~~
For efficient implementation, we use the fact that
$$\begin{gathered}
  \ln(a+b) = \ln a + \ln (1 + e^{\ln b - \ln a}),~\textrm{so} \\
  \textstyle\operatorname{logsumexp}_x\big(f(x)\big) = \max_x\big(f(x)\big) + \log(âˆ‘\nolimits_x e^{f(x) - \max_x(f(x))}).
\end{gathered}$$

---
# Conditional Random Fields (CRF)

<div class="algorithm">

**Inputs**: Network computing $f_{â†’Î¸}(y_t = k | â‡‰X)$, an unnormalized probability
of output sequence element probability being $k$ at time $t$.<br>
**Inputs**: Transition matrix $â‡‰A âˆˆ â„^{YÃ—Y}$.<br>
**Inputs**: Input sequence $â‡‰X$ of length $N$, gold labeling $â†’g âˆˆ Y^N$.<br>
**Outputs**: Value of $\log p(â†’g | â‡‰X)$.<br>
**Time Complexity**: $ğ“(N â‹… Y^2)$.

~~~
- For $t = 1, \ldots, N$:
  - For $k = 1, \ldots, Y:$
    - $Î±_t(k) â† f_{â†’Î¸}(y_t=k | â‡‰X)$
    - If $t > 1$:
      - $Î±_t(k) â† Î±_t(k) + \operatorname{logsumexp}\big(Î±_{t-1}(j) + â‡‰A_{j, k} \,\big|\, j = 1, \ldots, Y\big)$
~~~
- Return $âˆ‘_{t=1}^N f_{â†’Î¸}(y_t=g_t | â‡‰X) + âˆ‘_{t=2}^N â‡‰A_{g_{t-1}, g_t} - \operatorname{logsumexp}_{k=1}^Y(Î±_N(k))$
</div>

---
# Conditional Random Fields (CRF)

## Decoding

We can perform decoding optimally, by using the same algorithm, only replacing
$\operatorname{logsumexp}$ with $\max$, and tracking where the maximum was attained.

~~~
## Applications

![w=65%,f=right](../07/cle_rnn_gru.png)

The CRF output layer is useful for **span labeling** tasks, like
- named entity recognition,
- dialog slot filling.

It can be also useful for image segmentation.

---
section: CTC
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y_1, \ldots, y_M$ given input
$â†’x_1, \ldots, â†’x_N$, but this time $M â‰¤ N$, and there is no explicit alignment
of $â†’x$ and $y$ in the gold data.

~~~
![w=100%,mh=90%,v=middle](ctc_example.svgz)

---
# Connectionist Temporal Classification

We enlarge the set of the output labels by a â€“ (**blank**), and perform a classification for every
input element to produce an **extended labeling**. We then post-process it by the
following rules (denoted as $ğ“‘$):
1. We collapse multiple neighboring occurrences of the same symbol into one.
2. We remove the blank â€“.

~~~
Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

~~~
Denoting the probability of label $l$ at time $t$ as $p_l^t$, we define
$$Î±^t(s) â‰ âˆ‘_{\substack{\textrm{extended}\\\textrm{labelings~}â†’Ï€:\\ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}}} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'}.$$

---
# CRF and CTC Comparison

In CRF, we normalize the whole sentences, therefore we need to compute
unnormalized probabilities for all the (exponentially many) sentences. Decoding
can be performed optimally.

~~~
In CTC, we normalize per each label. However, because we do not have explicit
alignment, we compute probability of a labeling by summing probabilities
of (generally exponentially many) extended labelings.

---
# Connectionist Temporal Classification

## Computation

When aligning an extended labeling to a regular one, we need to consider
whether the extended labeling ends by a _blank_ or not. We therefore define
$$\begin{aligned}
  Î±_-^t(s) &â‰ âˆ‘_{\substack{\textrm{extended}\\\textrm{labelings~}â†’Ï€:\\ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}, Ï€_t=-}} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'} \\
  Î±_*^t(s) &â‰ âˆ‘_{\substack{\textrm{extended}\\\textrm{labelings~}â†’Ï€:\\ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}, Ï€_tâ‰ -}} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'}

\end{aligned}$$
and compute $Î±^t(s)$ as $Î±_-^t(s) + Î±_*^t(s)$.

---
# Connectionist Temporal Classification

## Computation â€“ Initialization

![w=35%,f=right](ctc_computation.svgz)

We initialize $Î±$ as follows:
- $Î±_-^1(0) â† p_-^1$
- $Î±_*^1(1) â† p_{y_1}^1$

~~~
## Computation â€“ Induction Step

We then proceed recurrently according to:
- $Î±_-^t(s) â† p_-^t \big(Î±_*^{t-1}(s) + Î±_-^{t-1}(s)\big)$
~~~
- $Î±_*^t(s) â† \begin{cases}
  p_{y_s}^t\big(Î±_*^{t-1}(s) + Î±_-^{t-1}(s-1) + Î±_*^{t-1}(s-1)\big)\textrm{, if }y_sâ‰ y_{s-1}\\
  p_{y_s}^t\big(Î±_*^{t-1}(s) + Î±_-^{t-1}(s-1)\big)\textrm{, if }y_s=y_{s-1}\\
\end{cases}$

---
section: CTCDecoding
# CTC Decoding

Unlike CRF, we cannot perform the decoding optimally.

~~~
The key observation is that while an optimal extended labeling can be extended
into an optimal labeling of a larger length, the same does not apply to
a regular (non-extended) labeling. The problem is that regular labeling
corresponds to many extended labelings, which are modified each in a different
way during an extension of the regular labeling.

~~~
![w=75%,h=center](ctc_decoding.svgz)

---
# CTC Decoding

## Beam Search

~~~
To perform a beam search, we keep $k$ best **regular** (non-extended) labelings.
Specifically, for each regular labeling $â†’y$ we keep both $Î±^t_-(â†’y)$ and
$Î±^t_*(â†’y)$, which are probabilities of all (modulo beam search) extended
labelings of length $t$ which produce the regular labeling $â†’y$; we therefore
keep $k$ regular labelings with the highest $Î±^t_-(â†’y) + Î±^t_*(â†’y)$.

~~~
To compute the best regular labelings for longer prefix of extended labelings,
for each regular labeling in the beam we consider the following cases:
~~~
- adding a _blank_ symbol, i.e., contributing to $Î±^{t+1}_-(â†’y)$ both from
  $Î±^t_-(â†’y)$ and $Î±^t_*(â†’y)$;
~~~
- adding a non-blank symbol, i.e., contributing to $Î±^{t+1}_*(â†’â‹…)$ from
  $Î±^t_-(â†’y)$ and to possibly different $Î±^{t+1}_*(â†’â‹…)$ from $Î±^t_*(â†’y)$.

~~~
Finally, we merge the resulting candidates according to their regular labeling, and
keep only the $k$ best.

---
section: Word2Vec
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

~~~

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

> **Words that are used in the same contexts tend to have similar meanings**.

~~~
The distributional hypothesis is usually attributed to Firth (1957):
> _You shall know a word by a company it keeps._

---
# Word2Vec

![w=70%,h=center](word2vec.svgz)

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

---
# Word2Vec

![w=100%](word2vec_composability.svgz)

---
# Word2Vec â€“ SkipGram Model

![w=50%,h=center,mh=64%](word2vec.svgz)

Considering input word $w_i$ and output $w_o$, the Skip-gram model defines
$$p(w_o | w_i) â‰ \frac{e^{â‡‰V_{w_i}^\top â‡‰W_{w_o}}}{âˆ‘_w e^{â‡‰V_{w_i}^\top â‡‰W_w}}.$$
After training, the final embeddings are the rows of the $â‡‰V$ matrix.

---
# Word2Vec â€“ Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n_1, n_2, \ldots, n_L$, we define
$$p_\textrm{HS}(w | w_i) â‰ âˆ_{j=1}^{L-1} Ïƒ(\textrm{[+1 if }n_{j+1}\textrm{  is right child else -1]} \cdot â‡‰V_{w_i}^\top â‡‰W_{n_j}).$$

---
# Word2Vec â€“ Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

~~~
We could also only sample several _negative examples_. This gives rise to the
following _negative sampling_ objective (instead of just summing all the
sigmoidal losses):
$$l_\textrm{NEG}(w_o, w_i) â‰ -\log Ïƒ(â‡‰V_{w_i}^\top â‡‰W_{w_o}) - âˆ‘_{j=1}^k ğ”¼_{w_j âˆ¼ P(w)} \log \big(1 - Ïƒ(â‡‰V_{w_i}^\top â‡‰W_{w_j})\big).$$

~~~
The usual value of negative samples $k$ is 5, but it can be even 2 for extremely
large corpora.

~~~
Each expectation in the loss is estimated using a single sample.

~~~
For $P(w)$, both uniform and unigram distribution $U(w)$ work, but
$$U(w)^{3/4}$$
outperforms them significantly (this fact has been reported in several papers by
different authors).

---
section: CLEs
# Recurrent Character-level WEs

![w=80%,h=center](../07/cle_rnn_examples.svgz)

---
# Convolutional Character-level WEs

![w=100%](../07/cle_cnn_examples.svgz)

---
section: Subword Embeddings
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

~~~
The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept;
~~~
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K âˆ¼ 10^6$ is used).

---
# Charagram WEs

![w=100%,v=middle](cle_charagram_examples.svgz)

---
# Charagram WEs

![w=48%,h=center](cle_charagram_ngrams.svgz)

---
# FastText

The word2vec enriched with subword embeddings is implemented in publicly
available `fastText` library https://fasttext.cc/.

~~~
Pre-trained embeddings for 157 languages (including Czech) trained on
Wikipedia and CommonCrawl are also available at
https://fasttext.cc/docs/en/crawl-vectors.html.
