title: NPFL114, Lecture 13
class: title, langtech, cc-by-nc-sa

# Introduction to Deep Reinforcement Learning

## Milan Straka

### May 9, 2022

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s â€“ Richard Bellman

~~~
- Trial and error learning â€“ since 1850s
  - Law and effect â€“ Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, â€¦ â€“ 1950s and 1960s
  - Tsetlin, Holland, Klopf â€“ 1970s
  - Sutton, Barto â€“ since 1980s

~~~
- Arthur Samuel â€“ first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro â€“ 1992, human-level Backgammon program trained solely by self-play

~~~
- IBM Watson in Jeopardy â€“ 2011

---
# History of Reinforcement Learning â€“ Recent successes

- Human-level [video game playing](https://www.youtube.com/watch?v=pzYZvYhGxH8&list=PL2D_SqpHWZGgQu4gIUARUWk3rrwBrBEgq) (DQN) â€“ 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C â€“ 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow â€“ 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala â€“ Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala â€“ Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Reinforcement Learning â€“ Recent successes

![w=29%,f=right](r2d2_results.svgz)

- R2D2 â€“ Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training
~~~
- MuZero â€“ Nov 2019
  - planning with a learned model: 4999.2%, median: 2041.1%
~~~
- Agent57 - Mar 2020
  - super-human performance on all 57 Atari games
~~~
![w=69%,mw=30%,h=center,f=right](der-progress.svgz)

- Data-efficient Rainbow â€“ Jun 2019
  - learning from ~2 hours of game experience

---
# History of Reinforcement Learning â€“ Recent successes

- AlphaGo

  - Mar 2016 â€“ beat 9-dan professional player Lee Sedol
~~~
- AlphaGo Master â€“ Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero â€“ 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero â€“ Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=30%,h=center](a0_results.svgz)

---
# History of Reinforcement Learning â€“ Recent successes

- Dota2 â€“ Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN â€“ Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW â€“ Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games

~~~
- OpenAI Five â€“ Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs, 180 years of experience per day

~~~
- AlphaStar
  - Jan 2019: won 10 out of 11 StarCraft II games against two professional players
  - Oct 2019: ranked 99.8% on `Battle.net`, playing with full game rules

~~~
- Multiagent hide and seek: https://openai.com/blog/emergent-tool-use/

---
# History of Reinforcement Learning â€“ Recent successes

- Neural Architecture Search â€“ since 2017

  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
  - NasNet, EfficientNet, EfficientNetV2, â€¦
~~~
  - AutoML: automatically discovering
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - â€¦

~~~
- Optimize non-differentiable loss
~~~
  - improved translation quality in 2016

~~~
- Discovering discrete latent structures
~~~
- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, â€¦

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ğ”¼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) â‰ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t â‰ \argmax_a Q_t(a).$$

---
section: $Îµ$-greedy
# $Îµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$Îµ$-greedy_ method follows the greedy action with probability $1-Îµ$, and
chooses a uniformly random action with probability $Îµ$.

---
# $Îµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.svgz)

---
# $Îµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} âˆ‘_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} âˆ‘_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $Îµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.svgz)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.svgz)

~~~~
# Markov Decision Process

![w=53%,h=center](diagram.svgz)

A **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a **discount factor** (we will always use $Î³=1$ and finite episodes in this course).

~~~
Let a **return** $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_Ï€(s) & â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right] \\
         & = ğ”¼_{A_t âˆ¼ Ï€(s)} ğ”¼_{S_{t+1},R_{t+1} âˆ¼ p(s,A_t)} \big[R_{t+1}
           + Î³ ğ”¼_{A_{t+1} âˆ¼ Ï€(S_{t+1})} ğ”¼_{S_{t+2},R_{t+2} âˆ¼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + â€¦ \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and state-value function can be of course expressed using one another:
$$v_Ï€(s) = ğ”¼_{aâˆ¼Ï€}\big[q_Ï€(s, a)\big],~~~~~~~q_Ï€(s, a) = ğ”¼_{s', r âˆ¼ p}\big[r + Î³v_Ï€(s')\big].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $Î³ < 1$, there always exists a unique optimal
state-value function, aÂ unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: MonteCarlo
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$, finitely many
actions $ğ“$ and we will store estimates for every possible state-action pair.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

---
# Monte Carlo and $Îµ$-soft Policies

For the estimates to converge to the real values, every reachable state must be
visited infinitely many times and every action in such a state must be selected
infinitely many times in limit.

~~~
A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$
and we call it $Îµ$-greedy, if one action has a maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
It can be shown that when considering the class of $Îµ$-soft policies, one of the
optimal policies is always $Îµ$-greedy â€“ we will therefore search among the
$Îµ$-greedy policies only.

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{t+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: REINFORCE
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$Ï€(a | s; â†’Î¸).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $Ï€$ instead of just $Îµ$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_Ï€(s)$ and to
that account we need to compute its **gradient** $âˆ‡_{â†’Î¸} v_Ï€(s)$.

---
# Policy Gradient Theorem

Assume that $ğ“¢$ and $ğ“$ are finite, $Î³=1$, and that maximum episode length $H$ is also finite.

Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.
Let also $J(â†’Î¸) â‰ ğ”¼_{sâˆ¼h} v_Ï€(s)$.

~~~
Then
$$âˆ‡_{â†’Î¸} v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸)$$
and
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸),$$

~~~
where $P(s â†’ â€¦ â†’ s'|Ï€)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, â€¦ steps.


---
# Proof of Policy Gradient Theorem

$\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \Big[ âˆ‘\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ \big(âˆ‘\nolimits_{s'} p(s'|s, a)(r + v_Ï€(s'))\big) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \big(âˆ‘\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\big) \Big]$

~~~
_We now expand $v_Ï€(s')$._

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \Big(âˆ‘\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad âˆ‘\nolimits_{a'} \Big[ âˆ‡ Ï€(a'|s'; â†’Î¸) q_Ï€(s', a') + Ï€(a'|s'; â†’Î¸) \big(âˆ‘\nolimits_{s''} p(s''|s', a') âˆ‡ v_Ï€(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_Ï€(s'')$, we obtain the following:_

$\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} âˆ‘_{k=0}^H P(s â†’ s'\textrm{~in~}k\textrm{~steps~}|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, it is enough to realize that
$$âˆ‘\nolimits_{k=0}^H P(s â†’ s'\textrm{~in~}k\textrm{~steps~}|Ï€) âˆ P(s â†’ â€¦ â†’ s'|Ï€).$$

~~~
For the second part, we know that
$$âˆ‡_{â†’Î¸} J(â†’Î¸) = ğ”¼_{s âˆ¼ h} âˆ‡_{â†’Î¸} v_Ï€(s) âˆ ğ”¼_{s âˆ¼ h} âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸),$$
~~~
therefore using the fact that $Î¼(s') = ğ”¼_{s âˆ¼ h} P(s â†’ â€¦ â†’ s'|Ï€)$ we get
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
Finally, note that the theorem can be proven with infinite $ğ“¢$ and $ğ“$; and
also for infinite episodes when discount factor $Î³<1$.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(â†’Î¸) â‰ -ğ”¼_{sâˆ¼h} v_Ï€(s)$. The loss gradient is then
$$âˆ‡_{â†’Î¸} -J(â†’Î¸) âˆ -âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = -ğ”¼_{s âˆ¼ Î¼} âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$âˆ‡_{â†’Î¸} -J(â†’Î¸) âˆ ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_{â†’Î¸} -\ln Ï€(a | s; â†’Î¸),$$
where we used the fact that
$$âˆ‡_{â†’Î¸} \ln Ï€(a | s; â†’Î¸) = \frac{1}{Ï€(a | s; â†’Î¸)} âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_{â†’Î¸} -\ln Ï€(a | s; â†’Î¸),$$
estimating the $q_Ï€(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$âˆ‘_a b(s) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = b(s) âˆ‘_a âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = b(s) âˆ‡_{â†’Î¸} âˆ‘_a Ï€(a | s; â†’Î¸) = b(s) âˆ‡1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize the
variance of the gradient estimator. Such baseline reminds centering of the
returns, given that
$$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_Ï€(s, a) - v_Ï€(s)$ function is also called the **advantage** function
$$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s).$$

~~~
Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.svgz)

---
section: AdvancedRL
# What Next

If you liked the introduction to the deep reinforcement learning, I have
a whole course **NPFL122 â€“ Deep Reinforcement Learning** in the winter
semester.

~~~
- It covers a range of reinforcement learning algorithms, from the basic
  ones to more advanced algorithms utilizing deep neural networks.

~~~
- 2/2 C+Ex

~~~
- An elective (povinnÄ› volitelnÃ½) course in the programs:
  - Artificial Intelligence,
  - Language Technologies and Computational Linguistics.
