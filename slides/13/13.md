title: NPFL114, Lecture 13
class: title, langtech, cc-by-sa

# Generative Adversarial Networks, Diffusion Models

## Milan Straka

### May 9, 2023

---
# Generative Models

![w=46%,f=right](generative_models.png)

There are several approaches how to represent a probability distribution
$P(â‡â†’x)$. **Likelihood-based models** represent the probability density function
directly, often using an unnormalized probabilistic model (also called
energy-based model; i.e., specifying a non-zero _score_ or _density_ or
_logits_):
$$P_{â†’Î¸}(â‡â†’x) = \frac{e^{f_{â†’Î¸}(â‡â†’x)}}{Z_{â†’Î¸}}.$$

~~~
However, estimating the normalization constant $Z_{â†’Î¸} = âˆ«e^{f_{â†’Î¸}(â‡â†’x)}\dâ‡â†’x$ is
often intractable.

~~~
- We can compute $Z_{â†’Î¸}$ by restricting the model architecture (sequence
  modeling, invertible networks in normalizing flows);
~~~
- we can only approximate it (using for example variational inference as in
  VAE);
~~~
- we can use **implicit generative models**, which avoid representing likelihood
  (like GANs).

---
section: GAN
# Generative Adversarial Networks

We have a **generator** $G(â†’z; â†’Î¸_g)$, which given $â†’z âˆ¼ P(â‡â†’z)$ generates data $â†’x$.

~~~
Then we have a **discriminator** $D(â†’x; â†’Î¸_d)$, which given data $â†’x$ generates a probability
whether $â†’x$ comes from real data or is generated by a generator.

~~~
The discriminator and generator play the following game:
$$\min_G \max_D ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

![w=48%,h=center](gan_architecture.png)

---
# Generative Adversarial Networks

![w=75%,h=center](gan_training.svgz)

The generator and discriminator are alternately trained, the discriminator by
$$\argmax_{â†’Î¸_d} ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))]$$
and the generator by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

~~~
Basically, the discriminator acts as a trainable loss for the generator.

---
# Generative Adversarial Networks

Because $\log (1 - D(G(â†’z)))$ can saturate in the beginning of the training,
where the discriminator can easily distinguish real and generated samples,
the generator can be trained by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[-\log D(G(â†’z))]$$
instead, which results in the same fixed-point dynamics, but much stronger
gradients early in learning.

~~~
On top of that, if you train the generator by using â€œrealâ€ as the gold label
of the discriminator, you naturally get the above loss (which is the negative log
likelihood, contrary to the original formulation).


---
# Generative Adversarial Networks

![w=75%,h=center](gan_algorithm.svgz)

---
# Generative Adversarial Networks

![w=68%,h=center](gan_visualization.svgz)

---
# Conditional GAN

![w=55%,h=center](cgan.svgz)

---
# Deep Convolutional GAN

In Deep Convolutional GAN, the discriminator is a convolutional network (with
batch normalization) and the generator is also a convolutional network,
utilizing transposed convolutions.

![w=100%](dcgan_architectures.svgz)

---
# Deep Convolutional GAN

![w=100%,v=middle](dcgan_lsun_architecture.png)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch5.jpg)

---
# Deep Convolutional GAN

![w=50%,h=center](dcgan_interpolation.jpg)

---
# Deep Convolutional GAN

![w=70%,h=center](dcgan_latent_arithmetic.jpg)

---
# Deep Convolutional GAN

![w=75%,h=center](dcgan_latent_arithmetic_2.jpg)

---
# Deep Convolutional GAN

![w=85%,h=center](dcgan_latent_turn.jpg)

---
# GANs Training â€” Training Experience

![w=70%,h=center](gan_tom.jpg)

---
# GANs Training â€“ Results of In-House BigGAN Training

![w=100%](gan_custom.jpg)

---
section: GANConvergence
# GANs are Problematic to Train

Unfortunately, alternating SGD steps are not guaranteed to reach even
a local optimum of a minimax problem â€“ consider the following one:
$$\min_x \max_y xâ‹…y.$$

~~~
The update rules of $x$ and $y$ for learning rate $Î±$ are
$$\begin{bmatrix} x_{n+1} \\ y_{n+1} \end{bmatrix} = \begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} \begin{bmatrix} x_n \\ y_n \end{bmatrix}.$$

~~~
The update matrix is a rotation matrix multiplied by a constant $\sqrt{1 + Î±^2} > 1$
$$\begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} = \sqrt{1 + Î±^2} â‹… \begin{bmatrix} \cos Ï† & -\sin Ï† \\ \sin Ï† & \cos Ï† \end{bmatrix},$$
so the SGD will not converge with arbitrarily small step size.

---
# GANs are Problematic to Train

![w=100%](minimax_sgd_divergence.svgz)

---
# GANs are Problematic to Train

- Mode collapse

  ![w=100%](mode_collapse.svgz)

~~~
  - If the discriminator could see the whole batch, similar samples in it would
    be candidates for fake images.

    - Batch normalization helps a lot with this.

~~~
  - Historical averaging

~~~
- Label smoothing of only positive samples helps with the gradient flow.

---
# Comparison of VAEs and GANs

The Variational Autoencoders:
- are theoretically-pleasing;
~~~
- also provide an encoder, so apart from generation, they can be used as
  unsupervised feature extraction (the VAE encoder is used in various
  modeling architectures);
~~~
- the generated samples tend to be blurry, especially with $L^1$ or $L^2$ loss
  (because of the sampling used in the reconstruction; patch-based discriminator
  with perceptual loss helps).

~~~
The Generative Adversarial Networks:
- offer high sample quality;
~~~
- are difficult to train and suffer from mode collapse.

~~~
In past few years, GANs saw a big development, improving the sample quality
substantially.
~~~
However, since 2019/2020, VAEs have shown remarkable progress
(alleviating the blurriness issue by using perceptual loss and a 2D grid of
latent variables), and are being used for generation too.
~~~
Furthermore, additional approaches (normalizing flows, diffusion models) were
also being explored, with diffusion models becoming the most promising approach
since Q2 of 2021, surpassing both VAEs and GANs.

---
section: DiffusionModels
# Diffusion Models

Currently (as of May 2023), the best architecture for generating images seems to
be the **diffusion models**.

![w=47%,h=center](../12/stable_diffusion.jpg)

~~~
The diffusion models are deeply connected to **score-based generative models**,
which were developed independently. These two approaches are in fact just
different perspectives of the same model family, and many recent papers utilize
both sides of these models.

---
# Diffusion Models â€“ Diffusion Process, Reverse Process

![w=80%,h=center](ddpm_model.svgz)

Given a data point $â‡â†’x_0$ from a real data distribution $q(â‡â†’x)$, we define
a $T$-step _diffusion process_ (or the _forward process_) which gradually adds
Gaussian noise to the input image:

$$q(â‡â†’x_{1:T}|â‡â†’x_0) = âˆ\nolimits_{t=1}^T q(â‡â†’x_t|â‡â†’x_{t-1}).$$

~~~
Our goal is to reverse the forward process $q(â‡â†’x_t|â‡â†’x_{t-1})$, and generate an
image by starting with $â‡â†’x_T âˆ¼ ğ“(â†’0, â‡‰I)$, and then performing the forward
process in reverse.
~~~
We therefore learn a model $p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ to approximate the reverse
of $q(â‡â†’x_t|â‡â†’x_{t-1})$, and obtain a _reverse process_:
$$p_{â†’Î¸}(â‡â†’x_{0:T}) = p(â‡â†’x_T) âˆ\nolimits_{t=1}^T p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t).$$

---
# Diffusion Models â€“ The Reverse Process

![w=70%,h=center](diffusion_forward_backward.png)

---
# Diffusion Models â€“ Model Overview

![w=88%,mw=19%,h=right,f=right](imagen_architecture_overall.svgz)

The $p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ is commonly modelled using a UNet architecture
with skip connections.

~~~
### Training

During training, we randomly sample a time step $t$, and perform an update
of the parameters $â†’Î¸$ in order for $p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ to
better approximate the reverse of $q(â‡â†’x_t|â‡â†’x_{t-1})$.

~~~
### Sampling

In order to sample an image, we start by sampling $â‡â†’x_T âˆ¼ ğ“(â†’0, â‡‰I)$,
and then perform $T$ steps of the reverse process by sampling
$â‡â†’x_{t-1} âˆ¼ p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ for $t$ from $T$ down to 1.

---
section: $ğ“$
style: .katex-display { margin: .7em 0 }
# Normal Distribution Reminder

![w=28%,f=right](../01/normal_distribution.svgz)

Normal (or Gaussian) distribution is a continuous distribution parametrized by
a mean $Î¼$ and variance $Ïƒ^2$:

$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

~~~
For a $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x; â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |â‡‰Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

~~~
The biggest difference compared to the single-dimensional case is the _covariance
matrix_ $â‡‰Î£$, which is (in the non-degenerate case, which is the only one
considered here) a _symmetric positive-definite matrix_ of size $D Ã— D$.

~~~
However, in this lecture we will only consider _isotropic_ distribution, where $â‡‰Î£ = Ïƒ^2â‡‰I$:
$$ğ“(â†’x; â†’Î¼, Ïƒ^2â‡‰I) = âˆ\nolimits_i ğ“(x_i; Î¼_i, Ïƒ^2).$$

---
# Normal Distribution Reminder

A normally-distributed random variable $â‡â†’x âˆ¼ ğ“(â†’Î¼, Ïƒ^2â‡‰I)$ can be written using
the reparametrization trick also as
$$â‡â†’x = â†’Î¼ + Ïƒ â‡â†’e,\textrm{~~where~~}â‡â†’e âˆ¼ ğ“(â†’0, â‡‰I).$$

~~~
The sum of two independent normally-distributed random variables $â‡â†’x_1 âˆ¼ ğ“(â†’Î¼_1, Ïƒ_1^2â‡‰I)$
and $â‡â†’x_2 âˆ¼ ğ“(â†’Î¼_2, Ïƒ_2^2â‡‰I)$ has normal distribution $â‡ğ“\big(â†’Î¼_1 + â†’Î¼_2, (Ïƒ_1^2 + Ïƒ_2^2)â‡‰I\big)$.

~~~
Therefore, if we have three standard normal random variables $â‡â†’e_1, â‡â†’e_2, â‡â†’e âˆ¼ ğ“(â†’0, â‡‰I)$,
then
$$Ïƒ_1 â‡â†’e_1 + Ïƒ_2 â‡â†’e_2 = \sqrt{Ïƒ_1^2 + Ïƒ_2^2} â‡â†’e.$$

---
section: DDPM
# DDPM â€“ The Forward Process

We now describe _Denoising Diffusion Probabilistic Models (DDPM)_.

~~~
![w=80%,h=center](ddpm_model.svgz)

Given a data point $â‡â†’x_0$ from a real data distribution $q(â‡â†’x)$, we define
a $T$-step _diffusion process_ (or the _forward process_) which gradually adds
Gaussian noise according to some variance schedule $Î²_1, â€¦, Î²_T$:

$\displaystyle \kern11em{}\mathllap{q(â‡â†’x_{1:T}|â‡â†’x_0)} = âˆ_{t=1}^T q(â‡â†’x_t|â‡â†’x_{t-1}),$

~~~
$\displaystyle \kern11em{}\mathllap{q(â‡â†’x_t|â‡â†’x_{t-1})} = ğ“(â‡â†’x_t; \sqrt{1 - Î²_t} â‡â†’x_{t-1}, Î²_t â‡‰I),$

~~~
$\displaystyle \kern11em{}\mathllap{} = \sqrt{1 - Î²_t} â‡â†’x_{t-1} + \sqrt{Î²_t} â‡â†’e\textrm{~~for~~}â‡â†’eâˆ¼ğ“(â†’0, â‡‰I).$

~~~
More noise gets gradually added to the original image $â‡â†’x_0$, converging to
pure Gaussian noise.

---
# DDPM â€“ The Forward Process

Let $Î±_t = 1-Î²_t$ and $Î±Ì„_t = âˆ_{i=1}^t Î±_i$.
~~~
Then we have

$\displaystyle\kern3em{}\mathllap{â‡â†’x_t} = \sqrt{Î±_t} \textcolor{blue}{â‡â†’x_{t-1}} + \sqrt{1-Î±_t}â‡â†’e_t$

~~~
$\displaystyle\kern3em{} = \sqrt{Î±_t} \textcolor{blue}{\big(\sqrt{Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{1-Î±_{t-1}}â‡â†’e_{t-1}\big)} + \sqrt{1-Î±_t}â‡â†’e_t$

~~~
$\displaystyle\kern3em{} = \sqrt{Î±_t Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{Î±_t(1-Î±_{t-1}) + (1-Î±_t)}â‡â†’eÌ„_{t-1}$

~~~
$\displaystyle\kern3em{} = \sqrt{Î±_t Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{1 - Î±_t Î±_{t-1}}â‡â†’eÌ„_{t-1}$

~~~
$\displaystyle\kern3em{} = \sqrt{Î±_t Î±_{t-1} Î±_{t-2}} â‡â†’x_{t-3} + \sqrt{1 - Î±_t Î±_{t-1} Î±_{t-2}}â‡â†’eÌ„_{t-2}$

~~~
$\displaystyle\kern3em{} = â€¦$

~~~
$\displaystyle\kern3em{} = \sqrt{Î±Ì„_t} â‡â†’x_0 + \sqrt{1-Î±Ì„_t}â‡â†’eÌ„_0$

for standard normal random variables $â‡â†’e_i$ and $â‡â†’eÌ„_i$.

~~~
In other words, we have shown that $q(â‡â†’x_t | â‡â†’x_0) = ğ“\big(\sqrt{Î±Ì„_t}â‡â†’x_0, (1-Î±Ì„_t)â‡‰I\big)$.

~~~
Therefore, if $Î±Ì„_t â†’ 0$ as $t â†’ âˆ$, the $â‡â†’x_t$ converges to $ğ“(â†’0, â‡‰I)$ as $t â†’ âˆ$.

---
# DDPM â€“ Noise Schedule

![w=50%,f=right](diffusion_schedule.svgz)

Originally, linearly increasing sequence of noise variations
$Î²_1=0.0001, â€¦, Î²_T=0.04$ was used.

~~~
However, the resulting sequence $Î±Ì„_t$ was not ideal (nearly the whole second
half of the diffusion process was mostly just random noise), so later a cosine
schedule was proposed:
$$Î±Ì„_t = \frac{1}{2}\Big(\cos(t/T â‹… Ï€)+1\Big),$$
and now it is dominantly used.

~~~
In practice, we want to avoid both the values of 0 and 1, and keep $Î±_t$ in $[Îµ, 1-Îµ]$ range.

---
# DDPM â€“ Noise Schedule

We assume the images $â‡â†’x_0$ have zero mean and unit variance (we normalize them
to achieve that).
~~~
Then every
$$q(â‡â†’x_t|â‡â†’x_0) = \textcolor{red}{\sqrt{Î±Ì„_t}} â‡â†’x_0 + \textcolor{blue}{\sqrt{1-Î±Ì„_t}}â‡â†’e$$
has also zero mean and unit variance.

~~~
![w=29%,f=right](signal_noise_rates.png)

The $\textcolor{red}{\sqrt{Î±Ì„_t}}$ and $\textcolor{blue}{\sqrt{1-Î±Ì„_t}}$ can be
considered as the _signal rate_ and the _noise rate_.

~~~
Because $\textcolor{red}{\sqrt{Î±Ì„_t}}^2 + \textcolor{blue}{\sqrt{1-Î±Ì„_t}}^2 = 1$,
the signal rate and the noise rate form a circular arc. The proposed cosine
schedule
$$\begin{aligned}
  \textcolor{red}{\sqrt{Î±Ì„_t}} &= \cos(t/T â‹… Ï€/2), \\
  \textcolor{blue}{\sqrt{1-Î±Ì„_t}} &= \sin(t/T â‹… Ï€/2),
\end{aligned}$$
corresponds to an uniform movement on this arc.

---
# DDPM â€“ The Reverse Process

![w=80%,h=center](ddpm_model.svgz)

In order to be able to generate imates, we therefore learn a model
$p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ to approximate the reverse of $q(â‡â†’x_t|â‡â†’x_{t-1})$.

~~~
When $Î²_t$ is small, this reverse is nearly Gaussian, so we represent $p_{â†’Î¸}$
as
$$p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t) = ğ“\big(â‡â†’x_{t-1}; â†’Î¼_{â†’Î¸}(â‡â†’x_t, t), Ïƒ_t^2â‡‰I\big)$$
for some fixed sequence of $Ïƒ_1, â€¦, Ïƒ_T$.

~~~
The whole reverse process is then
$$p_{â†’Î¸}(â‡â†’x_{0:T}) = p(â‡â†’x_T) âˆ\nolimits_{t=1}^T p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t).$$

---
# DDPM â€“ Loss

We now want to derive the loss. First note that the reverse of $q(â‡â†’x_t|â‡â†’x_{t-1})$
is actually tractable when conditioning on $â‡â†’x_0$:


$\displaystyle\kern9em{}\mathllap{q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0)} = ğ“\big(â‡â†’x_{t-1}; \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}, \textcolor{green}{Î²Ìƒ_t}â‡‰I\big),$

~~~
$\displaystyle\kern9em{}\mathllap{\textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}} = \frac{\sqrt{Î±Ì„_{t-1}}Î²_t}{1-Î±Ì„_t}â‡â†’x_0 + \frac{\sqrt{Î±_t}(1-Î±Ì„_{t-1})}{1-Î±Ì„_t}â‡â†’x_t,$

~~~
$\displaystyle\kern9em{}\mathllap{\textcolor{green}{Î²Ìƒ_t}} = \frac{1-Î±Ì„_{t-1}}{1-Î±Ì„_t}Î²_t.$

~~~
We present the proof on the next slide for completeness.

---
class: dbend
# Forward Process Reverse Derivation

Starting with the Bayes' rule, we get

$\displaystyle\kern7em{}\mathllap{q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0)} = \textcolor{purple}{q(â‡â†’x_t | â‡â†’x_{t-1}, â‡â†’x_0)} \frac{\textcolor{darkcyan}{q(â‡â†’x_{t-1} | â‡â†’x_0)}}{\textcolor{red}{q(â‡â†’x_t | â‡â†’x_0)}}$

~~~
$\displaystyle\kern2em{} âˆ \exp\Big(-\frac{1}{2}\Big(\textcolor{purple}{\frac{(â‡â†’x_t - \sqrt{Î±_t}â‡â†’x_{t-1})^2}{Î²_t}} + \textcolor{darkcyan}{\frac{(â‡â†’x_{t-1} - \sqrt{Î±Ì„_{t-1}}â‡â†’x_0)^2}{1-Î±Ì„_{t-1}}} - \textcolor{red}{\frac{(â‡â†’x_t - \sqrt{Î±Ì„_t}â‡â†’x_0)^2}{1-Î±Ì„_t}}\Big)\Big)$

~~~
$\displaystyle\kern2em{} = \exp\Big(-\frac{1}{2}\Big(\tfrac{â‡â†’x_t^2 - 2\sqrt{Î±_t}â‡â†’x_t\textcolor{orange}{â‡â†’x_{t-1}} + Î±_t\textcolor{magenta}{â‡â†’x_{t-1}^2}}{Î²_t} + \tfrac{\textcolor{magenta}{â‡â†’x_{t-1}^2} - 2\sqrt{Î±Ì„_{t-1}}\textcolor{orange}{â‡â†’x_{t-1}}â‡â†’x_0 + Î±Ì„_{t-1}â‡â†’x_0^2}{1-Î±Ì„_{t-1}} + â€¦\Big)\Big)$

~~~
$\displaystyle\kern2em{} = \exp\Big(-\frac{1}{2}\Big(\big(\tfrac{Î±_t}{Î²_t} + \tfrac{1}{1-Î±Ì„_{t-1}}\big)\textcolor{magenta}{â‡â†’x_{t-1}^2} - 2\big(\tfrac{\sqrt{Î±_t}}{Î²_t}â‡â†’x_t + \tfrac{\sqrt{Î±Ì„_{t-1}}}{1-Î±Ì„_{t-1}}â‡â†’x_0\big)\textcolor{orange}{â‡â†’x_{t-1}} + â€¦\Big)\Big)$

~~~
From this formulation, we can derive that $q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0) = ğ“\big(â‡â†’x_{t-1}; \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}, \textcolor{green}{Î²Ìƒ_t}â‡‰I\big)$ for

~~~
$\displaystyle\kern5em{}\mathllap{\textcolor{green}{Î²Ìƒ_t}} = 1/\big(\tfrac{Î±_t}{Î²_t} + \tfrac{1}{1-á¾±_{t-1}}\big) = 1/\big(\tfrac{Î±_t(1-Î±Ì„_{t-1})+Î²_t}{Î²_t(1-á¾±_{t-1})}\big) = 1/\big(\tfrac{Î±_t+Î²_t-Î±Ì„_t}{Î²_t(1-á¾±_{t-1})}\big) = \frac{1-Î±Ì„_{t-1}}{1-Î±Ì„_t}Î²_t,$

~~~
$\displaystyle\kern5em{}\mathllap{\textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}} = \big(\tfrac{\sqrt{Î±_t}}{Î²_t}â‡â†’x_t + \tfrac{\sqrt{á¾±_{t-1}}}{1-á¾±_{t-1}}â‡â†’x_0\big) \textcolor{green}{\tfrac{1-á¾±_{t-1}}{1-á¾±_t}Î²_t} = \frac{\sqrt{á¾±_{t-1}}Î²_t}{1-á¾±_t}â‡â†’x_0 + \frac{\sqrt{Î±_t}(1-á¾±_{t-1})}{1-á¾±_t}â‡â†’x_t.$

---
# DDPM â€“ Deriving Loss using Jensen's Inequality

$\displaystyle -ğ”¼_{q(â‡â†’x_0)}\big[\log p_{â†’Î¸}(â‡â†’x_0)\big] = -ğ”¼_{q(â‡â†’x_0)}\big[\log ğ”¼_{p_{â†’Î¸}(â‡â†’x_{1:T})}[p_{â†’Î¸}(â‡â†’x_0)]\big]$

~~~
$\displaystyle \kern2em{} = -ğ”¼_{q(â‡â†’x_0)}\Big[\log ğ”¼_{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big[\tfrac{p_{â†’Î¸}(â‡â†’x_{0:T})}{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big]\Big]$

~~~
$\displaystyle \kern2em{} â‰¤ -ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{p_{â†’Î¸}(â‡â†’x_{0:T})}{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big] = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{q(â‡â†’x_{1:T}|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{0:T})}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_t|â‡â†’x_{t-1})}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \Big(\tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)}\tfrac{q(â‡â†’x_t|â‡â†’x_0)}{q(â‡â†’x_{t-1}|â‡â†’x_0)}\Big) + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} + \log \tfrac{q(â‡â†’x_T|â‡â†’x_0)}{q(â‡â†’x_1|â‡â†’x_0)} + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{q(â‡â†’x_T|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_T)} + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} -\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\underbrace{\scriptsize D_\textrm{KL}(q(â‡â†’x_T|â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_T))}_{L_T} + âˆ‘\nolimits_{t=2}^T\underbrace{\scriptsize D_\textrm{KL}(q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)}_{L_t} \underbrace{-\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}_{L_0}\Big]$

---
# DDPM â€“ Deriving Loss using Jensen's Inequality

The whole loss is therefore composed of the following components:

~~~
- $L_T = D_\textrm{KL}\big(q(â‡â†’x_T|â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_T)\big)$ is constant
  with respect to $â†’Î¸$ and can be ignored,

~~~
- $L_t = D_\textrm{KL}\big(q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)\big)$
  is KL divergence between two Gaussians, so it can be computed explicitly as

  $$L_t = ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0) - â†’Î¼_{â†’Î¸}(â‡â†’x_t, t) \Big\|^2\bigg]$$
~~~
- $L_0 = -\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)$ can be used to generate discrete $â‡â†’x_0$
  from the continuous $â‡â†’x_1$; we will ignore it.

---
# DDPM â€“ Reparametrizing Model Prediction

Recall that $q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0) = ğ“\big(â‡â†’x_{t-1}; \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}, \textcolor{green}{Î²Ìƒ_t}â‡‰I\big)$ for

$\displaystyle\kern5em{}\mathllap{\textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}} = \frac{\sqrt{Î±Ì„_{t-1}}Î²_t}{1-Î±Ì„_t}â‡â†’x_0 + \frac{\sqrt{Î±_t}(1-Î±Ì„_{t-1})}{1-Î±Ì„_t}â‡â†’x_t,$

$\displaystyle\kern5em{}\mathllap{\textcolor{green}{Î²Ìƒ_t}} = \frac{1-Î±Ì„_{t-1}}{1-Î±Ì„_t}Î²_t.$

~~~
Because $â‡â†’x_t = \sqrt{Î±Ì„_t} â‡â†’x_0 + \sqrt{1-Î±Ì„_t}â‡â†’e_t$, we get $â‡â†’x_0 = \textcolor{red}{\frac{1}{\sqrt{Î±Ì„_t}}\big(â‡â†’x_t - \sqrt{1-Î±Ì„_t}â‡â†’e_t\big)}$.

~~~
Substituting $â‡â†’x_0$ to $â†’Î¼Ìƒ_t$, we get

$\displaystyle\kern5em{}\mathllap{\textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}} = \frac{\sqrt{á¾±_{t-1}}Î²_t}{1-á¾±_t}\textcolor{red}{\frac{1}{\sqrt{á¾±_t}}\Big(â‡â†’x_t - \sqrt{1-á¾±_t}â‡â†’e_t\Big)} + \frac{\sqrt{Î±_t}(1-á¾±_{t-1})}{1-á¾±_t}â‡â†’x_t$

~~~
$\displaystyle\kern5em{} = \Big(\frac{\sqrt{á¾±_{t-1}}Î²_t}{1-á¾±_t}\frac{1}{\sqrt{á¾±_t}} + \frac{\sqrt{Î±_t}(1-á¾±_{t-1})}{1-á¾±_t}\Big)â‡â†’x_t - \Big(\frac{\sqrt{á¾±_{t-1}}Î²_t}{1-á¾±_t}\frac{\sqrt{1-á¾±_t}}{\sqrt{á¾±_t}}\Big)â‡â†’e_t$

~~~
$\displaystyle\kern5em{} = \frac{Î²_t + Î±_t(1-Î±Ì„_{t-1})}{(1-á¾±_t)\sqrt{Î±_t}}â‡â†’x_t - \Big(\frac{Î²_t}{\sqrt{1-á¾±_t}\sqrt{Î±_t}}\Big)â‡â†’e_t = \textcolor{blue}{\frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â‡â†’e_t\Big)}.$

---
# DDPM â€“ Reparametrizing Model Prediction

We change our model to predict $â†’Îµ_{â†’Î¸}(â‡â†’x_t, t)$ instead of
$â†’Î¼_{â†’Î¸}(â‡â†’x_t, t)$.
~~~
The loss $L_t$ then becomes

$\displaystyle\kern3em{}\mathllap{L_t} = ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)} - \textcolor{green}{â†’Î¼_{â†’Î¸}(â‡â†’x_t, t)} \Big\|^2\bigg]$

~~~
$\displaystyle\kern3em{} = ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| \textcolor{blue}{\frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â‡â†’e_t\Big)} - \textcolor{green}{\frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â†’Îµ_{â†’Î¸}(â‡â†’x_t, t)\Big)} \Big\|^2\bigg]$

~~~
$\displaystyle\kern3em{} = ğ”¼\bigg[\frac{(1-Î±_t)^2}{2Î±_t(1-Î±Ì„_t)\|Ïƒ_tâ‡‰I\|^2}\Big\| â‡â†’e_t - â†’Îµ_{â†’Î¸}(â‡â†’x_t, t) \Big\|^2\bigg]$

~~~
$\displaystyle\kern3em{} = ğ”¼\bigg[\frac{(1-Î±_t)^2}{2Î±_t(1-Î±Ì„_t)\|Ïƒ_tâ‡‰I\|^2}\Big\| â‡â†’e_t - â†’Îµ_{â†’Î¸}\big(\sqrt{á¾±_t} â‡â†’x_0 + \sqrt{1-á¾±_t}â‡â†’e_t, t\big) \Big\|^2\bigg].$

~~~
The authors found that training without the weighting term performs better, so
the final loss is
$$L_t^\textrm{simple} = ğ”¼_{tâˆˆ\{1..T\},â‡â†’x_0,â‡â†’e_t}\Big[\big\| â‡â†’e_t - â†’Îµ_{â†’Î¸}\big(\sqrt{á¾±_t} â‡â†’x_0 + \sqrt{1-á¾±_t}â‡â†’e_t, t\big) \big\|^2\Big].$$

~~~
Note that both losses have the same optimum if we used independent $â†’Îµ_{â†’Î¸_t}$ for every $t$.

---
# DDPM â€“ Training and Sampling Algorithms

![w=100%](ddpm_algorithms.svgz)

~~~
Sampling using the proposed algorithm is slow â€“ it is common to use $T=1000$
steps during sampling.

~~~
The value of $Ïƒ_t^2$ is chosen to be either $Î²_t$ or $Î²Ìƒ_t$, or any value
in between (it can be proven that these values correspond to upper and lower
bounds on the reverse process entropy).

~~~
Both of these issues will be alleviated later, when we present DDIM providing
an updated sampling algorithm, which runs in several tens of steps and does not
use $Ïƒ_t^2$.

---
# DDPM â€“ Diffusion Models Architecture

The DDPM models the noise prediction $â†’Îµ_{â†’Î¸}(â‡â†’x_t, t)$ using a UNet
architecture with pre-activated ResNet blocks.

~~~
- The current time step is represented using the Transformer sinusoidal
  embeddings and added â€œin the middleâ€ of every residual block (after the
  first convolution).

~~~
- Additionally, on several lower-resolution levels, a self-attention
  block (an adaptation of the Transformer self-attention, which considers
  the 2D grid of features as a sequence of feature vectors) is commonly used.

  ![w=72%,h=center](sagan.png)

---
# Diffusion Models Architecture â€“ ImaGen

![w=36%,mw=49%,h=center](imagen_architecture_overall.svgz)
~~~
![w=88%,mw=49%,h=center](imagen_architecture_block.svgz)

---
# Diffusion Models Architecture â€“ ImaGen

![w=100%,mw=51%,mh=75%,h=center](imagen_architecture_dblock.svgz)
~~~
![w=100%,mw=48%,mh=75%,h=center](imagen_architecture_ublock.svgz)

~~~
There are just minor differences in the ImaGen architecture â€“ for example the
place where the time sinusoidal embeddings are added.

---
# Conditional Models, Classifier-Free Guidance

In many cases we want the generative model to be conditional.
~~~
We have already seen how to condition it on the current time step. Additionally,
we might consider also conditioning on
- an image (e.g., for super-resolution): the image is then resized and
  concatenated with the input noised image (and optionally in other places,
  like after every resolution change);

~~~
- a text: the usual approach is to encode the text using some pre-trained
  encoder, and then to introduce an â€œimage-textâ€ attention layer (usually
  after the self-attention layers).

~~~
To make the effect of conditioning stronger during sampling, we might
also employ _classifier-free guidance_:
~~~
- During training, we sometimes train $â†’Îµ_{â†’Î¸}(â‡â†’x_t, t, y)$ with the conditioning $y$,
  and sometimes we train $â†’Îµ_{â†’Î¸}(â‡â†’x_t, t, \varnothing)$ without the conditioning.
~~~
- During sampling, we pronounce the effect of the conditioning by taking
  the unconditioned noise and adding the difference between conditioned and
  unconditioned noise _weighted by the weight_ $w$ (Stable Diffusion uses
  $w=7.5$):
  $$â†’Îµ_{â†’Î¸}(â‡â†’x_t, t, \varnothing) + w\big(â†’Îµ_{â†’Î¸}(â‡â†’x_t, t, y) - â†’Îµ_{â†’Î¸}(â‡â†’x_t, t, \varnothing)\big).$$

---
section: DDIM
# Denoising Diffusion Implicit Models

We now describe _Denoising Diffusion Implicit Models (DDIM)_, which utilize
a different forward process.

~~~
This forward process is designed to:
- allow faster sampling,

~~~
- have the same â€œmarginalsâ€ $q(â‡â†’x_t | â‡â†’x_0) = ğ“\big(\sqrt{Î±Ì„_t}â‡â†’x_0, (1-Î±Ì„_t)â‡‰I\big)$.

~~~
The second condition will allow us to use the same loss as in DDPM â€“ therefore,
the training algorithm is exactly identical do DDPM, only the sampling algorithm
is different.

~~~
Note that in the slides, only a special case of DDIM is described; the original
paper describes a more general forward process. However, the special case
presented here is almost exclusively used.

---
# Denoising Diffusion Implicit Models â€“ The Forward Process

The forward process of DDIM can be described using

$$q_0(â‡â†’x_{1:T}|â‡â†’x_0) = q_0(â‡â†’x_T | â‡â†’x_0) âˆ\nolimits_{t=2}^T q_0(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0),$$

where

~~~
- $q_0(â‡â†’x_T | â‡â†’x_0) = ğ“\big(\sqrt{Î±Ì„_T}, (1-Î±Ì„_T)â‡‰I\big)$,

~~~
- $q_0(â‡â†’x_{t-1} | â‡â†’x_t, â‡â†’x_0) = ğ“\Big(\sqrt{Î±Ì„_{t-1}}â‡â†’x_0 + \sqrt{1-Î±Ì„_{t-1}}\big(\textcolor{green}{\tfrac{â‡â†’x_t - \sqrt{Î±Ì„_t}â‡â†’x_0}{\sqrt{1-Î±Ì„_t}}}\big), 0â‹…â‡‰I\Big)$.

~~~
With these definitions, we can prove by induction that $q_0(â‡â†’x_t | â‡â†’x_0) = ğ“\big(\sqrt{Î±Ì„_t}â‡â†’x_0, (1-Î±Ì„_t)â‡‰I\big)$:

~~~
$\displaystyle\kern3em{}\mathllap{â‡â†’x_{t-1}} = \sqrt{á¾±_{t-1}}â‡â†’x_0 + \sqrt{1-á¾±_{t-1}}\big(\tfrac{\textcolor{blue}{â‡â†’x_t} - \sqrt{á¾±_t}â‡â†’x_0}{\sqrt{1-á¾±_t}}\big)$

~~~
$\displaystyle\kern3em{} = \sqrt{á¾±_{t-1}}â‡â†’x_0 + \sqrt{1-á¾±_{t-1}}\big(\tfrac{\textcolor{blue}{\sqrt{Î±Ì„_t}â‡â†’x_0 + \sqrt{1-Î±Ì„_t}â‡â†’e_t} - \sqrt{á¾±_t}â‡â†’x_0}{\sqrt{1-á¾±_t}}\big) = \sqrt{á¾±_{t-1}}â‡â†’x_0 + \sqrt{1-á¾±_{t-1}}\textcolor{green}{â‡â†’e_t}$.

~~~
The real â€œforwardâ€ $q_0(â‡â†’x_t | â‡â†’x_{t-1}, â‡â†’x_0)$ can be expressed using Bayes'
theorem using the above definition, but we do not actually need it.

---
# Denoising Diffusion Implicit Models â€“ The Reverse Process

The definition of $q_0(â‡â†’x_{t-1} | â‡â†’x_t, â‡â†’x_0)$ provides us also with
a sampling algorithm â€“ after sampling the initial noise $â‡â†’x_T âˆ¼ ğ“(â†’0, â‡‰I)$,
we perform the following for $t$ from $T$ down to 1:
$$\begin{aligned}
â†’x_{t-1} 
  &= \sqrt{Î±Ì„_{t-1}}\textcolor{blue}{â‡â†’x_0} + \sqrt{1-Î±Ì„_{t-1}}â†’Îµ_{â†’Î¸}(â†’x_t, t) \\
  &= \sqrt{Î±Ì„_{t-1}}\Big(\textcolor{blue}{\tfrac{â†’x_t-\sqrt{1-Î±Ì„_t}â†’Îµ_{â†’Î¸}(â†’x_t, t)}{\sqrt{Î±Ì„_t}}}\Big) + \sqrt{1-Î±Ì„_{t-1}}â†’Îµ_{â†’Î¸}(â†’x_t, t).
\end{aligned}$$

~~~
An important property of $q_0$ is that it can also model several
steps at once:

![w=55%,h=center](ddim_accelerated_generation.png)

$$q_0(â‡â†’x_{t'} | â‡â†’x_t, â‡â†’x_0) = ğ“\big(\sqrt{Î±Ì„_{t'}}â‡â†’x_0 + \sqrt{1-Î±Ì„_{t'}}\big(\tfrac{â‡â†’x_t - \sqrt{á¾±_t}â‡â†’x_0}{\sqrt{1-á¾±_t}}\big), â‡‰0\big).$$

---
# Denoising Diffusion Implicit Models â€“ Accelerated Sampling

We base our accelerated sampling algorithm on the â€œmultistepâ€ $q_0(â‡â†’x_{t'} | â‡â†’x_t, â‡â†’x_0)$.

~~~
Let $t_S=T, t_{S-1}, â€¦, t_1$ be a subsequence of the process steps (usually,
a uniform subsequence of $T, â€¦, 1$ is used). Starting from the initial
noise $â‡â†’x_T âˆ¼ ğ“(â†’0, â‡‰I)$, we perform $S$ sampling steps for $i$ from
$S$ down to 1:

$$â†’x_{t_{i-1}} = \sqrt{Î±Ì„_{t_{i-1}}}\Big(\tfrac{â†’x_{t_i}-\sqrt{1-Î±Ì„_{t_i}}â†’Îµ_{â†’Î¸}(â†’x_{t_i}, t_i)}{\sqrt{Î±Ì„_{t_i}}}\Big) + \sqrt{1-Î±Ì„_{t_{i-1}}}â†’Îµ_{â†’Î¸}(â†’x_{t_i}, t_i).$$

~~~
The sampling procedure can be described in words as follows:
- using the current time step $t_i$, we compute the estimated
  noise $â†’Îµ_{â†’Î¸}(â†’x_{t_i}, t_i)$;
~~~
- by utilizing the current signal rate $\sqrt{Î±Ì„_{t_i}}$ and noise
  rate $\sqrt{1-Î±Ì„_{t_i}}$, we estimate $â‡â†’x_0$;
~~~
- we obtain $â†’x_{t_{i-1}}$ by combining the estimated signal $â‡â†’x_0$ and
  noise $â†’Îµ_{â†’Î¸}(â†’x_{t_i}, t_i)$ using the signal and noise rates of the time
  step $t_{i-1}$.

---
# DDIM â€“ Accelerated Sampling Examples

![w=85%,h=center](ddim_cifar10_samples.svgz)

![w=85%,h=center](ddim_lsun_samples.svgz)

---
section: StableDiffusion
# Stable Diffusion â€“ Semantic and Perceptual Compression

![w=65%,h=center](stable_diffusion_compression_kinds.png)

---
# Stable Diffusion â€“ Architecture

![w=100%,h=center](stable_diffusion_architecture.svgz)

---
section: NCSN
class: dbend
# Score Matching

Recall that loglikelihood-based models explicit represent the density
function, commonly using an unnormalized probabilistic model
$$p_{â†’Î¸}(â‡â†’x) = \frac{e^{f_{â†’Î¸}(â‡â†’x)}}{Z_{â†’Î¸}},$$
and it is troublesome to ensure the tractability of the normalization constant
$Z_{â†’Î¸}$.

~~~
One way how to avoid the normalization is to avoid the explicit density
$p_{â†’Î¸}(â‡x)$, and represent a **score function** instead, where the score
function is the gradient of the log density:
$$s_{â†’Î¸}(â‡â†’x) = âˆ‡_{â‡â†’x} \log p_{â†’Î¸}(â‡â†’x),$$

~~~
because
$$s_{â†’Î¸}(â‡â†’x) = âˆ‡_{â‡â†’x} \log p_{â†’Î¸}(â‡â†’x) = âˆ‡_{â‡â†’x} \log \frac{e^{f_{â†’Î¸}(â‡â†’x)}}{Z_{â†’Î¸}} = âˆ‡_{â‡â†’x} f_{â†’Î¸}(â‡â†’x) - \underbrace{âˆ‡_{â‡â†’x} \log Z_{â†’Î¸}}_0 = âˆ‡_{â‡â†’x} f_{â†’Î¸}(â‡â†’x).$$

---
class: dbend
# Langevin Dynamics

When we have a score function $âˆ‡_{â‡â†’x} \log p_{â†’Î¸}(â‡â†’x)$, we can use it to
perform sampling from the distribution $p_{â†’Î¸}(â‡â†’x)$ by using **Langevin
dynamics**, which is an algorithm akin to SGD, but performing sampling
instead of optimum finding.
~~~
Starting with $â‡â†’x_0$, we iteratively set
$$â‡â†’x_{i+1} â† â‡â†’x_i + Îµâˆ‡_{â‡â†’x_i} \log p_{â†’Î¸}(â‡â†’x_i) + \sqrt{2Îµ}\,â‡â†’z_i,\textrm{~~where~~}â†’z_i âˆ¼ ğ“(â†’0, â‡‰I).$$

~~~
![w=31%,f=right](langevin_dynamics.gif)

When $Îµ â†’ 0$ and $K â†’ âˆ$, $â‡â†’x_K$ obtained by the Langevin dynamics
converges to a sample from the distribution $p_{â†’Î¸}(â‡â†’x)$.

---
class: dbend
# Score-Based Generative Modeling

![w=100%](score_based_generative_modeling.png)

---
class: dbend
# Noise Conditional Score Network

However, estimating the score function from data is inaccurate
in low-density regions.

![w=70%,mw=90%,h=center](score_estimation_noiseless.png)

~~~
In order to accurately estimate the score function in low-density
regions, we perturb the data distribution by isotropic Gaussian noise
with various noise rates $Ïƒ_t$:
$$q_{Ïƒ_t}(â‡â†’xÌƒ) â‰ ğ”¼_{â‡â†’x âˆ¼ p(â‡â†’x)} \big[ğ“(â‡â†’xÌƒ; â‡â†’x, {Ïƒ_t}^2 â‡‰I)\big],$$
~~~
where the noise distribution $q_{Ïƒ_t}(â‡â†’xÌƒ|â‡â†’x) = ğ“(â‡â†’xÌƒ;â‡â†’x, Ïƒ_t^2 â‡‰I)$
as analogous to the forward process in the diffusion models.

---
class: dbend
style: .katex-display { margin: .8em 0 }
# Noise Conditional Score Network

To train the score function $â†’s_{â†’Î¸}(â‡â†’x, Ïƒ_t) = âˆ‡_{â‡â†’x} \log q_{Ïƒ_t}(â‡â†’x)$, we need to minimize
the following objective:
$$ğ”¼_{t, â‡â†’xÌƒ âˆ¼ q_{Ïƒ_t}}\Big[\big\|â†’s_{â†’Î¸}(â‡â†’xÌƒ, Ïƒ_t) - âˆ‡_{â‡â†’xÌƒ} \log q_{Ïƒ_t}(â‡â†’xÌƒ)\big\|^2\Big].$$

~~~
It can be shown (see _P. Vincent: A connection between score matching and
denoising autoencoders_) that it is equivalent to minimize the _denoising score
matching_ objective:
$$ğ”¼_{t, â‡â†’x âˆ¼ p(â‡â†’x), â‡â†’xÌƒ âˆ¼ q_{Ïƒ_t}(â‡â†’xÌƒ|â‡â†’x)}\Big[\big\|â†’s_{â†’Î¸}(â‡â†’xÌƒ, Ïƒ_t) - âˆ‡_{â†’xÌƒ} \log q_{Ïƒ_t}(â‡â†’xÌƒ|â‡â†’x)\big\|^2\Big].$$

~~~
In our case, $âˆ‡_{â‡â†’xÌƒ} \log q_{Ïƒ_t}(â‡â†’xÌƒ|â‡â†’x) = âˆ‡_{â‡â†’xÌƒ} \frac{-\|â‡â†’xÌƒ-â‡â†’x\|^2}{2Ïƒ_t^2} = -\frac{â‡â†’xÌƒ-â‡â†’x}{Ïƒ_t^2}$.
~~~
Because $â‡â†’xÌƒ = â‡â†’x + Ïƒ_t â‡â†’e$ for standard normal random variable $â‡â†’e âˆ¼ ğ“(â†’0,
â‡‰I)$, we can rewrite the objective to

$$ğ”¼_{t, â‡â†’x âˆ¼ p(â‡â†’x), â‡â†’e âˆ¼ ğ“(â†’0, â‡‰I)}\Big[\big\|â†’s_{â†’Î¸}(â‡â†’x+Ïƒ_t â‡â†’e, Ïƒ_t) - \frac{-â‡â†’e}{Ïƒ_t}\big\|^2\Big],$$

so the score function basically estimates the noise given a noised image.

---
class: dbend
# Noise Conditional Score Network

Once we have trained the score function for various noise rates $Ïƒ_t$, we can
sample using annealed Langevin dynamics, where we utilize using gradually
smaller noise rates $Ïƒ_t$.

![w=100%,mw=55%,h=center](langevin_dynamics_annealed.png)![w=85%,mw=44%,h=center](annealed_langevin_dynamics_algorithm.svgz)

~~~
Such a procedure is reminiscent to the reverse diffusion process sampling.

---
section: Reading
# Development of GANs

- Martin Arjovsky, Soumith Chintala, LÃ©on Bottou: **Wasserstein GAN** https://arxiv.org/abs/1701.07875
- Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville: **Improved Training of Wasserstein GANs** https://arxiv.org/abs/1704.00028
- Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: **Progressive Growing of GANs for Improved Quality, Stability, and Variation** https://arxiv.org/abs/1710.10196
- Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida: **Spectral Normalization for Generative Adversarial Networks** https://arxiv.org/abs/1802.05957
- Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang, Weinan Zhang, Zhihua Zhang, Yong Yu: **Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets** https://arxiv.org/abs/1807.00751
- Andrew Brock, Jeff Donahue, Karen Simonyan: **Large Scale GAN Training for High Fidelity Natural Image Synthesis** https://arxiv.org/abs/1809.11096
- Tero Karras, Samuli Laine, Timo Aila: **A Style-Based Generator Architecture for Generative Adversarial Networks** https://arxiv.org/abs/1812.04948

---
# BigGAN

![w=90%,h=center](biggan_examples.svgz)

![w=90%,h=center](biggan_truncation.jpg)

---
# BigGAN

![w=90%,h=center](biggan_easy_hard.jpg)

---
# Development of VAEs

- Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu: **Neural Discrete
  Representation Learning** https://arxiv.org/abs/1711.00937

- Ali Razavi, Aaron van den Oord, Oriol Vinyals: **Generating Diverse
  High-Fidelity Images with VQ-VAE-2** https://arxiv.org/abs/1906.00446

- Patrick Esser, Robin Rombach, BjÃ¶rn Ommer: **Taming Transformers for
  High-Resolution Image Synthesis** https://arxiv.org/abs/2012.09841

- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, Ilya Sutskever: **Zero-Shot Text-to-Image Generation**
  https://arxiv.org/abs/2102.12092

- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer:
  **High-Resolution Image Synthesis with Latent Diffusion Models**
  https://arxiv.org/abs/2112.10752

---
# Development of Diffusion Models

- Yang Song, Stefano Ermon: **Generative Modeling by Estimating Gradients of the
  Data Distribution** https://arxiv.org/abs/1907.05600

- Jonathan Ho, Ajay Jain, Pieter Abbeel: **Denoising Diffusion Probabilistic
  Models** https://arxiv.org/abs/2006.11239

- Jiaming Song, Chenlin Meng, Stefano Ermon: **Denoising Diffusion Implicit
  Models** https://arxiv.org/abs/2010.02502

- Alex Nichol, Prafulla Dhariwal: **Improved Denoising Diffusion Probabilistic
  Models** https://arxiv.org/abs/2102.09672

- Prafulla Dhariwal, Alex Nichol: **Diffusion Models Beat GANs on Image
  Synthesis** https://arxiv.org/abs/2105.05233

- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer:
  **High-Resolution Image Synthesis with Latent Diffusion Models**
  https://arxiv.org/abs/2112.10752

---
# SR3 Super-Resolution via Diffusion

- Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, M. Norouzi:
  **Image Super-Resolution via Iterative Refinement** https://arxiv.org/abs/2104.07636

<div style="text-align: center"><video controls style="width: 84%">
   <source src="https://iterative-refinement.github.io/images/super_res_movie.m4v" type="video/mp4">
</video></div>

---
# Diffusion-Based Text-Conditional Image Generation

- Alex Nichol et al.: **GLIDE: Towards Photorealistic Image Generation and
  Editing with Text-Guided Diffusion Models** https://arxiv.org/abs/2112.10741

![w=68%,h=center](glide_samples.jpg)

---
# Diffusion-Based Text-Conditional Image Generation

![w=57%,h=center](glide_impainting.jpg)

---
# Diffusion-Based Text-Conditional Image Generation

- Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, et al.:
  **Photorealistic Text-to-Image Diffusion Models with Deep Language
  Understanding** https://arxiv.org/abs/2205.11487

![w=50%,h=center](imagen_examples.svgz)

---
# Normalizing Flows

- Laurent Dinh, David Krueger, Yoshua Bengio: **NICE: Non-linear Independent Components Estimation** https://arxiv.org/abs/1410.8516

- Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio: **Density estimation using Real NVP** https://arxiv.org/abs/1605.08803

- Diederik P. Kingma, Prafulla Dhariwal: **Glow: Generative Flow with Invertible 1x1 Convolutions** https://arxiv.org/abs/1807.03039

![w=41%,h=center](glow_samples.jpg)
