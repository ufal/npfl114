title: NPFL114, Lecture 13
class: title, langtech, cc-by-sa

# Generative Adversarial Networks, Diffusion Models

## Milan Straka

### May 9, 2023

---
# Generative Models

![w=46%,f=right](generative_models.png)

There are several approaches how to represent a probability distribution
$P(â‡â†’x)$. **Likelihood-based models** represent the probability density function
directly, often using an unnormalized probabilistic model (also called
energy-based model; i.e., specifying a non-zero _score_ or _density_ or
_logits_):
$$P_{â†’Î¸}(â‡â†’x) = \frac{e^{f_{â†’Î¸}(â‡â†’x)}}{Z_{â†’Î¸}}.$$

~~~
However, estimating the normalization constant $Z_{â†’Î¸} = âˆ«p_{â†’Î¸}(â‡â†’x)\dâ‡â†’x$ is
often intractable.

~~~
- We can compute $Z_{â†’Î¸}$ by restricting the model architecture (sequence
  modeling, invertible networks in normalizing flows);
~~~
- we can only approximate it (using for example variational inference as in
  VAE);
~~~
- we can use **implicit generative models**, which avoid representing likelihood
  (like GANs).

---
section: GAN
# Generative Adversarial Networks

We have a **generator** $G(â†’z; â†’Î¸_g)$, which given $â†’z âˆ¼ P(â‡â†’z)$ generates data $â†’x$.

~~~
Then we have a **discriminator** $D(â†’x; â†’Î¸_d)$, which given data $â†’x$ generates a probability
whether $â†’x$ comes from real data or is generated by a generator.

~~~
The discriminator and generator play the following game:
$$\min_G \max_D ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

![w=48%,h=center](gan_architecture.png)

---
# Generative Adversarial Networks

![w=75%,h=center](gan_training.svgz)

The generator and discriminator are alternately trained, the discriminator by
$$\argmax_{â†’Î¸_d} ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))]$$
and the generator by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

~~~
In a sense, the discriminator acts as a trainable loss for the generator.

---
# Generative Adversarial Networks

Because $\log (1 - D(G(â†’z)))$ can saturate in the beginning of the training,
where the discriminator can easily distinguish real and generated samples,
the generator can be trained by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[-\log D(G(â†’z))]$$
instead, which results in the same fixed-point dynamics, but much stronger
gradients early in learning.

---
# Generative Adversarial Networks

![w=75%,h=center](gan_algorithm.svgz)

---
# Generative Adversarial Networks

![w=68%,h=center](gan_visualization.svgz)

---
# Conditional GAN

![w=55%,h=center](cgan.svgz)

---
# Deep Convolutional GAN

In Deep Convolutional GAN, the discriminator is a convolutional network (with
batch normalization) and the generator is also a convolutional network,
utilizing transposed convolutions.

![w=100%](dcgan_architectures.svgz)

---
# Deep Convolutional GAN

![w=100%,v=middle](dcgan_lsun_architecture.png)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch5.jpg)

---
# Deep Convolutional GAN

![w=50%,h=center](dcgan_interpolation.jpg)

---
# Deep Convolutional GAN

![w=70%,h=center](dcgan_latent_arithmetic.jpg)

---
# Deep Convolutional GAN

![w=75%,h=center](dcgan_latent_arithmetic_2.jpg)

---
# Deep Convolutional GAN

![w=85%,h=center](dcgan_latent_turn.jpg)

---
# GANs Training â€” Training Experience

![w=70%,h=center](gan_tom.jpg)

---
# GANs Training â€“ Results of In-House BigGAN Training

![w=100%](gan_custom.jpg)

---
section: GANConvergence
# GANs are Problematic to Train

Unfortunately, alternating SGD steps are not guaranteed to reach even
a local optimum of a minimax problem â€“ consider the following one:
$$\min_x \max_y xâ‹…y.$$

~~~
The update rules of $x$ and $y$ for learning rate $Î±$ are
$$\begin{bmatrix} x_{n+1} \\ y_{n+1} \end{bmatrix} = \begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} \begin{bmatrix} x_n \\ y_n \end{bmatrix}.$$

~~~
The update matrix is a rotation matrix multiplied by a constant $\sqrt{1 + Î±^2} > 1$
$$\begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} = \sqrt{1 + Î±^2} â‹… \begin{bmatrix} \cos Ï† & -\sin Ï† \\ \sin Ï† & \cos Ï† \end{bmatrix},$$
so the SGD will not converge with arbitrarily small step size.

---
# GANs are Problematic to Train

![w=100%](minimax_sgd_divergence.svgz)

---
# GANs are Problematic to Train

- Mode collapse

  ![w=100%](mode_collapse.svgz)

~~~
  - If the discriminator could see the whole batch, similar samples in it would
    be candidates for fake images.

    - Batch normalization helps a lot with this.

~~~
  - Historical averaging

~~~
- Label smoothing of only positive samples helps with the gradient flow.

---
# Comparison of VAEs and GANs

The Variational Autoencoders:
- are theoretically-pleasing;
~~~
- also provide an encoder, so apart from generation, they can be used as
  unsupervised feature extraction (the VAE encoder is used in various
  modeling architectures);
~~~
- the generated samples tend to be blurry, especially with $L^1$ or $L^2$ loss
  (because of the sampling used in the reconstruction; patch-based discriminator
  with perceptual loss helps).

~~~
The Generative Adversarial Networks:
- offer high sample quality;
~~~
- are difficult to train and suffer from mode collapse.

~~~
In past few years, GANs saw a big development, improving the sample quality
substantially.
~~~
However, since 2019/2020, VAEs have shown remarkable progress
(alleviating the blurriness issue by using perceptual loss and a 2D grid of
latent variables), and are being used for generation too.
~~~
Furthermore, additional approaches (normalizing flows, diffusion models) were
also being explored, with diffusion models becoming the most promising approach
since Q2 of 2021, surpassing both VAEs and GANs.

---
section: DiffusionModels
# Diffusion Models

Currently (as of May 2023), the best architecture for generating images seems to
be the **diffusion models**.

![w=47%,h=center](../12/stable_diffusion.jpg)

~~~
The diffusion models are deeply connected to **score-based generative models**,
which were developed independently. These two approaches are in fact just
different perspectives of the same model family, and many recent papers utilize
both sides of these models.

---
style: .katex-display { margin: .7em 0 }
# Normal Distribution Reminder

![w=28%,f=right](../01/normal_distribution.svgz)

Normal (or Gaussian) distribution is a continuous distribution parametrized by
a mean $Î¼$ and variance $Ïƒ^2$:

$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

~~~
For a $D$-dimensional vector $â†’x$, the multivariate Gaussian distribution takes
the form
$$ğ“(â†’x; â†’Î¼, â‡‰Î£) â‰ \frac{1}{\sqrt{(2Ï€)^D |â‡‰Î£|}} \exp \left(-\frac{1}{2}(â†’x-â†’Î¼)^T â‡‰Î£^{-1} (â†’x-â†’Î¼) \right).$$

~~~
The biggest difference compared to the single-dimensional case is the _covariance
matrix_ $â‡‰Î£$, which is (in the non-degenerate case, which is the only one
considered here) a _symmetric positive-definite matrix_ of size $D Ã— D$.

~~~
However, in this lecture we will only consider _isotropic_ distribution, where $â‡‰Î£ = Ïƒ^2â‡‰I$:
$$ğ“(â†’x; â†’Î¼, Ïƒ^2â‡‰I) = âˆ\nolimits_i ğ“(x_i; Î¼_i, Ïƒ^2).$$

---
# Normal Distribution Reminder

A normally-distributed random variable $â‡â†’x âˆ¼ ğ“(â†’Î¼, Ïƒ^2â‡‰I)$ can be written using
the reparametrization trick also as
$$â‡â†’x = â†’Î¼ + Ïƒ â‡â†’e,\textrm{~~where~~}â‡â†’e âˆ¼ ğ“(â†’0, â‡‰I).$$

~~~
The sum of two independent normally-distributed random variables $â‡â†’x_1 âˆ¼ ğ“(â†’Î¼_1, Ïƒ_1^2â‡‰I)$
and $â‡â†’x_2 âˆ¼ ğ“(â†’Î¼_2, Ïƒ_2^2â‡‰I)$ has normal distribution $â‡ğ“\big(â†’Î¼_1 + â†’Î¼_2, (Ïƒ_1^2 + Ïƒ_2^2)â‡‰I\big)$.

~~~
Therefore, if we have three standard normal random variables $â‡â†’e_1, â‡â†’e_2, â‡â†’e âˆ¼ ğ“(â†’0, â‡‰I)$,
then
$$Ïƒ_1 â‡â†’e_1 + Ïƒ_2 â‡â†’e_2 = \sqrt{Ïƒ_1^2 + Ïƒ_2^2} â‡â†’e.$$

---
section: DDPM
# Diffusion Models â€“ Diffusion Process

![w=80%,h=center](ddpm_model.svgz)

Given a data point $â‡â†’x_0$ from a real data distribution $q(â‡â†’x)$, we define
a $T$-step _diffusion process_ (or the _forward process_) which gradually adds
Gaussian noise according to some variance schedule $Î²_1, â€¦, Î²_T$:

~~~
$$\begin{aligned}
  q(â‡â†’x_t|â‡â†’x_{t-1}) &= ğ“(â‡â†’x_t; \sqrt{1 - Î²_t} â‡â†’x_{t-1}, Î²_t â‡‰I),\\
                     &= \sqrt{1 - Î²_t} â‡â†’x_{t-1} + \sqrt{Î²_t} â‡â†’e\textrm{~~for~~}â‡â†’eâˆ¼ğ“(â†’0, â‡‰I),\\
  q(â‡â†’x_{1:T}|â‡â†’x_0) &= âˆ_{t=1}^T q(â‡â†’x_t|â‡â†’x_{t-1}).
\end{aligned}$$

~~~
More noise gets gradually added to the original image $â‡â†’x_0$, converging to
isotropic Gaussian.

---
# Diffusion Models â€“ Diffusion Process

Let $Î±_t = Î²_t$ and $Î±Ì„_t = âˆ_{i=1}^t Î±_i$.

~~~
Then we have
$$\begin{aligned}
â‡â†’x_t
  &= \sqrt{Î±_t} \textcolor{blue}{â‡â†’x_{t-1}} + \sqrt{1-Î±_t}â‡â†’e_t \\
  &= \sqrt{Î±_t} \textcolor{blue}{\big(\sqrt{Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{1-Î±_{t-1}}â‡â†’e_{t-1}\big)} + \sqrt{1-Î±_t}â‡â†’e_t \\
  &= \sqrt{Î±_t Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{Î±_t(1-Î±_{t-1}) + \sqrt{1-Î±_t}}â‡â†’eÌ„_{t-1} \\
  &= \sqrt{Î±_t Î±_{t-1}} â‡â†’x_{t-2} + \sqrt{1 - Î±_t a_{t-1}}â‡â†’eÌ„_{t-1} \\
  &= \sqrt{Î±_t Î±_{t-1} Î±_{t-2}} â‡â†’x_{t-3} + \sqrt{1 - Î±_t a_{t-1} Î±_{t-2}}â‡â†’eÌ„_{t-2} \\
  &= â€¦\\
  &= \sqrt{Î±Ì„_t} â‡â†’x_0 + \sqrt{1-Î±Ì„_t}â‡â†’eÌ„_0\\
\end{aligned}$$
for standard normal random variables $â‡â†’e_i$ and $â‡â†’eÌ„_i$.

~~~
Therefore, if $Î±Ì„_t â†’ 0$ as $t â†’ âˆ$, the $â‡â†’x_t$ converges to $ğ“(â†’0, â‡‰I)$ as $t â†’ âˆ$.

---
# Diffusion Models â€“ Noise Schedule

![w=50%,f=right](diffusion_schedule.svgz)

Originally, linearly increasing sequence of noise variations
$Î²_1=0.0001, â€¦, Î²_T=0.04$ was used.

~~~
However, the resulting sequence $Î±Ì„_t$ was not ideal (nearly the whole second
half of the diffusion process was mostly just random noise), so later a cosine
schedule was proposed:
$$Î±Ì„_t = \frac{1}{2}\Big(\cos(t/T â‹… Ï€)+1\Big),$$
and now it is dominantly used.

~~~
In practice, we want to avoid both the values of 0 and 1, and keep $Î±_t$ in $[Îµ, 1-Îµ]$ range.

---
# Diffusion Models â€“ Noise Schedule

We assume the images $â‡â†’x_0$ have zero mean and unit variance (we will normalize
them to achieve it).
~~~
Then every
$$q(â‡â†’x_t|â‡â†’x_0) = \textcolor{red}{\sqrt{Î±Ì„_t}} â‡â†’x_0 + \textcolor{blue}{\sqrt{1-Î±Ì„_t}}â‡â†’e$$
has also zero mean and unit variance.

~~~
![w=29%,f=right](signal_noise_rates.png)

The $\textcolor{red}{\sqrt{Î±Ì„_t}}$ and $\textcolor{blue}{\sqrt{1-Î±Ì„_t}}$ can be
considered as the _signal rate_ and the _noise rate_, and the cosine schedule then
corresponds to

$$\begin{aligned}
  \textcolor{red}{\sqrt{Î±Ì„_t}} &= \cos(t/T â‹… Ï€/2), \\
  \textcolor{blue}{\sqrt{1-Î±Ì„_t}} &= \sin(t/T â‹… Ï€/2).
\end{aligned}$$

---
# Diffusion Models â€“ Reverse Process

![w=80%,h=center](ddpm_model.svgz)

If we could reverse the forward process $q(â‡â†’x_t|â‡â†’x_{t-1})$, we could sample an
image by starting with $â‡â†’x_T âˆ¼ ğ“(â†’0, â‡‰I)$ and then performing the forward
process in reverse.

~~~
We therefore learn a model $p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)$ to approximate the reverse
of $q(â‡â†’x_t|â‡â†’x_{t-1})$. When $Î²_t$ is small, this reverse is nearly Gaussian,
so we represent $p_{â†’Î¸}$ as

$$p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t) = ğ“\big(â‡â†’x_{t-1}; â†’Î¼_{â†’Î¸}(â‡â†’x_t, t), Ïƒ_t^2â‡‰I\big)$$
for some fixed sequence of $Ïƒ_1, â€¦, Ïƒ_T$.
~~~
The whole reverse process is then
$$p_{â†’Î¸}(â‡â†’x_{0:T}) = p(â‡â†’x_T) âˆ\nolimits_{t=1}^T p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t).$$

---
# Diffusion Models â€“ Reverse Process

![w=70%,h=center](diffusion_forward_backward.png)

---
# Diffusion Models â€“ Loss

We now want to derive the loss. First note that the reverse of $q(â‡â†’x_t|â‡â†’x_{t-1})$
is actually tractable when conditioning on $â‡â†’x_0$:
$$\begin{aligned}
  q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0) &= ğ“\big(â‡â†’x_{t-1}; \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}, \textcolor{green}{Î²Ìƒ_t}â‡‰I\big),\\
  \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)} &= \frac{\sqrt{Î±Ì„_{t-1}}Î²_t}{1-Î±Ì„_t}â‡â†’x_0 + \frac{\sqrt{Î±_t}(1-Î±Ì„_{t-1})}{1-Î±Ì„_t}â‡â†’x_t,\\
  \textcolor{green}{Î²Ìƒ_t} &= \frac{1-Î±Ì„_{t-1}}{1-Î±Ì„_t}Î²_t.
\end{aligned}$$

~~~
The nicest proof of this I have found is available at
https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process
(it will be added to the slides later for reference, but you do not need to learn it).

---
# Diffusion Models â€“ Deriving Loss using Jensen's Inequality

$\displaystyle -ğ”¼_{q(â‡â†’x_0)}\big[\log p_{â†’Î¸}(â‡â†’x_0)\big] = -ğ”¼_{q(â‡â†’x_0)}\big[\log ğ”¼_{p_{â†’Î¸}(â‡â†’x_{1:T})}[p_{â†’Î¸}(â‡â†’x_0)]\big]$

~~~
$\displaystyle \kern2em{} = -ğ”¼_{q(â‡â†’x_0)}\Big[\log ğ”¼_{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big[\tfrac{p_{â†’Î¸}(â‡â†’x_{0:T})}{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big]\Big]$

~~~
$\displaystyle \kern2em{} â‰¤ -ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{p_{â†’Î¸}(â‡â†’x_{0:T})}{q(â‡â†’x_{1:T}|â‡â†’x_0)}\Big] = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{q(â‡â†’x_{1:T}|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{0:T})}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_t|â‡â†’x_{t-1})}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \Big(\tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)}\tfrac{q(â‡â†’x_t|â‡â†’x_0)}{q(â‡â†’x_{t-1}|â‡â†’x_0)}\Big) + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[-\log p_{â†’Î¸}(â‡â†’x_T) + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} + \log \tfrac{q(â‡â†’x_T|â‡â†’x_0)}{q(â‡â†’x_1|â‡â†’x_0)} + \log \tfrac{q(â‡â†’x_1|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\log \tfrac{q(â‡â†’x_T|â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_T)} + âˆ‘\nolimits_{t=2}^T\log \tfrac{q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0)}{p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)} -\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)\Big]$

~~~
$\displaystyle \kern2em{} = ğ”¼_{q(â‡â†’x_{0:T})}\Big[\underbrace{\scriptsize D_\textrm{KL}(q(â‡â†’x_T|â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_T))}_{L_T} + âˆ‘\nolimits_{t=2}^T\underbrace{\scriptsize D_\textrm{KL}(q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t)}_{L_t} \underbrace{-\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)}_{L_0}\Big]$

---
# Diffusion Models â€“ Deriving Loss using Jensen's Inequality

The whole loss is therefore composed of the following components:

~~~
- $L_T = D_\textrm{KL}(q(â‡â†’x_T|â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_T))$ is constant and can be
  ignored,

~~~
- $L_t = D_\textrm{KL}(q(â‡â†’x_{t-1}|â‡â†’x_t,â‡â†’x_0) \| p_{â†’Î¸}(â‡â†’x_{t-1}|â‡â†’x_t))$ is KL divergence
  between two Gaussians, so it can be computed explicitly as

  $$L_t = ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0) - â†’Î¼_{â†’Î¸}(â‡â†’x_t, t) \Big\|^2\bigg]$$
~~~
- $L_0 = -\log p_{â†’Î¸}(â‡â†’x_0|â‡â†’x_1)$ can be used to generate discrete $â‡â†’x_0$
  from the continuous $â‡â†’x_1$; we will ignore it.

---
# Diffusion Models â€“ Reparametrizing Model Prediction

Recall that
$$\begin{aligned}
  q(â‡â†’x_{t-1}|â‡â†’x_t, â‡â†’x_0) &= ğ“\big(â‡â†’x_{t-1}; \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)}, \textcolor{green}{Î²Ìƒ_t}â‡‰I\big),\\
  \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)} &= \frac{\sqrt{Î±Ì„_{t-1}}Î²_t}{1-Î±Ì„_t}â‡â†’x_0 + \frac{\sqrt{Î±_t}(1-Î±Ì„_{t-1})}{1-Î±Ì„_t}â‡â†’x_t,\\
  \textcolor{green}{Î²Ìƒ_t} &= \frac{1-Î±Ì„_{t-1}}{1-Î±Ì„_t}Î²_t.
\end{aligned}$$

~~~
Because $â‡â†’x_t = \sqrt{Î±Ì„_t} â‡â†’x_0 + \sqrt{1-Î±Ì„_t}â‡â†’e_t$, we get $â‡â†’x_0 = \frac{1}{\sqrt{Î±Ì„_t}}\big(â‡â†’x_t - \sqrt{1-Î±Ì„_t}â‡â†’e_t\big)$.

~~~
Substituting to $â†’Î¼Ìƒ_t$, we get
$$\textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)} = \frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â‡â†’e_t\Big).$$

---
# Diffusion Models â€“ Reparametrizing Model Prediction

We change our model to predict $â†’e_{â†’Î¸}(â‡â†’x_t, t)$ instead of
$â†’Î¼_{â†’Î¸}(â‡â†’x_t, t)$.
~~~
The loss $L_t$ then becomes

$$\begin{aligned}
  L_t
    &= ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| \textcolor{blue}{â†’Î¼Ìƒ_t(â‡â†’x_t, â‡â†’x_0)} - \textcolor{green}{â†’Î¼_{â†’Î¸}(â‡â†’x_t, t)} \Big\|^2\bigg] \\
    &= ğ”¼\bigg[\frac{1}{2\|Ïƒ_tâ‡‰I\|^2}\Big\| \textcolor{blue}{\frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â‡â†’e_t\Big)} - \textcolor{green}{\frac{1}{\sqrt{Î±_t}}\Big(â‡â†’x_t - \frac{1-Î±_t}{\sqrt{1-Î±Ì„_t}}â†’e_{â†’Î¸}(â‡â†’x_t, t)\Big)} \Big\|^2\bigg] \\
    &= ğ”¼\bigg[\frac{(1-Î±_t)^2}{2Î±_t(1-Î±Ì„_t)\|Ïƒ_tâ‡‰I\|^2}\Big\| â‡â†’e_t - â†’e_{â†’Î¸}(â‡â†’x_t, t) \Big\|^2\bigg] \\
    &= ğ”¼\bigg[\frac{(1-Î±_t)^2}{2Î±_t(1-Î±Ì„_t)\|Ïƒ_tâ‡‰I\|^2}\Big\| â‡â†’e_t - â†’e_{â†’Î¸}(\sqrt{á¾±_t} â‡â†’x_0 + \sqrt{1-á¾±_t}â‡â†’e_t, t) \Big\|^2\bigg].
\end{aligned}$$

~~~
The authors found that training without the weighting term performs better, so
the final loss is
$$L_t^\textrm{simple} = ğ”¼_{tâˆˆ\{1..T\},â‡â†’x_0,â‡â†’e_t}\Big[\big\| â‡â†’e_t - â†’e_{â†’Î¸}(\sqrt{á¾±_t} â‡â†’x_0 + \sqrt{1-á¾±_t}â‡â†’e_t, t) \big\|^2\Big].$$

---
# Diffusion Models â€“ Training and Sampling Algorithms

![w=100%](ddpm_algorithms.svgz)

~~~
Sampling using the proposed algorithm is slow â€“ it is common to use $T=1000$
steps during sampling.

~~~
The value of $Ïƒ_t^2$ is chosen to be either $Î²_t$ or $Î²Ìƒ_t$, or any value
in between (it can be proven that these values correspond to upper and lower
bounds on the reverse process entropy).

~~~
Both of these issues will be alleviated later when we present an updated
sampling algorithm, which runs in several tens of steps and does not use
$Ïƒ_t^2$.

---
# Diffusion Models Architecture â€“ DDPM

The diffusion models are prevalently represented using a UNet
architecture with pre-activated ResNet blocks.

~~~
- The current timestep is represented using the Transformer sinusoidal
  embeddings and added â€œin the middleâ€ of every residual block (after the
  first convolution).

~~~
- Additionally, on several lower-resolution levels, a self-attention
  block (a generalization of a single-headed self-attention, which considers
  the 2D grid of features as a sequence of feature vectors) is commonly used.

  ![w=75%,h=center](sagan.png)

---
# Diffusion Models Architecture â€“ ImaGen

![w=36%,mw=49%,h=center](imagen_architecture_overall.svgz)
~~~
![w=88%,mw=49%,h=center](imagen_architecture_block.svgz)

---
# Diffusion Models Architecture â€“ ImaGen

![w=100%,mw=51%,mh=75%,h=center](imagen_architecture_dblock.svgz)
~~~
![w=100%,mw=48%,mh=75%,h=center](imagen_architecture_ublock.svgz)

~~~
There are just minor differences in the ImaGen architecture â€“ for example the
place where the time sinusidal embeddings are added.

---
section: DDIM
# Generating Samples Faster

TBD

---
# Conditional Models, Classifier-Free Guidance

In many cases we want the generative model to be conditional.
~~~
We have already seen how to condition it on the current timestep. Additionally,
we might consider also conditioning on
- an image (e.g., for super-resolution): the image is then resized and
  concatenated with the input noised image (and optionally in other places,
  like after every resolution change);

~~~
- a text: the usual approach is to encode the text using some pre-trained
  encoder, and then to introduce an â€œimage-textâ€ attention layer (usually
  after the self-attention layers).

~~~
To make the effect of conditioning $y$ stronger during sampling, we might
also employ _classifier-free guidance_:
~~~
- During training, we sometimes train $â†’e_{â†’Î¸}(â‡â†’x_t, t, y)$ with the conditioning,
  and sometimes we train $â†’e_{â†’Î¸}(â‡â†’x_t, t, âˆ…)$ without the conditioning.
~~~
- During sampling, we include the difference between conditioned and
  unconditioned noise, weighted by the weight $w$:
  $$â†’e_{â†’Î¸}(â‡â†’x_t, t, y) + w\big(â†’e_{â†’Î¸}(â‡â†’x_t, t, y) - â†’e_{â†’Î¸}(â‡â†’x_t, t, âˆ…)\big).$$

---
section: StableDiffusion
# Stable Diffusion â€“ Semantic and Perceptual Compression

![w=65%,h=center](stable_diffusion_compression_kinds.png)

---
# Stable Diffusion â€“ Architecture

![w=100%,h=center](stable_diffusion_architecture.svgz)

---
section: Reading
# Development of GANs

- Martin Arjovsky, Soumith Chintala, LÃ©on Bottou: **Wasserstein GAN** https://arxiv.org/abs/1701.07875
- Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville: **Improved Training of Wasserstein GANs** https://arxiv.org/abs/1704.00028
- Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: **Progressive Growing of GANs for Improved Quality, Stability, and Variation** https://arxiv.org/abs/1710.10196
- Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida: **Spectral Normalization for Generative Adversarial Networks** https://arxiv.org/abs/1802.05957
- Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang, Weinan Zhang, Zhihua Zhang, Yong Yu: **Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets** https://arxiv.org/abs/1807.00751
- Andrew Brock, Jeff Donahue, Karen Simonyan: **Large Scale GAN Training for High Fidelity Natural Image Synthesis** https://arxiv.org/abs/1809.11096
- Tero Karras, Samuli Laine, Timo Aila: **A Style-Based Generator Architecture for Generative Adversarial Networks** https://arxiv.org/abs/1812.04948

---
# BigGAN

![w=90%,h=center](biggan_examples.svgz)

![w=90%,h=center](biggan_truncation.jpg)

---
# BigGAN

![w=90%,h=center](biggan_easy_hard.jpg)

---
# Development of VAEs

- Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu: **Neural Discrete
  Representation Learning** https://arxiv.org/abs/1711.00937

- Ali Razavi, Aaron van den Oord, Oriol Vinyals: **Generating Diverse
  High-Fidelity Images with VQ-VAE-2** https://arxiv.org/abs/1906.00446

- Patrick Esser, Robin Rombach, BjÃ¶rn Ommer: **Taming Transformers for
  High-Resolution Image Synthesis** https://arxiv.org/abs/2012.09841

- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, Ilya Sutskever: **Zero-Shot Text-to-Image Generation**
  https://arxiv.org/abs/2102.12092

- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer:
  **High-Resolution Image Synthesis with Latent Diffusion Models**
  https://arxiv.org/abs/2112.10752

---
# Development of Diffusion Models

- Yang Song, Stefano Ermon: **Generative Modeling by Estimating Gradients of the
  Data Distribution** https://arxiv.org/abs/1907.05600

- Jonathan Ho, Ajay Jain, Pieter Abbeel: **Denoising Diffusion Probabilistic
  Models** https://arxiv.org/abs/2006.11239

- Jiaming Song, Chenlin Meng, Stefano Ermon: **Denoising Diffusion Implicit
  Models** https://arxiv.org/abs/2010.02502

- Alex Nichol, Prafulla Dhariwal: **Improved Denoising Diffusion Probabilistic
  Models** https://arxiv.org/abs/2102.09672

- Prafulla Dhariwal, Alex Nichol: **Diffusion Models Beat GANs on Image
  Synthesis** https://arxiv.org/abs/2105.05233

- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer:
  **High-Resolution Image Synthesis with Latent Diffusion Models**
  https://arxiv.org/abs/2112.10752

---
# SR3 Super-Resolution via Diffusion

- Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, M. Norouzi:
  **Image Super-Resolution via Iterative Refinement** https://arxiv.org/abs/2104.07636

<div style="text-align: center"><video controls style="width: 84%">
   <source src="https://iterative-refinement.github.io/images/super_res_movie.m4v" type="video/mp4">
</video></div>

---
# Diffusion-Based Text-Conditional Image Generation

- Alex Nichol et al.: **GLIDE: Towards Photorealistic Image Generation and
  Editing with Text-Guided Diffusion Models** https://arxiv.org/abs/2112.10741

![w=68%,h=center](glide_samples.jpg)

---
# Diffusion-Based Text-Conditional Image Generation

![w=57%,h=center](glide_impainting.jpg)

---
# Diffusion-Based Text-Conditional Image Generation

- Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, et al.:
  **Photorealistic Text-to-Image Diffusion Models with Deep Language
  Understanding** https://arxiv.org/abs/2205.11487

![w=50%,h=center](imagen_examples.svgz)

---
# Normalizing Flows

- Laurent Dinh, David Krueger, Yoshua Bengio: **NICE: Non-linear Independent Components Estimation** https://arxiv.org/abs/1410.8516

- Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio: **Density estimation using Real NVP** https://arxiv.org/abs/1605.08803

- Diederik P. Kingma, Prafulla Dhariwal: **Glow: Generative Flow with Invertible 1x1 Convolutions** https://arxiv.org/abs/1807.03039

![w=42%,h=center](glow_samples.jpg)
