class: title
## NPFL114, Lecture 11

# Sequence Prediction,<br> Reinforcement Learning

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
class: middle, center
# Structured Prediction

# Structured Prediction

---
# Structured Prediction

Consider generating a sequence of $y\_1, \ldots, y\_N âˆˆ Y^N$ given input
$â†’x\_1, \ldots, â†’x\_N$.

--
Predicting each sequence element models the distribution $P(y\_i | â‡‰X)$.

--
There are two problems with the above approach:
1. There may be dependencies among the $y\_i$ themselves.

--

2. Even if we consider dependencies, if we compute $\softmax$ for each $y\_i$
   individually, we will hit the _label bias problem_.

![:dot 65%,center](digraph G { rankdir=LR; splines=line;
  0 -> 1 [label="x"]
  1 -> 2 [label="b"]
  2 -> 3 [label="c"]
  0 -> 4 [label="a"]
  4 -> 5 [label="b"]
  5 -> 3 [label="c"]
})

---
# Seq2seq Beam Search Decoding

So far we described only greedy decoding in a sequence to sequence decoder.

However, such decoding might not find the most probable output sequence.

We might consider a _beam search_, where we iteratively compute some fixed
number (a _beam size_) of best output sequences.

---
# Conditional Random Fields

Let $G = (V, E)$ be a graph such that $Y$ is indexed by vertices of $G$.
Then $(â‡‰X, â†’y)$ is a conditional markov field, if the random variables $â†’y$
conditioned on $â‡‰X$ obey the Markov property with respect to the graph, i.e.,
$$P(y\_i | â‡‰X, y\_j, i â‰ j) = P(y\_i | â‡‰X, y\_j, (i, j) âˆˆ E).$$

--
Usually we assume that dependencies of $â†’y$, conditioned on $â‡‰X$, form a chain.

---
# CRF Output Layer

For a sequence of $(â†’x\_1, \ldots, â†’x\_N)$ and $(y\_1, \ldots, y\_N)$, we
define a score as
$$s(â‡‰X, â†’y; â†’Î¸, â‡‰A) = âˆ‘\_{i=1}^N \big(â‡‰A\_{y\_{i-1}, y\_i} + f\_â†’Î¸(y\_i | â‡‰X)\big).$$

--
We then define
$$p(â†’y | â‡‰X) = \softmax\_{â†’z âˆˆ Y^N}\big(s(â‡‰X, â†’z)\big)\_{â†’y},$$
so that
$$\log p(â†’y | â‡‰X) = s(â‡‰X, â†’y) - \operatorname{logadd}\_{â†’z âˆˆ Y^N}(s(â‡‰X, â†’z)).$$

---
# CRF Output Layer

We can compute $p(â†’y | â‡‰X)$ efficiently using dynamic programming. If we denote
$Î±\_t(k)$ as probability of all sentences with $t$ elements with the last $y$
being $k$.

We can then show that
$$\begin{aligned}
Î±\_t(k) &= \operatorname{logadd}\_{â†’z}(s(â‡‰X, â†’z)) \\\
        &= f\_â†’Î¸(y\_t=k | â‡‰X\_{1:t}) + \operatorname{logadd}\_i(Î±\_{t-1}(i) + â‡‰A\_{i, k}).
\end{aligned}$$

For efficient implementation, we use the fact that
$$\ln(a+b) = \ln a + \ln (1 + e^{\ln b - \ln a}).$$

---
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y\_1, \ldots, y\_M$ given input
$â†’x\_1, \ldots, â†’x\_N$, but this time $M â‰¤ N$ and there is no explicit alignment
of $â†’x$ and $y$ in the gold data.

--
![:pdf 100%,,margin-top:100px](ctc_example.pdf)

---
# Connectionist Temporal Classification

We enlarge the set of output labels by a â€“ (_blank_) and perform a classification for every
input element to produce an _extended labeling_. We then post-process it by the
following rules:
1. We remove neighboring symbols.
2. We remove the â€“.

--

Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

---
# Connectionist Temporal Classification

Let us denote the probability of label $l$ at time $t$ as $p\_l^t$.

We now consider a modified label sequence by inserting a â€“ to the beginning,
to the end and between every pair of labels. Using this modified labeling
we define
$$Î±\_t(s) â‰ âˆ‘\_{\textrm{labeling }â†’Ï€\textrm{ with labels of }â†’Ï€\_{1:t} = â†’y\_{1:s}} âˆ\_{t'=1}^t p\_{â†’Ï€\_{t'}}^{t'}.$$

--
Similarly to the CRF, we can use dynamic programming to compute $Î±$ in
polynomial time.

![:pdf 37%,center](ctc_computation.pdf)

---
# CTC Decoding

Unlike CRF, we cannot easily perform the decoding in an optimal way. We
therefore either use greedy decoding, or a beam search.

![:pdf 100%](ctc_decoding.pdf)

---
class: middle, center
# Reinforcement Learning

# Reinforcement Learning

---
# Reinforcement Learning

![:pdf 85%, center](diagram.pdf)

--

A _Markov decision process_ is a quadruple $(\mathcal S, \mathcal A, P, Î³)$,
where:
- $\mathcal S$ is a set of states,
- $\mathcal A$ is a set of actions,
- $P(S\_{t+1} = s', R\_{t+1} = r | S\_t = s, A\_t = a)$ is a probability that
  action $a âˆˆ \mathcal A$ will lead from state $s âˆˆ \mathcal S$ to $s'
  âˆˆ \mathcal S$, producing a _reward_ $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a _discount factor_.

--

Let a _return_ $G\_t$ be $G\_t â‰ âˆ‘\_{k=0}^\infty Î³^k R\_{t + 1 + k}$.

---
class: middle
# K-armed Bandits

![:pdf 100%](k-armed_bandits.pdf)

---
# K-armed Bandits

Let $q\_\*(a)$ be the real _value_ of an action $a$:
$$q\_\*(a) = ğ”¼[R\_{t+1} | A\_t = a].$$

--

Denoting $Q\_t(a)$ our estimated value of action $a$ at time $t$, we would like
$Q\_t(a)$ to converge to $q\_\*(a)$.

--

A natural way to estimate $Q\_t(a)$ is
$$Q\_t(a) â‰ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

---
# K-armed Bandits

Following the definition of $Q\_t(a)$, we could choose a _greedy action_ $A\_t$ as
$$A\_t(a) â‰ \argmax_a Q\_t(a).$$

--
## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

--

An _$Îµ$-greedy_ method follows the greedy action with probability $1-Îµ$, and
chooses a uniformly random action with probability $Îµ$.

---
class: center
# K-armed Bandits

![:pdf 72%](e_greedy.pdf)

---
# K-armed Bandits

## Incremental Implementation

Let $Q\_n$ be an estimate using $n$ rewards $R\_1, \ldots, R\_n$.

$$\begin{aligned}
Q\_n &= \frac{1}{n} âˆ‘\_{i=1}^n R\_i \\\
     &= \frac{1}{n} (R\_n + \frac{n-1}{n-1} âˆ‘\_{i=1}^{n-1} R\_i) \\\
     &= \frac{1}{n} (R\_n + (n-1) Q\_{n-1}) \\\
     &= \frac{1}{n} (R\_n + n Q\_{n-1} - Q\_{n-1}) \\\
     &= Q\_{n-1} + \frac{1}{n}(R\_n - Q\_{n-1})
\end{aligned}$$

---
# K-armed Bandits

## Non-stationary Problems

Analogously to the solution obtained for a stationary problem, we consider
$$Q\_{n+1} = Q\_n + Î±(R\_{n+1} - Q\_n).$$

---
# Policies

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,<br>
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

--

## Value Function
To evaluate a quality of policy, we define _value function_ $v\_Ï€(s)$, or more
explicitly _state-value function_, as
$$v\_Ï€(s) â‰ ğ”¼\_Ï€[G\_t | S\_t = s].$$

--
An _action-value function_ for policy $Ï€$ is defined analogously as
$$q\_Ï€(s, a) â‰ ğ”¼\_Ï€[G\_t | S\_t = s, A\_t = a].$$

--
It follows that
$$q\_Ï€(s, a) = ğ”¼\_Ï€[R\_{t+1} + Î³v\_Ï€(S_{t+1}) | S\_t = s, A\_t = a].$$

---
# Optimal Policy

As value functions define an partial ordering of policies ($Ï€' â‰¥ Ï€$ if
and only if for all states $s$, $v\_{Ï€'}(s) â‰¥ v\_Ï€(s)$), it can be proven
that there always exists an _optimal policy_ $Ï€\_\*$, which is better or equal
to all other policies.

Intuitively, $Ï€\_\*(s) = \argmax\nolimits\_a q\_\*(s, a)$.

--
## Policy Improvement Theorem

Let $Ï€$ and $Ï€'$ be any pair of deterministic policies, such that
$q\_Ï€(s, Ï€'(s)) â‰¥ v\_Ï€(s)$. Then $Ï€' â‰¥ Ï€$, i.e., for all states $s$, $v\_{Ï€'}(s) â‰¥ v\_Ï€(s)$.

---
class: middle
# Monte Carlo Control

![:pdf 100%](monte_carlo.pdf)

---
# Temporal Difference Methods

![:pdf 100%](td_example.pdf)

--

![:pdf 100%](td_example_update.pdf)

---
# Temporal Difference Methods

A straightforward modification of Monte Carlo algorithm with constant-step
update and temporal difference is given by
$$Q(S\_t, A\_T) â† Q(S\_t, A\_t) + Î±[R\_{t+1} + Î³ Q(S\_{t+1}, A\_{t+1}) -Q(S\_t, A\_t)]$$
and is called _Sarsa_ ($S\_t, A\_t, R\_{t+1}, S\_{t+1}, A\_{t+1}$).

--
![:pdf 100%](sarsa.pdf)

---
# Q-learning

_Q-learning_ is another TD control algorithm by (Watkins, 1989), defined by
$$Q(S\_t, A\_T) â† Q(S\_t, A\_t) + Î±[R\_{t+1} + Î³ \max\_a Q(S\_{t+1}, a) -Q(S\_t, A\_t)].$$

--
![:pdf 100%](q_learning.pdf)

---
class: center
# Sarsa vs Q-learning

![:pdf 65%](sarsa_vs_q.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](double_q_example.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](double_q.pdf)
