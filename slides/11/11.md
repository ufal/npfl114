title: NPFL114, Lecture 11
class: title, langtech, cc-by-nc-sa

# Speech Synthesis,<br>Reinforcement Learning

## Milan Straka

### May 13, 2019

---
section: WaveNet
# WaveNet

Our goal is to model speech, using a auto-regressive model
$$p(â†’x) = âˆ_t p(x_t | x_{t-1}, â€¦, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.pdf)

---
# WaveNet

![w=100%,v=middle](wavenet_dilated_convolutions.pdf)

---
# WaveNet

## Output Distribution
The raw audio is usually stored in 16-bit samples. However, classification
into $65\,536$ classes would not be tractable, and instead WaveNet adopts
$Î¼$-law transformation and quantize the samples into 256 values using
$$\operatorname{sign}(x)\frac{\ln(1 + 255|x|)}{\ln(1 + 255)}.$$

~~~
## Gated Activation
To allow greater flexibility, the outputs of the dilated convolutions are passed
through the gated activation units
$$â†’z = \tanh(â‡‰W_f * â†’x) â‹… Ïƒ(â‡‰W_g * â†’x).$$

---
# WaveNet

![w=80%,h=center](wavenet_block.pdf)

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $â†’h$,
changing the gated activation function to
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_fâ†’h) â‹… Ïƒ(â‡‰W_g * â†’x + â‡‰V_gâ†’h).$$

## Local Conditioning
For local conditioning, we are given a timeseries $h_t$, possibly with a lower
sampling frequency. We first use transposed convolutions $â†’y = f(â†’h)$ to match resolution
and then compute analogously to global conditioning
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_f * â†’y) â‹… Ïƒ(â‡‰W_g * â†’x + â‡‰V_g * â†’y).$$
---

# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 3
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1Ã—1$ output convolution produces 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of $0.0002$

---
# WaveNet

![w=85%,h=center](wavenet_results.pdf)

---
section: ParallelWaveNet
# Parallel WaveNet

The output distribution was changed from 256 $Î¼$-law values to a Mixture of
Logistic (suggested for another paper, but reused in other architectures since):
$$Î½ âˆ¼ âˆ‘_i Ï€_i \operatorname{logistic}(Î¼_i, s_i).$$

~~~
The logistic distribution is a distribution with a $Ïƒ$ as cumulative density function
(where the mean and steepness is parametrized by $Î¼$ and $s$). Therefore, we can
write
$$Î½ âˆ¼ âˆ‘_i Ï€_i \big[Ïƒ((x + 0.5 - Î¼_i) / s_i) - Ïƒ((x - 0.5 - Î¼_i) / s_i)\big].$$
(where we replace -0.5 and 0.5 in the edge cases by $-âˆ$ and $âˆ$).

---
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we use a following trick. We will model $p(x_t)$ as $p(x_t | â†’z_{â‰¤t})$
for a _random_ $â†’z$ drawn from a logistic distribution. Then, we compute
$$x_t = z_t â‹… s(â†’z_{< t}) + Î¼(â†’z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
â€“ 4 iterations were used by the authors. In further iterations,
$$x^i_t = x^{i-1}_t â‹… s^i(â†’x^{i-1}_{< t}) + Î¼^i(â†’x^{i-1}_{< t}).$$

---
# Parallel WaveNet

The network is trained using a _probability density distillation_ using
a teacher WaveNet, using KL-divergence as loss.

![w=75%,h=center](parallel_wavenet_distillation.pdf)

---
# Parallel WaveNet

![w=80%,v=middle,h=center](parallel_wavenet_results.pdf)

---
section: Tacotron
# Tacotron

![w=65%,h=center](tacotron.pdf)

---
# Tacotron

![w=65%,h=center,v=middle](tacotron_results.pdf)

---
# Tacotron

![w=100%,v=middle](tacotron_comparison.pdf)

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s â€“ Richard Bellman
~~~
- Trial and error learning â€“ since 1850s
  - Law and effect â€“ Edward Thorndike, 1911
  - Shannon, Minsky, Clark&Farley, â€¦ â€“ 1950s and 1960s
  - Tsetlin, Holland, Klopf â€“ 1970s
  - Sutton, Barto â€“ since 1980s
~~~
- Arthur Samuel â€“ first implementation of temporal difference methods
  for playing checkers

---
# Notable Successes of Reinforcement Learning

- IBM Watson in Jeopardy â€“ 2011
~~~
- Human-level video game playing (DQN) â€“ 2013 (2015 Nature), Mnih. et al, Deepmind
  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games
~~~
- A3C â€“ 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games
~~~
- Rainbow â€“ 2017
  - human-normalized median: 153%
~~~
- Impala â€“ Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

---
# Notable Successes of Reinforcement Learning
- AlphaGo
  - Mar 2016 â€“ beat 9-dan professional player Lee Sedol
~~~
- AlphaGo Master â€“ Dec 2016
  - beat 60 professionals
  - beat Ke Jie in May 2017
~~~
- AlphaGo Zero â€“ 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero â€“ Dec 2017
  - self-play only
  - defeated AlphaGo Zero after 34 hours of training (21 million games)
  - impressive chess and shogi performance after 9h and 12h, respectively

---
# Notable Successes of Reinforcement Learning

- Dota2 â€“ Aug 2017
  - won 1v1 matches against a professional player
~~~
- MERLIN â€“ Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation
~~~
- FTW â€“ Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - trained solely by self-play on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)
~~~
- OpenAI Five â€“ Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs
    - 180 years of experience per day
~~~
- AlphaStar â€“ Jan 2019
  - played 11 games against StarCraft II professionals,
    reaching 10 wins and 1 loss

---
# Notable Successes of Reinforcement Learning

- Neural Architecture Search â€“ 2017
  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
~~~
  - AutoML: automatically discovering 
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - â€¦
~~~
- System for automatic control of data-center cooling â€“ 2017

---
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, â€¦

~~~
Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = ğ”¼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) â‰ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t(a) â‰ \argmax_a Q_t(a).$$

---
# $Îµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$Îµ$-greedy_ method follows the greedy action with probability $1-Îµ$, and
chooses a uniformly random action with probability $Îµ$.

---
# $Îµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# $Îµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} âˆ‘_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} âˆ‘_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $Îµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.pdf)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.pdf)

~~~~
# Markov Decision Process

![w=55%,h=center](diagram.pdf)

A _Markov decision process_ (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a _reward_ $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a _discount factor_ (we will always use $Î³=1$).

~~~
Let a _return_ $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ğ“¢=\{S\}$;
~~~
- an action for every arm, $ğ“=\{a_1, a_2, â€¦, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ğ“(Î¼_i, Ïƒ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ğ“(r | Î¼_i, Ïƒ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to have
separate reward distribution for every state. Such generalization is
usually called _Contextualized Bandits_ problem.
Assuming that state transitions are independent on rewards and given by
a distribution $\textit{next}(s)$, the MDP dynamics function for contextualized
bandits problem is given by
$$p(s', r | s, a_i) = ğ“(r | Î¼_{i,s}, Ïƒ_{i,s}^2) â‹… \textit{next}(s'|s).$$

---
# (State-)Value and Action-Value Functions

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define _value function_ $v_Ï€(s)$, or
_state-value function_, as
$$v_Ï€(s) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right].$$

~~~
An _action-value function_ for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
Evidently,
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_Ï€[q_Ï€(s, a)], \\
  q_Ï€(s, a) &= ğ”¼_Ï€[R_{t+1} + Î³v_Ï€(S_{t+1}) | S_t = s, A_t = a].
\end{aligned}$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
analogously
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an _optimal policy_. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.

~~~
## Existence
Under some mild assumptions, there always exists a unique optimal state-value function,
unique optimal action-value function, and (not necessarily unique) optimal policy.
The mild assumptions are that either termination is guaranteed from all
reachable states, or $Î³ < 1$.

---
section: MonteCarlo
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ğ“¢$ and we will store
estimates for each of them.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

---
# Monte Carlo Methods

To guarantee convergence, we need to visit each state infinitely many times.
One of the simplest way to achieve that is to assume _exploring starts_, where
we randomly select the first state and first action, each pair with nonzero
probability.

~~~
Furthermore, if a state-action pair appears multiple times in one episode, the
sampled returns are not independent. The literature distinguishes two cases:
- _first visit_: only the first occurence of a state-action pair in an episode is
  considered
- _every visit_: all occurences of a state-action pair are considered.

Even though first-visit is easier to analyze, it can be proven that for both
approaches, policy evaluation converges. Contrary to the Reinforcement Learning:
An Introduction book, which presents first-visit algorithms, we use every-visit.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.pdf)


---
# Monte Carlo and $Îµ$-soft Policies

A policy is called $Îµ$-soft, if
$$Ï€(a|s) â‰¥ \frac{Îµ}{|ğ“(s)|}.$$

~~~
For $Îµ$-soft policy, Monte Carlo policy evaluation also converges, without the need
of exploring starts.

~~~
We call a policy $Îµ$-greedy, if one action has maximum probability of
$1-Îµ+\frac{Îµ}{|A(s)|}$.

~~~
The policy improvement theorem can be proved also for the class of $Îµ$-soft
policies, and using<br>$Îµ$-greedy policy in policy improvement step, policy
iteration has the same convergence properties. (We can embed the $Îµ$-soft behaviour
â€œinsideâ€ the environment and prove equivalence.)

---
# Monte Carlo for $Îµ$-soft Policies

### On-policy every-visit Monte Carlo for $Îµ$-soft Policies
Algorithm parameter: small $Îµ>0$

Initialize $Q(s, a) âˆˆ â„$ arbitrarily (usually to 0), for all $s âˆˆ ğ“¢, a âˆˆ ğ“$<br>
Initialize $C(s, a) âˆˆ â„¤$ to 0, for all $s âˆˆ ğ“¢, a âˆˆ ğ“$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, â€¦, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Îµ$, generate a random uniform action
  - Otherwise, set $A_t â‰ \argmax\nolimits_a Q(S_t, a)$
- $G â† 0$
- For each $t=T-1, T-2, â€¦, 0$:
  - $G â† Î³G + R_{T+1}$
  - $C(S_t, A_t) â† C(S_t, A_t) + 1$
  - $Q(S_t, A_t) â† Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: REINFORCE
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$Ï€(a | s; â†’Î¸).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $Ï€$ instead of just $Îµ$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_Ï€(s)$ and to
that account we need to compute its _gradient_ $âˆ‡_â†’Î¸ v_Ï€(s)$.

---
# Policy Gradient Methods

In addition to discarding $Îµ$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# Policy Gradient Theorem

Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.
Let also $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$.

~~~
Then
$$âˆ‡_â†’Î¸ v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸)$$
and
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$

~~~
where $P(s â†’ â€¦ â†’ s'|Ï€)$ is probability of transitioning from state $s$ to $s'$
using 0, 1, â€¦ steps.



---
# Proof of Policy Gradient Theorem

$\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \Big[ âˆ‘\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ \big(âˆ‘\nolimits_{s'} p(s'|s, a)(r + v_Ï€(s'))\big) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \big(âˆ‘\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\big) \Big]$

~~~
_We now expand $v_Ï€(s')$._

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \Big(âˆ‘\nolimits_{s'} p(s'|s, a)\Big(\\
                \qquad\qquad\qquad âˆ‘\nolimits_{a'} \Big[ âˆ‡ Ï€(a'|s'; â†’Î¸) q_Ï€(s', a') + Ï€(a'|s'; â†’Î¸) \Big(âˆ‘\nolimits_{s''} p(s''|s', a') âˆ‡ v_Ï€(s'')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_Ï€(s'')$, we obtain the following:_

$\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸).$

---
# Proof of Policy Gradient Theorem

Recall that the initial state distribution is $h(s)$ and the on-policy
distribution under $Ï€$ is $Î¼(s)$. If we let $Î·(s)$ denote the number
of time steps spent, on average, in state $s$ in a single episode,
we have
$$Î·(s) = h(s) + âˆ‘_{s'}Î·(s') âˆ‘_a Ï€(a|s') p(s|s',a).$$

~~~
The on-policy distribution is then the normalization of $Î·(s)$:
$$Î¼(s) â‰ \frac{Î·(s)}{âˆ‘_{s'} Î·(s')}.$$

~~~
The last part of the policy gradient theorem follows from the fact that $Î¼(s)$ is
$$Î¼(s) = ğ”¼_{s_0 âˆ¼ h(s)} P(s_0 â†’ â€¦ â†’ s | Ï€).$$

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$. The loss is defined as
$$\begin{aligned}
  -âˆ‡_â†’Î¸ J(â†’Î¸) &âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) \\
              &= ğ”¼_{s âˆ¼ Î¼} âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).
\end{aligned}$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$-âˆ‡_â†’Î¸ J(â†’Î¸) âˆ ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸),$$
where we used the fact that
$$âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸) = \frac{1}{Ï€(a | s; â†’Î¸)} âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

To compute the gradient
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$-ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \ln Ï€(a | s; â†’Î¸),$$
estimating the $q_Ï€(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.pdf)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$âˆ‘_a b(s) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‘_a âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‡1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_Ï€(s, a) - v_Ï€(s)$ function is also called an _advantage function_
$$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s).$$

~~~
Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.pdf)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.pdf)
