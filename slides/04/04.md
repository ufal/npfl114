title: NPFL114, Lecture 4
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Convolutional Neural Networks

## Milan Straka

### March 7, 2022

---
section: Convolution
class: middle, center
# Going Deeper

# Going Deeper

---
# Convolutional Networks

Consider data with some structure (temporal data, speech, images, â€¦).

~~~
Unlike densely connected layers, we might want:
~~~
- local interactions only;
~~~
- shift invariance (equal response everywhere);

  ![w=20%,h=center](shift_invariance.svgz)
~~~
- parameter sharing.

---
# 1D Convolution

![w=45%,h=center](1d_convolution.svgz)

---
# 2D Convolution

![w=53%,h=center](convolution_all.svgz)

---
# 2D Convolution

![w=73%,h=center](convolution.png)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $w * x$ is defined as
$$(w * x)(t) = âˆ«x(t - a)w(a)\d a.$$

~~~
![w=100%](windowing_signal.svg)

~~~
![w=100%,mh=35%,v=middle](windowing_applied.svg)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $w * x$ is defined as
$$(w * x)(t) = âˆ«x(t - a)w(a)\d a.$$

![w=55%,h=center](convolution_animation_box.gif)

~~~
![w=55%,h=center](convolution_animation_spike.gif)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $w * x$ is defined as
$$(w * x)(t) = âˆ«x(t - a)w(a)\d a.$$

For vectors, we have
$$(â†’w * â†’x)_t = âˆ‘\nolimits_i x_{t-i} w_i.$$

~~~
Convolution operation can be generalized to two dimensions by
$$(â‡‰K * â‡‰I)_{i, j} = âˆ‘\nolimits_{m, n} â‡‰I_{i-m, j-n} â‡‰K_{m, n}.$$

~~~
Closely related is _cross-correlation_, where $K$ is flipped:
$$(â‡‰K \star â‡‰I)_{i, j} = âˆ‘\nolimits_{m, n} â‡‰I_{i+m, j+n} â‡‰K_{m, n}.$$

---
section: CNNs
# Convolution Layer

The $K$ is usually called a **kernel** or a **filter**.

~~~
Note that usually we have a whole vector of values for a single pixel,
the so-called **channels**. These single pixel channel values have no longer any
spacial structure, so the kernel contains a different set of weights for every
input dimension, obtaining

![w=10%,f=right](3d_kernel.svgz)

$$(â‡¶K \star â‡¶I)_{i, j} = âˆ‘_{m, n, c} â‡¶I_{i + m, j + n, c} â‡¶K_{m, n, c}.$$

~~~
Furthermore, we usually want to be able to specify the output dimensionality
similarly to for example a fully connected layer â€“ the number of **output
channels** for every pixel. Each output channel is then the output of an
independent convolution operation, so we can consider $â‡¶K$ to be
a four-dimensional tensor and the convolution if computed as

$$(â‡¶K \star â‡¶I)_{i, j, o} = âˆ‘_{m, n, c} â‡¶I_{i + m, j + n, c} â‡¶K_{m, n, c, o}.$$

---
# Convolution Layer

To arrive at the complete convolution layer, we need to specify:
- the width $W$ and height $H$ of the kernel;
~~~
- the number of output channels $F$;
~~~
- the **stride** denoting that every output pixel is computed for
  every **stride**-th input pixel (e.g., the output is half
  the size if stride is 2).

~~~
Considering an input image with $C$ channels, the convolution layer is then
parametrized by a kernel $â‡¶K$ of total size $W Ã— H Ã— C Ã— F$ and is computed as
$$(â‡¶K \star â‡¶I)_{i, j, o} = âˆ‘_{m, n, c} â‡¶I_{iâ‹…S + m, jâ‹…S + n, c} â‡¶K_{m, n, c, o}.$$

~~~
Note that while only local interactions are performed in the image spacial dimensions
(width and height), we combine input channels in a fully connected manner.

---
class: tablewide
style: 
# Convolution Layer

There are multiple padding schemes, most common are:
- `valid`: Only use valid pixels, which causes the result to be smaller than the input.
- `same`: Pad original image with zero pixels so that the result is exactly
  the size of the input.

Illustration of the padding schemes and different strides for a $3Ã—3$ kernel:
- **valid** padding, stride=1: ![w=50%,mw=28%,h=center](conv_valid_padding_no_strides.gif)
  stride=2: ![w=50%,mw=28%,h=center](conv_valid_padding_strides.gif)
- **same** padding, stride=1: ![w=55%,mw=28%,h=center](conv_same_padding_no_strides.gif)
  stride=2: ![w=60%,mw=28%,h=center](conv_same_padding_strides.gif)


|Padding| No stride (stride=1) | Stride=2 |
|-------|:--------------------:|:--------:|
| Valid | ![w=5em](conv_valid_padding_no_strides.gif) | ![w=5em](conv_valid_padding_strides.gif) |
| Same  | ![w=5em](conv_same_padding_no_strides.gif)  | ![w=5em](conv_same_padding_strides.gif)![w=5em](conv_same_padding_strides_odd.gif) |

---
# Convolution Layer Representation

There are two prevalent image formats (called `data_format` in TensorFlow):
- `channels_last`: The dimensions of the 4-dimensional image tensor are batch,
  height, width, and channels.

  The original TensorFlow format, faster on CPU.

~~~
- `channels_first`: The dimensions of the 4-dimensional image tensor are batch,
  channel, height, and width.

  Usual GPU format (used by CUDA and nearly all frameworks); on TensorFlow, not
  all CPU kernels are available with this layout.

~~~
In TensorFlow, data is represented using the `channels_last` approach and the
runtime will automatically convert it to `channels_first` if it is more suitable
for available hardware (especially for a GPU).

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling (minor translation invariance)
- Average pooling

![w=60%,h=center](pooling.svgz)

---
section: AlexNet
# High-level CNN Architecture

We repeatedly use the following block:
1. Convolution operation
2. Non-linear activation (usually ReLU)
3. Pooling

![w=90%,h=center](cnn.jpg)

---
# AlexNet â€“ 2012 (16.4% error)

![w=100%](alexnet.svgz)

---
# AlexNet â€“ 2012 (16.4% error)

Training details:
- 2 GPUs for 5-6 days

~~~
- SGD with batch size 128, momentum 0.9, $L^2$ regularization strength (weight
  decay) 0.0005
~~~
  - $â†’v â† 0.9 â‹… â†’v - Î± â‹… \frac{âˆ‚L}{âˆ‚â†’Î¸} - 0.0005 â‹… Î± â‹… â†’Î¸$
  - $â†’Î¸ â† â†’Î¸ + â†’v$

~~~
- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

~~~
- ReLU nonlinearities

~~~
- dropout with rate 0.5 on the fully-connected layers (except for the output layer)

~~~
- data augmentation using translations and horizontal reflections (choosing random
  $224 Ã— 224$ patches from $256 Ã— 256$ images)
~~~
  - during inference, 10 patches are used (four corner patches and a center
    patch, as well as their reflections)

---
# AlexNet â€“ ReLU vs tanh

![w=60%,h=center](relu_vs_tanh.svgz)

---
# LeNet â€“ 1998

AlexNet built on already existing CNN architectures, mostly on LeNet, which
achieved 0.8% test error on MNIST.

![w=100%,mh=80%,v=middle](lenet.svgz)

---
class: middle
# Similarities in Primary Visual Cortex (V1) and CNNs

![w=100%](gabor_functions.svgz)
The primary visual cortex recognizes Gabor functions.

---
# Similarities in Primary Visual Cortex (V1) and CNNs

![w=90%,h=center](first_convolutional_layer.svgz)
Similar functions are recognized in the first layer of a CNN.

---
section: Deep Prior
# CNNs as Regularizers â€“ Deep Prior

![w=50%,h=center](deep_prior_superresolution.jpg)

---
# CNNs as Regularizers â€“ Deep Prior

![w=95%,h=center](deep_prior_network.png)
![w=50%,mw=50%,h=center,f=right](deep_prior_network_input.svgz)

Random noise from $U[0, \frac{1}{10}]$ used on input; in large inpainting,
meshgrid is used instead and the skip-connections are not used.

---
# CNNs as Regularizers â€“ Deep Prior

![w=82%,h=center](deep_prior_inpainting.jpg)

---
# CNNs as Regularizers â€“ Deep Prior

![w=100%,mh=85%,v=bottom](deep_prior_inpainting_diversity.jpg)

~~~
[Deep Prior paper website with supplementary material](https://dmitryulyanov.github.io/deep_image_prior)

---
section: VGG
# VGG â€“ 2014 (6.8% error)

![w=48%,f=left](vgg_architecture.svgz)

![w=70%,mw=50%,h=center](inception3_conv5.png)
![w=95%,mw=50%,h=center,mh=40%,v=middle](vgg_parameters.svgz)

---
# VGG â€“ 2014 (6.8% error)

Training detail similar to AlexNet:
- SGD with batch size ~~128~~ 256, momentum 0.9, weight decay 0.0005

- initial learning rate 0.01, manually divided by 10 when validation error rate
  stopped improving

- ReLU nonlinearities

- dropout with rate 0.5 on the fully-connected layers (except for the output layer)

- data augmentation using translations and horizontal reflections (choosing random
  $224 Ã— 224$ patches from $256 Ã— 256$ images)

~~~
  - additionally, a multi-scale training and evaluation was performed. During
    training, each image was resized so that its smaller size was equal
    to $S$, which was sampled uniformly from $[256, 512]$
~~~
  - during test time, the image was rescaled three times so that the smaller
    size was $256, 384, 512$, respectively, and the results on the three images
    were averaged

---
# VGG â€“ 2014 (6.8% error)

![w=59%,h=center](vgg_test_scale_single.svgz)

~~~
![w=59%,h=center](vgg_test_scale_multi.svgz)

---
# VGG â€“ 2014 (6.8% error)

![w=100%,v=middle](vgg_inception_results.svgz)

---
section: Inception
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

Inception block:

![w=95%,h=center](inception_block.svgz)

---
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

Inception block with dimensionality reduction:

![w=88%,h=center](inception_block_reduction.svgz)

---
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

![w=93%,h=center](inception_architecture.svgz)

---
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

![w=27.5%,mw=50%,h=center](inception_graph.1.svgz)![w=40%,mw=50%,h=center](inception_graph.2.svgz)

---
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

Training details:
- SGD with momentum 0.9

~~~
- fixed learning rate schedule of decreasing the learning rate by 4% each
  8 epochs
~~~
- during test time, the image was rescaled four times so that the smaller
  size was $256, 288, 320, 352$, respectively.

~~~
  For each image, the left, center and right square was considered, and from
  each square six crops of size $224 Ã— 224$ were extracted (4 corners, middle
  crop and the whole scaled-down square) together with their horizontal flips,
  arriving at $4 â‹… 3 â‹… 6 â‹… 2 = 144$ crops per image

~~~
- 7 independently trained models were ensembled

---
# Inception (GoogLeNet) â€“ 2014 (6.7% error)

![w=100%,v=middle](inception_ablations.svgz)

---
section: BatchNorm
# Batch Normalization

**Internal covariate shift** refers to the change in the distributions
of hidden node activations due to the updates of network parameters
during training.

~~~
Let $â†’x = (x_1, \ldots, x_d)$ be $d$-dimensional input. We would like to
normalize each dimension as
$$xÌ‚_i = \frac{x_i - ğ”¼[x_i]}{\sqrt{\Var[x_i]}}.$$

~~~
Furthermore, it may be advantageous to learn suitable scale $Î³_i$ and shift $Î²_i$ to
produce normalized value
$$y_i = Î³_i xÌ‚_i + Î²_i.$$

---
# Batch Normalization

**Batch normalization** of a mini-batch of $m$ examples $(â†’x^{(1)}, \ldots, â†’x^{(m)})$ is the following:

<div class="algorithm">

**Inputs**: Mini-batch $(â†’x^{(1)}, \ldots, â†’x^{(m)})$, $Îµ âˆˆ â„$ with default value 0.001<br>
**Parameters**: $â†’Î²$ initialized to $â†’0$, $â†’Î³$ initialized to $â†’1$; both trained by the optimizer<br>
**Outputs**: Normalized batch $(â†’y^{(1)}, \ldots, â†’y^{(m)})$

~~~
- $â†’Î¼ â† \frac{1}{m} âˆ‘_{i = 1}^m â†’x^{(i)}$

~~~
- $â†’Ïƒ^2 â† \frac{1}{m} âˆ‘_{i = 1}^m (â†’x^{(i)} - â†’Î¼)^2$
~~~
- $â†’xÌ‚^{(i)} â† (â†’x^{(i)} - â†’Î¼) / \sqrt{â†’Ïƒ^2 + Îµ}$
~~~
- $â†’y^{(i)} â† â†’Î³ â†’xÌ‚^{(i)} + â†’Î²$
</div>

~~~
Batch normalization is added just before a nonlinearity $f$, and it is useless
to add bias before it (because it will cancel out). Therefore, we replace
$f(â‡‰Wâ†’x + â†’b)$ by
$$f(\textit{BN}(â‡‰Wâ†’x)).$$

---
# Batch Normalization during Inference

During inference, $â†’Î¼$ and $â†’Ïƒ^2$ are fixed (so that prediction does not depend
on other examples in a batch).

~~~
They could be precomputed after training on the whole training data, but in
practice we estimate $â†’Î¼Ì‚$ and $â†’ÏƒÌ‚^2$ during training using an
exponential moving average.

<div class="algorithm">

**Additional Inputs**: momentum $Ï„ âˆˆ â„$ with default value of 0.99<br>
**Additional Parameters**: $â†’Î¼Ì‚$ initialized to $â†’0$, $â†’ÏƒÌ‚^2$ initialized to $â†’1$; both updated manually
~~~

During training, also perform:
- $â†’Î¼Ì‚ â† Ï„â†’Î¼Ì‚ + (1 - Ï„)â†’Î¼$
- $â†’ÏƒÌ‚^2 â† Ï„â†’ÏƒÌ‚^2 + (1 - Ï„)â†’Ïƒ^2$
~~~

Batch normalization is then during inference computed as:
- $â†’xÌ‚^{(i)} â† (â†’x^{(i)} - â†’Î¼Ì‚) / \sqrt{â†’ÏƒÌ‚^2 + Îµ}$
- $â†’y^{(i)} â† â†’Î³ â†’xÌ‚^{(i)} + â†’Î²$
</div>

---
# Batch Normalization

When a batch normalization is used on a fully connected layer, each neuron
is normalized individually across the minibatch.

~~~
However, for convolutional networks we would like the normalization to
honour their properties, most notably the shift invariance. We therefore
normalize each channel across not only the minibatch, but also across
all corresponding spacial/temporal locations.

![w=70%,h=center](batch_normalization_variants.svgz)

---
# Inception with BatchNorm (4.8% error)

![w=100%,h=center](inception_batchnorm.svgz)

~~~
The BN-x5 and BN-x30 use 5/30 times larger initial learning rate, faster
learning rate decay, no dropout, weight decay smaller by a factor of 5,
and several more minor changes.

---
# Inception v2 and v3 â€“ 2015 (3.6% error)

![w=90%,mw=61%,h=center](inception3_conv5.png)
![w=90%,mw=37%,h=center](inception3_conv3.png)

---
class: middle
# Inception v2 and v3 â€“ 2015 (3.6% error)

![w=32%](inception3_inception_a.svgz)
![w=32%](inception3_inception_b.svgz)
![w=32%](inception3_inception_c.svgz)

---
# Inception v2 and v3 â€“ 2015 (3.6% error)

![w=55%,h=center](inception3_architecture.svgz)

---
# Inception v2 and v3 â€“ 2015 (3.6% error)

Training details:
- RMSProp with momentum of $Î²=0.9$ and $Îµ=1.0$

~~~
- batch size of 32 for 100 epochs

~~~
- initial learning rate of 0.045, decayed by 6% every two epochs

~~~
- gradient clipping with threshold 2.0 was used to stabilize the training

~~~
- label smoothing was first used in this paper, with $Î±=0.1$

~~~
- input image size enlarged to $299 Ã— 299$

---
# Inception v2 and v3 â€“ 2015 (3.6% error)

![w=60%,h=center](inception3_ablation.svgz)

---
# Inception v2 and v3 â€“ 2015 (3.6% error)

![w=50%,h=center](inception3_single_model.svgz)
~~~
![w=60%,h=center](inception3_ensemble.svgz)

---
section: ResNet
# ResNet â€“ 2015 (3.6% error)

![w=95%,h=center](resnet_depth_effect.svgz)

---
# ResNet â€“ 2015 (3.6% error)

![w=90%,h=center](resnet_block.svgz)

---
# ResNet â€“ 2015 (3.6% error)

![w=100%](resnet_block_reduced.svgz)

---
# ResNet â€“ 2015 (3.6% error)

![w=100%](resnet_architecture.svgz)

---
# ResNet â€“ 2015 (3.6% error)

![w=42%,mw=50%,h=center,f=left](resnet_overall.svgz)

~~~
The residual connections cannot be applied directly when
number of channels increases.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1Ã—1$ convolution + BN is used on the projections to match the
required number of channels. The required spacial resolution is achieved by
using stride 2.

---
# ResNet â€“ 2015 (3.6% error)

![w=100%,v=middle](resnet_residuals.svgz)

---
# ResNet â€“ 2015 (3.6% error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
# ResNet â€“ 2015 (3.6% error)

Training details:
- batch normalizations after each convolution and before activation

~~~
- SGD with batch size 256 and momentum of 0.9

~~~
- learning rate starts with 0.1 and is divided by 10 when error plateaus

~~~
- no dropout, weight decay 0.0001

~~~
- during testing, 10-crop evaluation strategy is used, averaging scores across
  multiple scales â€“ the images are resized so that their smaller size is in
  $\{224, 256, 384, 480, 640\}$

---
class: middle
# ResNet â€“ 2015 (3.6% error)

![w=49%](resnet_validation.svgz)
![w=49%](resnet_testing.svgz)

The ResNet-34 B uses the $1Ã—1$ convolution on residual connections with
different number of input and output channels; ResNet-34 C uses this
convolution on all residual connections. Variant B is used for
ResNet-50/101/152.

---
# Main Takeaways

- Convolutions can provide

  - local interactions in spacial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3Ã—3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double the number of channels (i.e., the first
  convolution following the pooling layer will have twice as many output
  channels).

~~~
- If your network is deep enough (the last hidden neurons have a large receptive
  fields), final fully connected layers are not needed, and global average pooling
  is enough.

~~~
- Batch normalization is a great regularization method for CNNs, allowing
  removal/decrease of dropout and $L^2$ regularization.

~~~
- Small weight decay (i.e., $L^2$ regularization) of usually 1e-4 is still useful
  for regularizing convolutional kernels.
