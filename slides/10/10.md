class: title
## NPFL114, Lecture 10

# Deep Generative Models

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Generative Models

Generative models are given a set $\mathcal X$ of realizations of a random
variable $â‡â†’x$ and their goal is to estimate $P(â†’x)$.

Usually the goal is to be able to sample from $P(â‡â†’x)$, but sometimes an
explicit calculation of $P(â†’x)$ is also possible.

---
# Deep Generative Models

![:pdf 40%,center](generative_model.pdf)

One possible approach to estimate $P(â†’x)$ is to assume that the random variable
$â‡â†’x$ depends on a _latent variable_ $â‡â†’z$:
$$P(â†’x) = P(â†’z) P(â†’x | â†’z).$$

--

We will use neural networks to estimate the conditional probability with
$P\_â†’Î¸(â†’x | â†’z)$.

---
# AutoEncoders

![:pdf 60%,center](ae.pdf)

--
- unsupervised feature extraction

- input compression

- when $â†’x + â†’Îµ$ is used as input, autoencoders can perform denoising

---
# Variational AutoEncoders

We assume $P(â‡â†’z)$ is fixed and independent on $â‡â†’x$.

We will approximate $P(â†’x | â†’z)$ using $P\_â†’Î¸(â†’x | â†’z)$. However, in order
to compute $P\_â†’Î¸(â†’z)$, we need to know $P\_â†’Î¸(â†’z | â†’x)$, which is usually
intractable.

--

We will therefore approximate $P\_â†’Î¸(â†’z | â†’x)$ by a trainable $Q\_â†’Ï†(â†’z | â†’x)$.

---
# Variational AutoEncoders

Let us define _variational lower bound_ or _evidence lower bound_ (ELBO),
denoted $\mathcal{L}(â†’Î¸, â†’Ï†;â‡â†’x)$, as
$$\mathcal{L}(â†’Î¸, â†’Ï†;â‡â†’x) = \log P\_â†’Î¸(â†’x) - D\_\textrm{KL}(Q\_â†’Ï†(â†’z | â†’x) || P\_â†’Î¸(â†’z | â†’x)).$$

--
Because KL-divergence is non-negative, $\mathcal{L}(â†’Î¸, â†’Ï†;â‡â†’x) â‰¤ \log P\_â†’Î¸(â†’x).$

--

By using simple properties of conditional and joint probability, we get that
$$\begin{aligned}
\mathcal{L}(â†’Î¸, â†’Ï†;â‡â†’x) &= ğ”¼\_{Q\_â†’Ï†(â†’z | â†’x)} [\log P\_â†’Î¸(â†’x) + \log P\_â†’Î¸(â†’z | â†’x) - \log Q\_â†’Ï†(â†’z | â†’x)] \\\
                        &= ğ”¼\_{Q\_â†’Ï†(â†’z | â†’x)} [\log P\_â†’Î¸(â†’x, â†’z) - \log Q\_â†’Ï†(â†’z | â†’x)] \\\
                        &= ğ”¼\_{Q\_â†’Ï†(â†’z | â†’x)} [\log P\_â†’Î¸(â†’x | â†’z) + \log P\_â†’Î¸(â†’z) - \log Q\_â†’Ï†(â†’z | â†’x)] \\\
                        &= ğ”¼\_{Q\_â†’Ï†(â†’z | â†’x)} [\log P\_â†’Î¸(â†’x | â†’z)] - D\_\textrm{KL}(Q\_â†’Ï†(â†’z | â†’x) || P\_â†’Î¸(â†’z)).
\end{aligned}$$


---
# Variational AutoEncoders

In order to derivate through $â†’zâˆ¼Q\_â†’Ï†(â†’z | â†’x)$, note that if
$$â†’z âˆ¼ \mathcal{N}(â†’Î¼, â†’Ïƒ^2),$$
we can write $â†’z$ as
$$â†’z âˆ¼ â†’Î¼ + â†’Ïƒ \cdot \mathcal{N}(0, 1).$$
Such formulation then allows differentiating $â†’z$ with respect to
$â†’Î¼$ and $â†’Ïƒ$.

---
class: middle
# Variational AutoEncoders

![:pdf 100%](vae_manifold.pdf)

---
class: middle
# Variational AutoEncoders

![:pdf 100%](vae_dimensionality.pdf)

---
class: center
# VAE â€“ Too High Latent Loss

![:image 65%](vae_high_latent_loss.png)

---
class: center
# VAE â€“ Too High Reconstruction Loss

![:image 65%](vae_high_reconstruction_loss.png)

---
# Generative Adversarial Networks

We have a _generator_, which given $â†’z âˆ¼ P(â‡â†’z)$ generates data $â†’x$.

We denote the generator as $G(â†’z; â†’Î¸\_g)$.

--

Then we have a _discriminator_, which given data $â†’x$ generates a probability
whether $â†’x$ comes from real data or is generated by a generator.

We denote the discrimininator as $D(â†’x; â†’Î¸\_d)$.

--

In other words, D and G play the following game:
$$\min\_G \max\_D ğ”¼\_{â†’x âˆ¼ P\_\textrm{data}}[\log D(â†’x)] + ğ”¼\_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

---
class: middle
# Generative Adversarial Networks

![:pdf 100%](gan_training.pdf)

---
class: middle
# Generative Adversarial Networks

![:pdf 100%](gan_algorithm.pdf)

---
class: center, middle
# Generative Adversarial Networks

![:pdf 95%](gan_visualization.pdf)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_architectures.png)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_lsun_architecture.png)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_lsun_epoch1.jpg)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_lsun_epoch5.jpg)

---
class: center
# Deep Convolutional GAN

![:image 68%](dcgan_interpolation.jpg)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_latent_arithmetic.jpg)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_latent_arithmetic_2.jpg)

---
class: middle
# Deep Convolutional GAN

![:image 100%](dcgan_latent_turn.jpg)

---
# GANs are Problematic to Train

- Feature matching

- Minibatch discrimination

- Historical averaging

- Label smoothing

---
class: middle
# Minibatch Discrimination

![:image 100%](gan_minibatch_discrimination.png)

---
class: center
# Wasserstein GAN

![:image 90%](wgan_gradients.png)

---
class: middle
# Wasserstein GAN

![:pdf 100%](wgan_visualization.pdf)

---
class: middle
# Wasserstein GAN

![:pdf 100%](wgan_visualization_2.pdf)
