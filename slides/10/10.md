title: NPFL114, Lecture 10
class: title, langtech, cc-by-nc-sa

# Deep Generative Models

## Milan Straka

### May 4, 2020

---
# Generative Models

Generative models are given a set $\mathcal X$ of realizations of a random
variable $â‡â†’x$ and their goal is to estimate $P(â†’x)$.

~~~
Usually the goal is to be able to sample from $P(â‡â†’x)$, but sometimes an
explicit calculation of $P(â†’x)$ is also possible.

---
# Deep Generative Models

![w=25%,h=center](generative_model.pdf)

One possible approach to estimate $P(â†’x)$ is to assume that the random variable
$â‡â†’x$ depends on a _latent variable_ $â‡â†’z$:
$$P(â†’x) = âˆ‘_â†’z P(â†’z) P(â†’x | â†’z) = ğ”¼_{â†’z âˆ¼ P(â‡â†’z)} P(â†’x | â†’z).$$

~~~
We use neural networks to estimate the conditional probability with
$P_â†’Î¸(â†’x | â†’z)$.

---
section: Autoencoders
# AutoEncoders

![w=50%,h=center](ae.pdf)

- Autoencoders are useful for unsupervised feature extraction, especially when
  performing input compression (i.e., when the dimensionality of the latent
  space $â†’z$ is smaller than the dimensionality of the input).

~~~
- When $â†’x + â†’Îµ$ is used as input, autoencoders can perform denoising.

~~~
- However, the latent space $â†’z$ does not need to be fully covered, so
  a randomly chosen $â†’z$ does not need to produce a valid $â†’x$.

---
section: VAE
# Variational AutoEncoders

We assume $P(â‡â†’z)$ is fixed and independent on $â‡â†’x$.

We approximate $P(â†’x | â†’z)$ using $P_â†’Î¸(â†’x | â†’z)$. However, in order
to train an autoencoder, we need to know the posterior $P_â†’Î¸(â†’z | â†’x)$, which is usually
intractable.

~~~
We therefore approximate $P_â†’Î¸(â†’z | â†’x)$ by a trainable $Q_â†’Ï†(â†’z | â†’x)$.

---
# Variational AutoEncoders

Let us define _variational lower bound_ or _evidence lower bound_ (ELBO),
denoted $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$, as
$$ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = \log P_â†’Î¸(â†’x) - D_\textrm{KL}\big(Q_â†’Ï†(â†’z | â†’x) || P_â†’Î¸(â†’z | â†’x)\big).$$

~~~
Because KL-divergence is non-negative, $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) â‰¤ \log P_â†’Î¸(â†’x).$

~~~
By using simple properties of conditional and joint probability, we get that

~~~
$\displaystyle \kern9em\mathllap{ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)} = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x) + \log P_â†’Î¸(â†’z | â†’x) - \log Q_â†’Ï†(â†’z | â†’x)]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x, â†’z) - \log Q_â†’Ï†(â†’z | â†’x)]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z) + \log P(â†’z) - \log Q_â†’Ï†(â†’z | â†’x)]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z)] - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P(â†’z)).$

---
# Variational AutoEncoders

$$ ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z)] - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P(â†’z))$$

- We train a VAE by maximizing $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$.
~~~
- The distribution $Q_â†’Ï†(â†’z | â†’x)$ is parametrized as a normal distribution
  $ğ“(â†’z | â†’Î¼, â†’Ïƒ^2)$, with the model predicting $â†’Î¼$ and $\log â†’Ïƒ^2$ given $â†’x$.
~~~
  - The normal distribution is used, because we can sample from it efficiently,
    we can backpropagate through it and we can compute $D_\textrm{KL}$
    analytically; furthermore, if we decide to parametrize $Q_â†’Ï†(â†’z | â†’x)$ using
    mean and variance, the maximum entropy principle suggests we should use the
    normal distribution.
~~~
- The $ğ”¼_{Q_â†’Ï†(â†’z | â†’x)}$ is estimated using a single sample.
~~~
- We use a prior $P(â†’z) = ğ“(â†’0, â†’1)$.

---
# Variational AutoEncoders

$$ ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z)] - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P(â†’z))$$

![w=50%,h=center](vae_architecture.pdf)

Note that the loss has 2 intuitive components:
- **reconstruction loss** â€“ starting with $â†’x$, passing though $Q_â†’Ï†$, sampling
  $â†’z$ and then passing through $P_â†’Î¸$ should arrive back at $â†’x$;
~~~
- **latent loss** â€“ over all $â†’x$, the distribution of $Q_â†’Ï†(â†’z | â†’x)$ should be as close as
  possible to the prior $P(â†’z) = ğ“(â†’0, â†’1)$, which is independent on $â†’x$.

---
# VAE â€“ Reparametrization Trick

In order to backpropagate through $â†’zâˆ¼Q_â†’Ï†(â†’z | â†’x)$, note that if
$$â†’z âˆ¼ ğ“(â†’Î¼, â†’Ïƒ^2),$$

~~~
we can write $â†’z$ as
$$â†’z âˆ¼ â†’Î¼ + â†’Ïƒ \cdot ğ“(â†’0, â†’1).$$

~~~
Such formulation then allows differentiating $â†’z$ with respect to
$â†’Î¼$ and $â†’Ïƒ$ and is called a _reparametrization trick_ (Kingma and Welling, 2013).

---
# Variational AutoEncoders

![w=80%,h=center](vae_manifold.pdf)

---
# Variational AutoEncoders

![w=100%,v=middle](vae_dimensionality.pdf)

---
# VAE â€“ Too High Latent Loss

![w=50%,h=center](vae_high_latent_loss.png)

---
# VAE â€“ Too High Reconstruction Loss

![w=50%,h=center](vae_high_reconstruction_loss.png)

---
section: GAN
# Generative Adversarial Networks

We have a _generator_, which given $â†’z âˆ¼ P(â‡â†’z)$ generates data $â†’x$.

We denote the generator as $G(â†’z; â†’Î¸_g)$.

~~~
Then we have a _discriminator_, which given data $â†’x$ generates a probability
whether $â†’x$ comes from real data or is generated by a generator.

We denote the discriminator as $D(â†’x; â†’Î¸_d)$.

~~~
The discriminator and generator play the following game:
$$\min_G \max_D ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

---
# Generative Adversarial Networks

![w=75%,h=center](gan_training.pdf)

The generator and discriminator are alternately trained, the discriminator by
$$\argmax_{â†’Î¸_d} ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))]$$
and the generator by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

~~~
In a sense, the discriminator acts as a trainable loss for the generator.

---
# Generative Adversarial Networks

Because $\log (1 - D(G(â†’z)))$ can saturate in the beginning of the training,
where the discriminator can easily distinguish real and generated samples,
the generator can be trained by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[-\log D(G(â†’z))]$$
instead, which results in the same fixed-point dynamics, but much stronger
gradients early in learning.

---
# Generative Adversarial Networks

![w=75%,h=center](gan_algorithm.pdf)

---
# Generative Adversarial Networks

![w=68%,h=center](gan_visualization.pdf)

---
section: CGAN
# Conditional GAN

![w=55%,h=center](cgan.pdf)

---
section: DCGAN
# Deep Convolutional GAN

In Deep Convolutional GAN, the discriminator is a convolutional network (with
batch normalization) and the generator is also a convolutional network,
utilizing transposed convolutions.

![w=100%](dcgan_architectures.pdf)

---
# Deep Convolutional GAN

![w=100%,v=middle](dcgan_lsun_architecture.png)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch1.jpg)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch5.jpg)

---
# Deep Convolutional GAN

![w=50%,h=center](dcgan_interpolation.jpg)

---
# Deep Convolutional GAN

![w=70%,h=center](dcgan_latent_arithmetic.jpg)

---
# Deep Convolutional GAN

![w=75%,h=center](dcgan_latent_arithmetic_2.jpg)

---
# Deep Convolutional GAN

![w=85%,h=center](dcgan_latent_turn.jpg)

---
section: GANConvergence
# GANs are Problematic to Train

Unfortunately, alternating SGD steps are not guaranteed to reach even
a local optimum of a minimax problem â€“ consider the following one:
$$\min_x \max_y xâ‹…y.$$

~~~
The update rules of $x$ and $y$ for learning rate $Î±$ are
$$\begin{bmatrix} x_{n+1} \\ y_{n+1} \end{bmatrix} = \begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} \begin{bmatrix} x_n \\ y_n \end{bmatrix}.$$

~~~
The update matrix is a rotation matrix multiplied by a constant $\sqrt{1 + Î±^2} > 1$
$$\begin{bmatrix} 1 & -Î± \\ Î± & 1 \end{bmatrix} = \sqrt{1 + Î±^2} â‹… \begin{bmatrix} \cos Ï† & -\sin Ï† \\ \sin Ï† & \cos Ï† \end{bmatrix},$$
so the SGD will not converge with arbitrarily small step size.

---
# GANs are Problematic to Train

![w=100%](minimax_sgd_divergence.pdf)

---
# GANs are Problematic to Train

- Mode collapse

  ![w=100%](mode_collapse.pdf)

~~~
  - If the generator could see the whole batch, similar samples in it would
    be candidates for fake images.

    - Batch normalization helps a lot with this.

~~~
  - Historical averaging

~~~
- Label smoothing of only positive samples helps with the gradient flow.

---
section: WGAN
class: dbend
# Wasserstein GAN

Standard GANs optimize the Jensen-Shannon divergence
$$JS(p, q) = \frac{1}{2} D_\textrm{KL}\big(p || (p+q)/2\big) + \frac{1}{2} D_\textrm{KL}\big(q || (p+q)/2\big),$$
because for a fixed generator $G$, the optimum discriminator $D^*_G(â†’x) = \frac{P_\textrm{data}(â†’x)}{P_\textrm{data}(â†’x) + P_\textrm{generator}(â†’x)}.$

~~~
Therefore,
$$\begin{aligned}
  & ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D^*_G(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D^*_G(G(â†’z)))] \\
={} & ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D^*_G(â†’x)] + ğ”¼_{â†’x âˆ¼ P_\textrm{generator}}[\log (1 - D^*_G(â†’x)))] \\
={} & ğ”¼_{â†’x âˆ¼ P_\textrm{data}}\left[\log\frac{P_\textrm{data}(â†’x)}{P_\textrm{data}(â†’x) + P_\textrm{generator}(â†’x)}\right] + ğ”¼_{â†’x âˆ¼ P_\textrm{generator}}\left[\log \frac{P_\textrm{generator}(â†’x)}{P_\textrm{data}(â†’x) + P_\textrm{generator}(â†’x)}\right] \\
={} & D_\textrm{KL}\left(P_\textrm{data}\middle\Vert\frac{P_\textrm{data} + P_\textrm{generator}}{2}\right) + D_\textrm{KL}\left(P_\textrm{generator}\middle\Vert\frac{P_\textrm{data} + P_\textrm{generator}}{2}\right) + c.
\end{aligned}$$

---
class: dbend
# Wasserstein GAN

Instead of minimizing JS divergence
$$JS(p, q) = \frac{1}{2} D_\textrm{KL}\big(p || (p+q)/2\big) + \frac{1}{2} D_\textrm{KL}\big(q || (p+q)/2\big),$$

~~~
Wasserstein GAN minimizes Earth-Mover distance
$$W(p, q) = \inf_{Î³ âˆˆ Î (p, q)} ğ”¼_{(x, y) âˆ¼ Î³} \big[ ||x - y|| \big].$$

~~~
The joint distribution $Î³ âˆˆ Î (p, q)$ indicates how much â€œmassâ€ must be transported
from $x$ to $y$, and EM is the â€œcostâ€ of the optimal transport plan.

---
class: dbend
# Wasserstein GAN

The EM distance behaves much better than JS.

~~~
For example, imagine that $P_0$ is a distribution on $â„^2$, which is uniform on
$(0, y)$ for $0 â‰¤ y â‰¤ 1$ and that $P_Î¸$ is a distribution on $â„^2$ uniform on
$(Î¸, y)$ for $0 â‰¤ y â‰¤ 1$.

~~~
Then

![w=60%,f=right](wgan_metrics.pdf)

$$JS(P_0, P_Î¸) = \begin{cases} 0 & \textrm{if~} Î¸ = 0 \\ \log_2 & \textrm{if~} Î¸ â‰  0\end{cases},$$
while
$$W(P_0, P_Î¸) = |Î¸|.$$

---
class: dbend
# Wasserstein GAN

Using a dual version of the Earth-Mover definition, we arrive at
$$W(p, q) = \sup_{f, ||f||_L â‰¤ 1} ğ”¼_{xâˆ¼p} \big[f(x)\big] - ğ”¼_{yâˆ¼q} \big[f(x)\big],$$

so the discriminator returns a single output without activation and it needs to be 1-Lipschitz.

~~~
![w=42%,h=center](wgan_gradients.png)

---
# Wasserstein GAN

![w=80%,h=center](wgan_algorithm.pdf)

---
# Wasserstein GAN

![w=80%,h=center](wgan_visualization.pdf)

---
# Wasserstein GAN

![w=100%,v=middle](wgan_visualization_2.pdf)

---
section: *GAN
# Development of GANs

Generative Adversarial Networks are still in active development:

- Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: **Progressive Growing of GANs for Improved Quality, Stability, and Variation** https://arxiv.org/abs/1710.10196

- Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida: **Spectral Normalization for Generative Adversarial Networks** https://arxiv.org/abs/1802.05957

- Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang, Weinan Zhang, Zhihua Zhang, Yong Yu: **Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets** https://arxiv.org/abs/1807.00751

- Andrew Brock, Jeff Donahue, Karen Simonyan: **Large Scale GAN Training for High Fidelity Natural Image Synthesis** https://arxiv.org/abs/1809.11096

- Tero Karras, Samuli Laine, Timo Aila: **A Style-Based Generator Architecture for Generative Adversarial Networks** https://arxiv.org/abs/1812.04948

Alternative approaches are also explored: Diederik P. Kingma, Prafulla Dhariwal: **Glow: Generative Flow with Invertible 1x1 Convolutions** https://arxiv.org/abs/1807.03039

---
section: BigGAN
# BigGAN

![w=90%,h=center](biggan_examples.pdf)

![w=90%,h=center](biggan_truncation.jpg)

---
# BigGAN

![w=90%,h=center](biggan_easy_hard.jpg)

---
class: dbend
# BigGAN Ingredients â€“ Hinge Loss

The Wasserstein GAN formulation can be considered a linear classifier, which
tries to maximize the mean distance of real and generated images using their
features.

![w=50%,h=center](wgan_interpretation.pdf)

---
class: dbend
# BigGAN Ingredients â€“ Hinge Loss

We could aim for maximum margin classifier by using Hinge loss, updating the
discriminator by

![w=42%,f=right](hinge_gan_interpretation.pdf)

$$\begin{aligned}
  \argmax_{â†’Î¸_d}~& ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\min(0, -1 + D(â†’x)] \\
                 & + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\min(0, -1 - D(G(â†’z))]
\end{aligned}$$
and the generator by
$$\argmin_{â†’Î¸_g} ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[- D(G(â†’z)))].$$

---
class: dbend
# BigGAN Ingredients â€“ Spectral Normalization

Satisfying the Lipschitz constraint by truncation is not very effective.
Better approaches were proposed, by using for example gradient penalties
(WGAN-GP) or _spectral normalization_.

~~~
In spectral normalization, the idea is to keep the _spectral norm_ (the largest
singular value) of all convolutional and dense layers equal or close to 1,
which in turn guarantees the Lipschitz constraint of the model.

~~~
![w=70,f=right](spectral_normalization.pdf)

Spectral normalization can be implemented efficiently by performing one step of
power iteration each time the kernel in question is used in training.

---
class: dbend
# BigGAN Ingredients â€“ Self Attention

Because convolutions process local information only, non-local _self attention_
module has been proposed.

![w=100%](sagan.pdf)

---
class: dbend
# BigGAN Ingredients â€“ Self Attention

```python
def attention(self, x, ch):
  f = conv(x, ch // 8, kernel=1, stride=1) # [bs, h, w, c']
  g = conv(x, ch // 8, kernel=1, stride=1) # [bs, h, w, c']
  h = conv(x, ch, kernel=1, stride=1) # [bs, h, w, c]

  # N = h * w
  s = tf.matmul(
        hw_flatten(g), hw_flatten(f), transpose_b=True) # [bs, N, N]
  beta = tf.nn.softmax(s)  # attention map

  o = tf.matmul(beta, hw_flatten(h)) # [bs, N, C]
  gamma = tf.get_variable("gamma", initializer=[0.0])

  o = tf.reshape(o, shape=x.shape) # [bs, h, w, C]
  x = gamma * o + x
  return x
```

---
class: dbend
# BigGAN Ingredients â€“ Architecture

![w=90%](biggan_architecture_blocks.pdf)

---
class: dbend
# BigGAN Ingredients â€“ Architecture

![w=90%](biggan_architecture_table.pdf)

---
class: dbend
# BigGAN Ingredients â€“ Architecture

![w=90%](biggan_architecture_table.pdf)

---
class: dbend
# BigGAN Ingredients â€“ Truncation Trick

The so-called **tuncation trick** is used to trade between fidelity and variety
â€“ during training, $â†’z$ is sampled from $ğ“(â†’0, â†’1)$, while it is sampled from
_truncated normal_ during generation.

In the following examle, samples were generated using threshold $2, 1, 0.5,
0.04$.

![w=90%](biggan_truncation.jpg)
