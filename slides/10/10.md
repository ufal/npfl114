title: NPFL114, Lecture 10
class: title, langtech, cc-by-nc-sa

# Deep Generative Models

## Milan Straka

### May 6, 2019

---
# Generative Models

Generative models are given a set $\mathcal X$ of realizations of a random
variable $â‡â†’x$ and their goal is to estimate $P(â†’x)$.

~~~
Usually the goal is to be able to sample from $P(â‡â†’x)$, but sometimes an
explicit calculation of $P(â†’x)$ is also possible.

---
# Deep Generative Models

![w=25%,h=center](generative_model.pdf)

One possible approach to estimate $P(â†’x)$ is to assume that the random variable
$â‡â†’x$ depends on a _latent variable_ $â‡â†’z$:
$$P(â†’x) = P(â†’z) P(â†’x | â†’z).$$

~~~
We use neural networks to estimate the conditional probability with
$P_â†’Î¸(â†’x | â†’z)$.

---
section: Autoencoders
# AutoEncoders

![w=60%,h=center](ae.pdf)

~~~
- unsupervised feature extraction

~~~
- input compression for $z < x$

~~~
- when $â†’x + â†’Îµ$ is used as input, autoencoders can perform denoising

---
section: VAE
# Variational AutoEncoders

We assume $P(â‡â†’z)$ is fixed and independent on $â‡â†’x$.

We approximate $P(â†’x | â†’z)$ using $P_â†’Î¸(â†’x | â†’z)$. However, in order
to train an autoencoder, we need to know the posterior $P_â†’Î¸(â†’z | â†’x)$, which is usually
intractable.

~~~
We therefore approximate $P_â†’Î¸(â†’z | â†’x)$ by a trainable $Q_â†’Ï†(â†’z | â†’x)$.

---
# Variational AutoEncoders

Let us define _variational lower bound_ or _evidence lower bound_ (ELBO),
denoted $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$, as
$$ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = \log P_â†’Î¸(â†’x) - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P_â†’Î¸(â†’z | â†’x)).$$

~~~
Because KL-divergence is non-negative, $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) â‰¤ \log P_â†’Î¸(â†’x).$

~~~
By using simple properties of conditional and joint probability, we get that
$$\begin{aligned}
ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) &= ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x) + \log P_â†’Î¸(â†’z | â†’x) - \log Q_â†’Ï†(â†’z | â†’x)] \\
              &= ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x, â†’z) - \log Q_â†’Ï†(â†’z | â†’x)] \\
              &= ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z) + \log P_â†’Î¸(â†’z) - \log Q_â†’Ï†(â†’z | â†’x)] \\
              &= ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z)] - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P_â†’Î¸(â†’z)).
\end{aligned}$$

---
# Variational AutoEncoders
$$ ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = ğ”¼_{Q_â†’Ï†(â†’z | â†’x)} [\log P_â†’Î¸(â†’x | â†’z)] - D_\textrm{KL}(Q_â†’Ï†(â†’z | â†’x) || P_â†’Î¸(â†’z))$$

We train a VAE by maximizing $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$, taking a single point estimate of the expectation and using
a prior $P_â†’Î¸(â†’z) = ğ“(0, 1)$.

~~~
Note that the loss has 2 intuitive components:
- reconstruction loss: Starting with $â†’x$, and passing though $Q$ and then again
  through $P$ should arrive back at $â†’x$.
~~~
- latent loss: The distribution of $Q_â†’Ï†(â†’z | â†’x)$ should be as close to the prior $P_â†’Î¸(â†’z) = ğ“(0, 1)$,
  which is independent on $â†’x$.

---
section: ReparametrizationTrick
# Variational AutoEncoders

In order to derivate through $â†’zâˆ¼Q_â†’Ï†(â†’z | â†’x)$, note that if
$$â†’z âˆ¼ \mathcal{N}(â†’Î¼, â†’Ïƒ^2),$$

~~~
we can write $â†’z$ as
$$â†’z âˆ¼ â†’Î¼ + â†’Ïƒ \cdot \mathcal{N}(0, 1).$$

~~~
Such formulation then allows differentiating $â†’z$ with respect to
$â†’Î¼$ and $â†’Ïƒ$ and is called a _reparametrization trick_ (Kingma and Welling, 2013).

---
# Variational AutoEncoders

![w=80%,h=center](vae_manifold.pdf)

---
# Variational AutoEncoders

![w=100%,v=middle](vae_dimensionality.pdf)

---
# VAE â€“ Too High Latent Loss

![w=50%,h=center](vae_high_latent_loss.png)

---
# VAE â€“ Too High Reconstruction Loss

![w=50%,h=center](vae_high_reconstruction_loss.png)

---
section: GAN
# Generative Adversarial Networks

We have a _generator_, which given $â†’z âˆ¼ P(â‡â†’z)$ generates data $â†’x$.

We denote the generator as $G(â†’z; â†’Î¸_g)$.

~~~
Then we have a _discriminator_, which given data $â†’x$ generates a probability
whether $â†’x$ comes from real data or is generated by a generator.

We denote the discrimininator as $D(â†’x; â†’Î¸_d)$.

~~~
The discriminator and generator play the following game:
$$\min_G \max_D ğ”¼_{â†’x âˆ¼ P_\textrm{data}}[\log D(â†’x)] + ğ”¼_{â†’z âˆ¼ P(â‡â†’z)}[\log (1 - D(G(â†’z)))].$$

---
# Generative Adversarial Networks

![w=100%,v=middle](gan_training.pdf)

---
# Generative Adversarial Networks

![w=75%,h=center](gan_algorithm.pdf)

---
# Generative Adversarial Networks

![w=68%,h=center](gan_visualization.pdf)

---
section: CGAN
# Conditional GAN

![w=55%,h=center](cgan.pdf)

---
section: DCGAN
# Deep Convolutional GAN

![w=100%,v=middle](dcgan_architectures.png)

---
# Deep Convolutional GAN

![w=100%,v=middle](dcgan_lsun_architecture.png)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch1.jpg)

---
# Deep Convolutional GAN

![w=100%](dcgan_lsun_epoch5.jpg)

---
# Deep Convolutional GAN

![w=50%,h=center](dcgan_interpolation.jpg)

---
# Deep Convolutional GAN

![w=70%,h=center](dcgan_latent_arithmetic.jpg)

---
# Deep Convolutional GAN

![w=75%,h=center](dcgan_latent_arithmetic_2.jpg)

---
# Deep Convolutional GAN

![w=85%,h=center](dcgan_latent_turn.jpg)

---
# GANs are Problematic to Train

![w=100%](mode_collapse.pdf)

- Feature matching

~~~
- Minibatch discrimination

~~~
- Historical averaging

~~~
- Label smoothing

---
class: middle
# Minibatch Discrimination

![w=75%,h=center](gan_minibatch_discrimination.png)

---
section: WGAN
# Wasserstein GAN

Instead of minimizing JS divergence
$$JS(p, q) = KL(p || q) + KL(q || p),$$

~~~
Wasserstein GAN minimizes Earth-Mover distance
$$W(p, q) = \inf_{Î³ âˆˆ Î (p, q)} ğ”¼_{(x, y) âˆ¼ Î³} \big[ ||x - y|| \big].$$

~~~
The joint distribution $Î³ âˆˆ Î (p, q)$ indicates how much â€œmassâ€ must be transported
from $x$ to $y$, and EM is the â€œcostâ€ of the optimal transport plan.

---
# Wasserstein GAN

Using a dual version of the Earth-Mover definition, we arrive at
$$W(p, q) = \sup_{f, ||f||_L â‰¤ 1} ğ”¼_{xâˆ¼p} \big[f(x)\big] - ğ”¼_{yâˆ¼q} \big[f(x)\big].$$

![w=45%,h=center](wgan_gradients.png)

---
# Wasserstein GAN

![w=80%,h=center](wgan_algorithm.pdf)

---
# Wasserstein GAN

![w=80%,h=center](wgan_visualization.pdf)

---
# Wasserstein GAN

![w=100%,v=middle](wgan_visualization_2.pdf)

---
section: *GAN
# Development of GANs

Generative Adversarial Networks are still in active development:

- Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen: **Progressive Growing of GANs for Improved Quality, Stability, and Variation** https://arxiv.org/abs/1710.10196

- Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida: **Spectral Normalization for Generative Adversarial Networks** https://arxiv.org/abs/1802.05957

- Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang, Weinan Zhang, Zhihua Zhang, Yong Yu: **Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets** https://arxiv.org/abs/1807.00751

- Andrew Brock, Jeff Donahue, Karen Simonyan: **Large Scale GAN Training for High Fidelity Natural Image Synthesis** https://arxiv.org/abs/1809.11096

- Tero Karras, Samuli Laine, Timo Aila: **A Style-Based Generator Architecture for Generative Adversarial Networks** https://arxiv.org/abs/1812.04948

Alternative approaches are also explored: Diederik P. Kingma, Prafulla Dhariwal: **Glow: Generative Flow with Invertible 1x1 Convolutions** https://arxiv.org/abs/1807.03039
