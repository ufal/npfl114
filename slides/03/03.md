title: NPFL114, Lecture 3
class: title, langtech, cc-by-nc-sa

# Training Neural Networks II

## Milan Straka

### February 28, 2022

---
section: NNTraining
# Putting It All Together

Let us have a dataset with training, validation and test sets, each containing
examples $(â†’x, y)$. Depending on $y$, consider one of the following output
activation functions:
$$\begin{cases}
  \textrm{none} & \textrm{ if } y âˆˆ â„,\\
  Ïƒ & \textrm{ if } y \textrm{ is a probability of an outcome},\\
  \softmax & \textrm{ if } y \textrm{ is a gold class index out of $K$ classes (or a full distribution)}.
\end{cases}$$

If $â†’x âˆˆ â„^D$, we can use a neural network with an input layer of size $D$,
hidden layer of size $H$ with a non-linear activation function, and an output
layer of size $O$ (either 1 or number of classification classes) with the
mentioned output function.

~~~
_BTW, there are of course many functions, which could be used as output
activations instead of $Ïƒ$ and $\softmax$; however, $Ïƒ$ and $\softmax$ are
almost universally used. One of the reason is that they can be derived
using the maximum-entropy principle from a set of conditions, see the
[Machine Learning for Greenhorns (NPFL129) lecture 5 slides](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/?05).
Additionally, they are the inverses of [canonical link functions](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)
of the Bernoulli and categorical distributions, respectively._

---
# Putting It All Together
![w=90%,mw=50%,f=left](../01/neural_network.svg)

~~~
We have
$$h_i = f^{(1)}\left(âˆ‘_j W^{(1)}_{i,j} x_j + b^{(1)}_i\right)$$
where
- $â‡‰W^{(1)} âˆˆ â„^{HÃ—D}$ is a matrix of _weights_,
- $â†’b^{(1)} âˆˆ â„^H$ is a vector of _biases_,
- $f^{(1)}$ is an activation function.

~~~
The weights are sometimes also called a _kernel_.

~~~
The biases define general behaviour in case of zero/very small input.

~~~
Transformations of type $â‡‰W^{(1)} â†’x + â†’b$ are called _affine_ instead of _linear_.

--- -----
Similarly
$$o_i = f^{(2)}\left(âˆ‘_j W^{(2)}_{i,j} h_j + b^{(2)}_i\right)$$
with
- $â‡‰W^{(2)} âˆˆ â„^{OÃ—H}$ another matrix of weights,
- $â†’b^{(2)} âˆˆ â„^O$ another vector of biases,
- $f^{(2)}$ being an output activation function.

---
# Putting It All Together

The parameters of the model are therefore $â‡‰W^{(1)}, â‡‰W^{(2)}, â†’b^{(1)}, â†’b^{(2)}$ of total size
$DÃ—H + HÃ—O + H + O$.

~~~
To train the network, we repeatedly sample $m$ training examples and perform
SGD (or any of its adaptive variants), updating the parameters to minimize the
loss:

$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚L}{âˆ‚Î¸_i},\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸ - Î±\frac{âˆ‚L}{âˆ‚â†’Î¸}.$$

~~~
We set the hyperparameters (size of the hidden layer, hidden layer activation
function, learning rate, â€¦) using performance on the validation set and evaluate
generalization error on the test set.

---
# Practical Issues

- Processing all data in _batches_.

~~~
- Vector representation of the network.

~~~
  Instead of $h_i = f^{(1)}\left(âˆ‘_j W^{(1)}_{i,j} x_j + b^{(1)}_i\right)$,
  we compute
  $$â†’h = f^{(1)}\left(â‡‰W^{(1)} â†’x + â†’b^{(1)}\right)$$
  $$â†’o = f^{(2)}\left(â‡‰W^{(2)} â†’h + â†’b^{(2)}\right) = f^{(2)}\left(â‡‰W^{(2)} \left(f^{(1)}\left(â‡‰W^{(1)} â†’x + â†’b^{(1)}\right)\right) + â†’b^{(2)}\right)$$

~~~
  The derivatives
  $$\frac{âˆ‚f^{(1)}\left(â‡‰W^{(1)} â†’x + â†’b^{(1)}\right)}{âˆ‚â†’x},
  \frac{âˆ‚f^{(1)}\left(â‡‰W^{(1)} â†’x + â†’b^{(1)}\right)}{âˆ‚â‡‰W^{(1)}}$$
  are then matrices (called _Jacobians_) or even higher-dimensional tensors.

---
class: center, middle
# Computation Graph

![w=35%](../01/neural_network.svg)
â†’
![w=26%](computation_graph.svg)

---
class: tablefull
# High Level Overview

|                 | Classical ('90s) | Deep Learning |
|-----------------|------------------|---------------|
|Architecture     | $\vdots\,\,\vdots\,\,\vdots$ | $\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots$ &nbsp; CNN, RNN, Transformer, VAE, GAN, â€¦
|ActivationÂ func. | $\tanh, Ïƒ$    | $\tanh$, ReLU, PReLU, ELU, GELU, Swish (SiLU), Mish, â€¦
|OutputÂ function  | none, $Ïƒ$     | none, $Ïƒ$, $\softmax$
|Loss function    | MSE           | NLL (or cross-entropy or KL-divergence)
|Optimization     | SGD, momentum | SGD (+ momentum), RMSProp, Adam, SGDW, AdamW, â€¦
|Regularization   | L2, L1        | L2, Dropout, Label smoothing, BatchNorm, LayerNorm, MixUp, WeightStandardization, â€¦

---
section: Metrics&Losses
# Metrics and Losses

During training and evaluation, we use two kinds of error functions:
~~~
- **loss** is a _differentiable_ function used during training,
~~~
  - NLL, MSE, Huber loss, Hinge, â€¦
~~~
- **metric** is any (and very often non-differentiable) function used during
  evaluation,
~~~
  - any loss, accuracy, F-score, BLEU, â€¦
~~~
  - possibly even human evaluation.

~~~
In TensorFlow, the losses and metrics are available in `tf.losses` and
`tf.metrics` (aliases for `tf.keras.losses` and `tf.keras.metrics`).

---
# TF Losses

The `tf.losses` offer two sets of APIs. The current ones are loss classes like
```python
tf.losses.MeanSquaredError(
    reduction=tf.losses.Reduction.AUTO, name='mean_squared_error'
)
```

~~~
The created objects are subclasses of `tf.losses.Loss` and can be always called
with three arguments:
```python
__call__(y_true, y_pred, sample_weight=None)
```
which returns the loss of the given data, _reduced_ using the specified
reduction. If `sample_weight` is given, it is used to weight (multiply) the
individual batch example losses before reduction.

~~~
- `tf.losses.Reduction.SUM_OVER_BATCH_SIZE`, which is the default of `.AUTO`;
~~~
- `tf.losses.Reduction.SUM`;
~~~
- `tf.losses.Reduction.NONE`.

---
# TF Cross-entropy Losses

The cross-entropy losses need to specify also the distribution in question:
~~~
- `tf.losses.BinaryCrossentropy`: the gold and predicted distributions are
  Bernoulli distributions (i.e., a single probability);
~~~
- `tf.losses.CategoricalCrossentropy`: the gold and predicted distributions are
  categorical distributions;
~~~
- `tf.losses.SparseCategoricalCrossentropy`: a special case, where the gold
  distribution is one-hot distribution (i.e., a single correct class), which
  is represented as the gold _class index_; therefore, it has one less dimension
  than the predicted distribution.

~~~
These losses expect probabilities on input, but offer `from_logits` argument,
which can be used to indicate that logits are used instead of probabilities.

~~~
## Old losses API

In addition to the loss objects, `tf.losses` offers methods like
`tf.losses.mean_squared_error`, which process two arguments `y_true` and
`y_pred` and do not reduce the batch example losses.

---
# TF Metrics

There are two important differences between metrics and losses.
1. metrics may be non-differentiable;
~~~
1. metrics **aggregate** results over multiple batches.

~~~
The metric objects are subclasses of `tf.losses.Metric` and offer the following
methods:
- `update_state(y_true, y_pred, sample_weight=None)` updates the value of the
  metric and stores it;
~~~
- `result()` returns the current value of the metric;
~~~
- `reset_states()` clears the stored state of the metric.

~~~
The most common pattern is using the provided
```python
__call__(y_true, y_pred, sample_weight=None)
```
method, which is a combination of `update_state` followed by a `result()`.

---
# TF Metrics

Apart from analogues of the losses
- `tf.metrics.MeanSquaredError`
- `tf.metrics.BinaryCrossentropy`
- `tf.metrics.CategoricalCrossentropy`
- `tf.metrics.SparseCategoricalCrossentropy`

the `tf.metrics` module provides
~~~
- `tf.metrics.Mean` computing averaged mean;
~~~
- `tf.metrics.Accuracy` returning accuracy, which is an average number of examples
  where the prediction is equal to the gold value;
~~~
- `tf.metrics.BinaryAccuracy` returning accuracy of predicting a Bernoulli
  distribution (the gold value is 0/1, the prediction is a probability);
~~~
- `tf.metrics.CategoricalAccuracy` returning accuracy of predicting a Categorical
  distribution (the argmaxes of gold and predicted distributions are equal);
~~~
- `tf.metrics.SparseCategoricalAccuracy` is again a special case of
  `CategoricalAccuracy`, where the gold distribution is represented as the gold
  class _index_.

---
section: âˆ‚Loss
# Derivative of MSE Loss

Given the MSE loss of
$$L = \big(y - yÌ‚(â†’x; â†’Î¸)\big)^2 = \big(yÌ‚(â†’x; â†’Î¸) - y\big)^2,$$
the derivative with respect to $yÌ‚$ is simply:
$$\frac{âˆ‚L}{yÌ‚(â†’x; â†’Î¸)} = 2\big(yÌ‚(â†’x; â†’Î¸) - y\big).$$

---
# Derivative of Softmax MLE Loss

![w=38%,h=center](softmax.svg)

Let us have a softmax output layer with
$$o_i = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}.$$

---
# Derivative of Softmax MLE Loss

Consider now the MLE estimation. The loss for gold class index $\textit{gold}$ is then
$$L(\softmax(â†’z), \textit{gold}) = - \log o_\textit{gold}.$$

~~~
The derivation of the loss with respect to $â†’z$ is then
$$\begin{aligned}
\frac{âˆ‚L}{âˆ‚z_i} = \frac{âˆ‚}{âˆ‚z_i} \left[-\log \frac{e^{z_\textit{gold}}}{âˆ‘_j e^{z_j}}\right] 
                 =& -\frac{âˆ‚z_\textit{gold}}{âˆ‚z_i} + \frac{âˆ‚\log(âˆ‘_j e^{z_j})}{âˆ‚z_i} \\
                 =& -[\textit{gold} = i] + \frac{1}{âˆ‘_j e^{z_j}} e^{z_i} \\
                 =& -[\textit{gold} = i] + o_i.
\end{aligned}$$

~~~
Therefore, $\frac{âˆ‚L}{âˆ‚â†’z} = â†’o - â†’1_\textit{gold}$, where $â†’1_\textit{gold}$
is 1 at index $\textit{gold}$ and 0 otherwise.

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_sparse.svgz)

---
# Derivative of Softmax and Sigmoid MLE Losses

In the previous case, the gold distribution was _sparse_, with only one
target probability being 1.

In the case of general gold distribution $â†’g$, we have
$$L(\softmax(â†’z), â†’g) = -âˆ‘_i g_i \log o_i.$$
Repeating the previous procedure for each target probability, we obtain
$$\frac{âˆ‚L}{âˆ‚â†’z} = â†’o - â†’g.$$

~~~
## Sigmoid

Analogously, for $o = Ïƒ(z)$ we get $\frac{âˆ‚L}{âˆ‚z} = o - g$,
where $g$ is the target gold probability.

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_full.svgz)

---
section: Regularization
# Regularization

As already mentioned, regularization is any change in the machine learning
algorithm that is designed to reduce generalization error but not necessarily
its training error.

Regularization is usually needed only if training error and generalization error
are different. That is often not the case if we process each training example
only once. Generally the more training data, the better generalization
performance.

- Early stopping

- L2, L1 regularization

- Dataset augmentation

- Ensembling

- Dropout

- Label smoothing

---
# Regularization â€“ Early Stopping

![w=100%](early_stopping.svgz)

---
# L2 Regularization

We prefer models with parameters small under L2 metric.

The L2 regularization, also called _weight decay_, _Tikhonov regularization_ or
_ridge regression_ therefore minimizes
$$JÌƒ(â†’Î¸; ð•) = J(â†’Î¸; ð•) + Î» \|â†’Î¸\|_2^2$$
for a suitable (usually very small) $Î»$.

During the parameter update of SGD, we get
$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚J}{âˆ‚Î¸_i} - 2Î±Î»Î¸_i,\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸ - Î±\frac{âˆ‚L}{âˆ‚â†’Î¸} - 2Î±Î»â†’Î¸.$$

~~~
This can be also written as
$$Î¸_i â† Î¸_i (1 - 2Î±Î») - Î±\frac{âˆ‚J}{âˆ‚Î¸_i},\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸(1 - 2Î±Î») - Î±\frac{âˆ‚L}{âˆ‚â†’Î¸}.$$

---
# L2 Regularization

![w=50%,h=center](l2_regularization.svgz)

---
# L2 Regularization as MAP

Another way to arrive at L2 regularization is to utilize Bayesian inference.

~~~
With MLE we have
$$â†’Î¸_\mathrm{MLE} = \argmax_{â†’Î¸} p(ð•; â†’Î¸).$$

~~~
Instead, we may want to maximize **maximum a posteriori (MAP)** point estimate:
$$â†’Î¸_\mathrm{MAP} = \argmax_{â†’Î¸} p(â†’Î¸; ð•)$$

~~~
Using Bayes' theorem
$$p(â†’Î¸; ð•) = p(ð•; â†’Î¸) p(â†’Î¸) / p(ð•),$$
we get
$$â†’Î¸_\mathrm{MAP} = \argmax_{â†’Î¸} p(ð•; â†’Î¸)p(â†’Î¸).$$

---
# L2 Regularization as MAP

The $p(â†’Î¸)$ are prior probabilities of the parameter values (our _preference_).

A common choice of the preference is the _small weights preference_, where the mean is assumed to
be zero, and the variance is assumed to be $Ïƒ^2$. Given that we have no further
information, we employ the maximum entropy principle, which results in
$p(Î¸_i) = ð“(Î¸_i; 0, Ïƒ^2)$, so that $p(â†’Î¸) = âˆ_i ð“(Î¸_i; 0, Ïƒ^2) = ð“(â†’Î¸; â‡‰0, Ïƒ^2
â‡‰I).$
~~~
Then
$$\begin{aligned}
â†’Î¸_\mathrm{MAP} &= \argmax\nolimits_{â†’Î¸} p(ð•; â†’Î¸)p(â†’Î¸) \\
                &= \argmax\nolimits_{â†’Î¸} âˆ\nolimits_{i=1}^m p(â†’x^{(i)}; â†’Î¸)p(â†’Î¸) \\
                &= \argmin\nolimits_{â†’Î¸} âˆ‘\nolimits_{i=1}^m -\log p(â†’x^{(i)}; â†’Î¸) - \log p(â†’Î¸).
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$â†’Î¸_\mathrm{MAP} = \argmin_{â†’Î¸} âˆ‘_{i=1}^m -\log p(â†’x^{(i)}; â†’Î¸) {\color{gray} - \frac{c}{2} \log(2Ï€Ïƒ^2)} + \frac{\|â†’Î¸\|_2^2}{2Ïƒ^2}.$$

---
# L1 Regularization

Similar to L2 regularization, but we prefer low L1 metric of parameters. We
therefore minimize
$$JÌƒ(â†’Î¸; ð•) = J(â†’Î¸; ð•) + Î» \|â†’Î¸\|_1$$

The corresponding SGD update is then
$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚J}{âˆ‚â†’Î¸_i} - \operatorname{sign}(Î¸_i)Î±Î».$$

---
# Regularization â€“ Dataset Augmentation

For some data, it is cheap to generate slightly modified examples.

- Image processing: translations, horizontal flips, scaling, rotations, color
  adjustments, â€¦

~~~
  - Mixup (appeared in 2017)

  ![w=25%,h=center](mixup.svgz)

~~~
- Speech recognition: noise, frequency change, â€¦

~~~
- More difficult for discrete domains like text.

---
# Regularization â€“ Ensembling

**Ensembling** (also called **model averaging** or in some contexts _bagging_) is a general technique
for reducing generalization error by combining several models. The models are
usually combined by averaging their outputs (either distributions or output
values in case of a regression).

~~~
The main idea behind ensembling it that if models have uncorrelated
(independent) errors, then by averaging model outputs the errors will cancel
out. If we denote the prediction of a model $y_i$ on a training example $(â†’x, y)$ as
$y_i(â†’x) = y + Îµ_i(â†’x)$, so that $Îµ_i(â†’x)$ is the model error on example $â†’x$,
the mean square error of the model is $ð”¼\big[(y_i(â†’x) - t)^2\big] = ð”¼\big[Îµ_i^2(â†’x)\big].$

~~~
Because for uncorrelated identically distributed random values $â‡x_i$ we have
$$\Var\left(âˆ‘ â‡x_i\right) = âˆ‘ \Var(â‡x_i), \Var(a â‹… â‡x) = a^2 \Var(â‡x),$$
~~~
we get that $\Var\left(\frac{1}{n}âˆ‘ â‡Îµ_i\right) = \frac{1}{n} â‹… âˆ‘ \frac{1}{n} \Var(â‡Îµ_i),$ so
the errors should decrease with the increasing number of models.

~~~
However, ensembling usually has high performance requirements.

---
# Regularization â€“ Ensembling

There are many possibilities how to train the models to average:

- Generate different datasets by sampling with replacement (bagging).

![w=50%,h=center](ensembling.svgz)

~~~
- Use random different initialization.

~~~
- Average models from last hours/days of training.

---
section: Dropout
# Regularization â€“ Dropout

How to design good universal features?

- In reproduction, evolution is achieved using gene swapping. The genes must not
  be just good with combination with other genes, they need to be universally
  good.

~~~
Idea of **dropout** by (Srivastava et al., 2014), in preprint since 2012.

~~~
When applying dropout to a layer, we drop each neuron independently with
a probability of $p$ (usually called **dropout rate**). To the rest of the
network, the dropped neurons have value of zero.

~~~
![w=60%,h=center](network_with_dropout.png)

---
# Regularization â€“ Dropout

Dropout is performed only when training, during inference no nodes are
dropped. However, in that case we need to _scale the activations down_
by a factor of $1-p$ to account for more neurons than usual.

![w=75%,h=center,mh=80%,v=middle](dropout_inference.svgz)

---
# Regularization â€“ Dropout

Alternatively, we might _scale the activations up_ during training by a factor
of $1/(1-p)$.

![w=75%,h=center,mh=90%,v=middle](dropout_inference_2.svgz)

---
# Regularization â€“ Dropout as Ensembling

![w=50%,h=center](dropout_ensembling.svgz)

---
# Regularization â€“ Dropout Implementation

```python
def dropout(inputs, rate=0.5, training=False):
    def do_inference():
        return inputs

    def do_train():
        random_noise = tf.random.uniform(tf.shape(inputs))
        mask = tf.cast(random_noise >= rate, tf.float32)
        return inputs * mask / (1 - rate)

    if training:
        return do_train()
    else:
        return do_inference()
```

---
# Regularization â€“ Dropout Effect
![w=85%,h=center](dropout_features.svgz)

---
section: LabelSmoothing
# Regularization â€“ Label Smoothing

Problem with softmax MLE loss is that it is _never satisfied_, always pushing
the gold label probability higher (but it saturates near 1).

~~~
This behaviour can be responsible for overfitting, because the network is always
commanded to respond more strongly to the training examples, not respecting
similarity of different training examples.

~~~
Ideally, we would like a full (non-sparse) categorical distribution of classes
for training examples, but that is usually not available.

~~~
We can at least use a simple smoothing technique, called _label smoothing_, which
allocates some small probability volume $Î±$ uniformly for all possible classes.

~~~
The target distribution is then
$$(1-Î±)â†’1_\textit{gold} + Î± \frac{â†’1}{\textrm{number~of~classes}}.$$

---
# Regularization â€“ Label Smoothing

![w=100%,v=middle](label_smoothing.svgz)

---
# Regularization â€“ Good Defaults

When you need to regularize (your model is overfitting), then a good default strategy is to:

~~~
- use data augmentation if possible;

~~~
- use dropout on all hidden dense layers (not on the output layer), good default
  dropout rate is 0.5 (or use 0.3 if the model is underfitting);
~~~
- use L2 regularization for your convolutional networks;
~~~
- use label smoothing (start with 0.1);
~~~
- if you require best performance and have a lot of resources, also
  perform ensembling.

---
section: Convergence
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

~~~
A major issue of convergence of deep networks is to make sure that the gradient
with respect to all parameters is reasonable at all times, i.e., it does not
decrease or increase too much with depth or in different batches.

~~~
There are _many_ factors influencing the gradient, convergence and its speed, we
now mention three of them:
- saturating non-linearities,
- parameter initialization strategies,
- gradient clipping.

---
class: middle
# Convergence â€“ Saturating Non-linearities

![w=55%,f=left](sigmoid.svgz)
![w=55%,f=left](tanh.svgz)
~~~

![w=44%](relu.svgz)

---
# Convergence â€“ Parameter Initialization

Neural networks usually need random initialization to _break symmetry_.

- Biases are usually initialized to a constant value, usually 0.

~~~

- Weights are usually initialized to small random values, either with uniform or
  normal distribution.
   - The scale matters for deep networks!

~~~
   - Originally, people used $U\left[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}\right]$ distribution.

~~~
   - Xavier Glorot and Yoshua Bengio, 2010:
     _Understanding the difficulty of training deep feedforward neural networks_.

     The authors theoretically and experimentally show that a suitable way to
     initialize a $â„^{nÃ—m}$ matrix is
     $$U\left[-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right].$$

---
# Convergence â€“ Parameter Initialization

![w=62%,h=center](glorot_activations.svgz)

---
# Convergence â€“ Parameter Initialization

![w=63%,h=center](glorot_gradients.svgz)

---
# Convergence â€“ Gradient Clipping

![w=100%](exploding_gradient.svgz)

---
# Convergence â€“ Gradient Clipping

![w=50%,h=center](gradient_clipping.svgz)

Using a given maximum norm, we may _clip_ the gradient.

~~~
$$â†’g â† \begin{cases}
  â†’g & \textrm{ if }\|â†’g\| â‰¤ c, \\
  c \frac{â†’g}{\|â†’g\|} & \textrm{ if }\|â†’g\| > c.
\end{cases}$$

~~~
Clipping can be performed per weight (parameter `clipvalue` of
`tf.optimizers.Optimizer`), per variable (`clipnorm`) or for the gradient as
a whole (`global_clipnorm`).
