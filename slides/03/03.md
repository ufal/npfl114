title: NPFL114, Lecture 3
class: title, langtech, cc-by-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Training Neural Networks II

## Milan Straka

### February 27, 2023

---
section: NNTraining
# Putting It All Together

Let us have a dataset with training, validation, and test sets, each containing
examples $(â†’x, y)$. Depending on $y$, consider one of the following output
activation functions:
$$\begin{cases}
  \textrm{none} & \textrm{ if } y âˆˆ â„ \textrm{ and we assume variance is constant everywhere},\\
  Ïƒ & \textrm{ if } y \textrm{ is a probability of an outcome},\\
  \softmax & \textrm{ if } y \textrm{ is a gold class index out of $K$ classes (or a full distribution)}.
\end{cases}$$

If $â†’x âˆˆ â„^D$, we can use a neural network with an input layer of size $D$, some
number of hidden layers with nonlinear activations, and an output layer of size
$O$ (either 1 or the number of classes $K$) with the mentioned output function.

~~~
_BTW, there are of course many functions, which could be used as output
activations instead of $Ïƒ$ and $\softmax$; however, $Ïƒ$ and $\softmax$ are
almost universally used. One of the reason is that they can be derived
using the maximum-entropy principle from a set of conditions, see the
[Machine Learning for Greenhorns (NPFL129) lecture 5 slides](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2223/slides/?05).
Additionally, they are the inverses of [canonical link functions](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function)
of the Bernoulli and categorical distributions, respectively._

---
# Putting It All Together â€“ Single-Hidden-Layer MLP
![w=90%,mw=52%,f=left](../01/neural_network.svgz)

~~~
We have
$$h_i = f^{(1)}\left(âˆ‘_j x_j W^{(1)}_{j,i} + b^{(1)}_i\right)$$
where
- $â‡‰W^{(1)} âˆˆ â„^{DÃ—H}$ is a matrix of **weights**,
- $â†’b^{(1)} âˆˆ â„^H$ is a vector of **biases**,
- $f^{(1)}$ is an activation function.

~~~
The weight matrix is also called a **kernel**.

~~~
The biases define general behaviour in case of zero/very small input.

~~~
Transformations of type $â†’x^T â‡‰W^{(1)} + â†’b$ are called **affine** instead of _linear_.

--- -----
Similarly
$$o_i = f^{(2)}\left(âˆ‘_j h_j W^{(2)}_{j,i} + b^{(2)}_i\right)$$
with
- $â‡‰W^{(2)} âˆˆ â„^{HÃ—O}$ another matrix of weights,
- $â†’b^{(2)} âˆˆ â„^O$ another vector of biases,
- $f^{(2)}$ being an output activation function.

---
# Putting It All Together â€“ Parameters and Training

Altogether, the $â‡‰W^{(1)}, â‡‰W^{(2)}, â†’b^{(1)}$, and $â†’b^{(2)}$ form the
**parameters** of the model, which we denote as a vector $â†’Î¸$ in the model
description and machine learning algorithms.

~~~
In our case, the parameters have a total size of $DÃ—H + HÃ—O + H + O$.

~~~
To train the network, we repeatedly sample $m$ training examples and perform
an SGD (or any of its adaptive variants), updating the parameters to minimize the
loss derived by MSE $E(â†’Î¸) = ğ”¼_{(â‡â†’x, â‡y)âˆ¼pÌ‚_\textrm{data}} L(f(â†’x; â†’Î¸), y)$:

$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚E(â†’Î¸)}{âˆ‚Î¸_i},\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸ - Î±âˆ‡_{â†’Î¸} E(â†’Î¸).$$

~~~
We set the hyperparameters (size of the hidden layer, hidden layer activation
function, learning rate, â€¦) using performance on the validation set and evaluate
generalization error on the test set.

---
# Putting It All Together â€“ Batches

- We always process data in **batches**, i.e., matrices whose rows are the batch examples.

~~~
- We represent the network in a vectorized way (tensorized would be more accurate).

~~~
  Instead of $H_{b,i} = f^{(1)}\left(âˆ‘_j X_{b,j} W^{(1)}_{j,i} + b^{(1)}_i\right)$,
  we compute
  $$\begin{aligned}
    â‡‰H &= f^{(1)}\left(â‡‰X â‡‰W^{(1)} + â†’b^{(1)}\right), \\
    â‡‰O &= f^{(2)}\left(â‡‰H â‡‰W^{(2)} + â†’b^{(2)}\right) = f^{(2)}\left(f^{(1)}\left(â‡‰X â‡‰W^{(1)} + â†’b^{(1)}\right) â‡‰W^{(2)} + â†’b^{(2)}\right). \\
  \end{aligned}$$

~~~
  The derivatives
  $$\frac{âˆ‚f^{(1)}\left(â‡‰X â‡‰W^{(1)} + â†’b^{(1)}\right)}{âˆ‚â‡‰X},
  \frac{âˆ‚f^{(1)}\left(â‡‰X â‡‰W^{(1)} + â†’b^{(1)}\right)}{âˆ‚â‡‰W^{(1)}}, â€¦$$
  are then batches of matrices (called **Jacobians**) or even higher-dimensional tensors.

---
class: middle
# Putting It All Together â€“ Computation Graph

![w=75%,mw=48%,h=center](../01/neural_network.svgz)
â†’
![w=75%,mw=35%,h=center](computation_graph.svgz)

---
# Putting It All Together â€“ Designing and Training Neural Networks

Designing and training a neural network is not a one-shot action,
but instead an iterative procedure.

~~~
- When choosing hyperparameters, it is important to verify that the model
  does not underfit and does not overfit.

~~~
- Underfitting can be checked by trying increasing model capacity or training longer,
  and observing whether the training performance increases.

~~~
- Overfitting can be tested by observing train/dev difference, or by trying
  stronger regularization and observing whether the development performance
  improves.

~~~
Regarding hyperparameters:
- We need to set the number of training epochs so that development performance
  stops increasing during training (usually later than when the training
  performance plateaus).

~~~
- Generally, we want to use large enough batch size, but such a one which does
  not slow us down too much (GPUs sometimes allow larger batches without slowing
  down training). However, because larger batch size implies less noise in the
  gradient, small batch size sometimes work as regularization (especially for
  vanilla SGD algorithm).

---
class: tablefull
# High Level Overview

|                 | Classical ('90s) | Deep Learning |
|-----------------|------------------|---------------|
|Architecture     | $\vdots\,\,\vdots\,\,\vdots$ | $\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots\,\,\vdots$ &nbsp; CNN, RNN, Transformer, VAE, GAN, â€¦
|ActivationÂ func. | $\tanh, Ïƒ$    | $\tanh$, ReLU, LReLU, GELU, Swish (SiLU), SwiGLU, â€¦
|OutputÂ function  | none, $Ïƒ$     | none, $Ïƒ$, $\softmax$
|Loss function    | MSE           | NLL (or cross-entropy or KL-divergence)
|Optimization     | SGD, momentum | SGD (+ momentum), RMSProp, Adam, SGDW, AdamW, â€¦
|Regularization   | $L^2$, $L^1$  | $L^2$, Dropout, Label smoothing, BatchNorm, LayerNorm, MixUp, WeightStandardization, â€¦

---
section: Metrics&Losses
# Metrics and Losses

During training and evaluation, we use two kinds of error functions:
~~~
- **loss** is a _differentiable_ function used during training,
~~~
  - NLL, MSE, Huber loss, Hinge, â€¦
~~~
- **metric** is any (and very often non-differentiable) function used during
  evaluation,
~~~
  - any loss, accuracy, F-score, BLEU, â€¦
~~~
  - possibly even human evaluation.

~~~
In TensorFlow, the losses and metrics are available in `tf.losses` and
`tf.metrics` (aliases for `tf.keras.losses` and `tf.keras.metrics`).

---
# TF Losses

The `tf.losses` offer two sets of APIs. The newer API ones are loss classes like
```python
tf.losses.MeanSquaredError(
    reduction=tf.losses.Reduction.AUTO, name='mean_squared_error'
)
```

~~~
The created objects are subclasses of `tf.losses.Loss` and can be always called
with three arguments:
```python
__call__(y_true, y_pred, sample_weight=None)
```
which returns the loss of the given data, _reduced_ using the specified
reduction. If `sample_weight` is given, it is used to weight (multiply) the
individual batch example losses before reduction.

~~~
- `tf.losses.Reduction.SUM_OVER_BATCH_SIZE`, which is the default of `.AUTO`;
~~~
- `tf.losses.Reduction.SUM`;
~~~
- `tf.losses.Reduction.NONE`.

---
# TF Cross-entropy Losses

The cross-entropy losses need to specify also the distribution in question:
~~~
- `tf.losses.BinaryCrossentropy`: the gold and predicted distributions are
  Bernoulli distributions (i.e., a single probability);
~~~
- `tf.losses.CategoricalCrossentropy`: the gold and predicted distributions are
  categorical distributions;
~~~
- `tf.losses.SparseCategoricalCrossentropy`: a special case, where the gold
  distribution is one-hot distribution (i.e., a single correct class), which
  is represented as the gold _class index_; therefore, it has one less dimension
  than the predicted distribution.

~~~
These losses expect probabilities on input, but offer `from_logits` argument,
which can be used to indicate that logits are used instead of probabilities.

~~~
## Old losses API

In addition to the loss objects, `tf.losses` offers methods like
`tf.losses.mean_squared_error`, which process two arguments `y_true` and
`y_pred` and do not reduce the batch example losses.

---
# TF Metrics

There are two important differences between metrics and losses.
1. metrics may be non-differentiable;
~~~
1. metrics **aggregate** results over multiple batches.

~~~
The metric objects are subclasses of `tf.metrics.Metric` and offer the following
methods:
- `update_state(y_true, y_pred, sample_weight=None)` updates the value of the
  metric and stores it;
~~~
- `result()` returns the current value of the metric;
~~~
- `reset_states()` clears the stored state of the metric.

~~~
The most common pattern is using the provided method
```python
__call__(y_true, y_pred, sample_weight=None)
```
which is a combination of `update_state` followed by a `result()`.

---
# TF Metrics

Apart from analogues of the losses
- `tf.metrics.MeanSquaredError`
- `tf.metrics.BinaryCrossentropy`
- `tf.metrics.CategoricalCrossentropy`
- `tf.metrics.SparseCategoricalCrossentropy`

the `tf.metrics` module provides
~~~
- `tf.metrics.Mean` computing averaged mean;
~~~
- `tf.metrics.Accuracy` returning accuracy, which is an average number of examples
  where the prediction is equal to the gold value;
~~~
- `tf.metrics.BinaryAccuracy` returning accuracy of predicting a Bernoulli
  distribution (the gold value is 0/1, the prediction is a probability);
~~~
- `tf.metrics.CategoricalAccuracy` returning accuracy of predicting a Categorical
  distribution (the argmaxes of gold and predicted distributions are equal);
~~~
- `tf.metrics.SparseCategoricalAccuracy` is again a special case of
  `CategoricalAccuracy`, where the gold distribution is represented as the gold
  class _index_.

---
section: âˆ‚Loss
# Derivative of MSE Loss

Given the MSE loss of
$$L = \big(f(â†’x; â†’Î¸) - y\big)^2,$$
the derivative with respect to the model output is simply:
$$\frac{âˆ‚L}{âˆ‚f(â†’x; â†’Î¸)} = 2\big(f(â†’x; â†’Î¸) - y\big).$$

---
# Derivative of Softmax MLE Loss

![w=38%,h=center](softmax.svg)

Let us have a softmax output layer with
$$o_i = \frac{e^{z_i}}{âˆ‘_j e^{z_j}}.$$

---
# Derivative of Softmax MLE Loss

Consider now the MLE estimation. The loss for gold class index $\textit{gold}$ is then
$$L(\softmax(â†’z), \textit{gold}) = - \log o_\textit{gold}.$$

~~~
The derivation of the loss with respect to $â†’z$ is then
$$\begin{aligned}
\frac{âˆ‚L}{âˆ‚z_i} = \frac{âˆ‚}{âˆ‚z_i} \left[-\log \frac{e^{z_\textit{gold}}}{âˆ‘_j e^{z_j}}\right] 
                 =& -\frac{âˆ‚z_\textit{gold}}{âˆ‚z_i} + \frac{âˆ‚\log(âˆ‘_j e^{z_j})}{âˆ‚z_i} \\
                 =& -[\textit{gold} = i] + \frac{1}{âˆ‘_j e^{z_j}} e^{z_i} \\
                 =& -[\textit{gold} = i] + o_i.
\end{aligned}$$

~~~
Therefore, $\frac{âˆ‚L}{âˆ‚â†’z} = â†’o - â†’1_\textit{gold}$, where $â†’1_\textit{gold}$
is the one-hot encoding (a vector with 1 at the index $\textit{gold}$ and
0 everywhere else).

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_sparse.svgz)

---
# Derivative of Softmax and Sigmoid MLE Losses

In the previous case, the gold distribution was _sparse_, with only one
target probability being 1.

In the case of general gold distribution $â†’g$, we have
$$L(\softmax(â†’z), â†’g) = -âˆ‘\nolimits_i g_i \log o_i.$$
Repeating the previous procedure for each target probability, we obtain
$$\frac{âˆ‚L}{âˆ‚â†’z} = â†’o - â†’g.$$

~~~
## Sigmoid

Analogously, for $o = Ïƒ(z)$ we get $\frac{âˆ‚L}{âˆ‚z} = o - g$,
where $g$ is the target gold probability.

~~~
The result follows automatically from the fact that $Ïƒ$ can be computed
using $\softmax$ as
$$\softmax\big([0~~x]\big)_1 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}} = Ïƒ(x).$$

---
# Derivative of Softmax MLE Loss

![w=100%,v=middle](softmax_loss_full.svgz)

---
section: Regularization
# Regularization

As already mentioned, regularization is any change in the machine learning
algorithm that is designed to reduce generalization error but not necessarily
its training error.

Regularization is usually needed only if training error and generalization error
are different. That is often not the case if we process each training example
only once. Generally the more training data, the better generalization
performance without any explicit regularization.

- Early stopping

- $L^2$, $L^1$ regularization

- Dataset augmentation

- Ensembling

- Dropout

- Label smoothing

---
# Regularization â€“ Early Stopping

![w=100%](early_stopping.svgz)

---
# L2 Regularization

$L^2$-regularization is one of the oldest regularization techniques, which tries
to prefer â€œsimplerâ€ models by endorsing models with **smaller weights**.

~~~
Concretely, **$\boldsymbol{L^2}$-regularization** (also called **Tikhonov
regularization** or **weight decay**) penalizes models with large weights by
utilizing the following error function:

$$EÌƒ(â†’Î¸; ğ•) = E(â†’Î¸; ğ•) + \frac{Î»}{2} \|â†’Î¸\|_2^2$$
for a suitable (usually very small) $Î»$.

~~~
Note that the $L^2$-regularization is usually not applied to the _bias_, only to the
â€œproperâ€ weights, because we cannot really overfit via the bias.

---
# L2 Regularization

![w=25%,f=right](l2_smoothness_data.png)

One way to look at $L^2$-regularization is that it promotes smaller
changes of the model (the Jacobian of a single layer with respect to the
inputs depends on the weight matrix, because $\frac{âˆ‚â†’x^Tâ‡‰W}{âˆ‚â†’x} = â‡‰W$).

~~~
Considering the data points on the right, we present mean squared errors
and $L^2$ norms of the weights for three linear regression models:

![w=70%,h=center](l2_smoothness.png)
![w=70%,h=center](l2_smoothness_equations.png)

---
# L2 Regularization as MAP

Another way to arrive at $L^2$ regularization is to utilize Bayesian inference.

~~~
With MLE we have
$$â†’Î¸_\mathrm{MLE} = \argmax\nolimits_{â†’Î¸} p(ğ•; â†’Î¸).$$

~~~
Instead, we may want to maximize **maximum a posteriori (MAP)** point estimate:
$$â†’Î¸_\mathrm{MAP} = \argmax\nolimits_{â†’Î¸} p(â†’Î¸ | ğ•).$$

~~~
Using Bayes' theorem stating that
$$p(â†’Î¸ | ğ•) = \frac{p(ğ• | â†’Î¸) p(â†’Î¸)}{p(ğ•)},$$
we can rewrite the MAP estimate to
$$â†’Î¸_\mathrm{MAP} = \argmax\nolimits_{â†’Î¸} p(ğ• | â†’Î¸)p(â†’Î¸).$$

---
# L2 Regularization as MAP

The $p(â†’Î¸)$ are prior probabilities of the parameter values (our _preference_).

A common choice of the preference is the _small weights preference_, where the mean is assumed to
be zero, and the variance is assumed to be $Ïƒ^2$. Given that we have no further
information, we employ the maximum entropy principle, which results in
$p(Î¸_i) = ğ“(Î¸_i; 0, Ïƒ^2)$, so that $p(â†’Î¸) = âˆ_i ğ“(Î¸_i; 0, Ïƒ^2) = ğ“(â†’Î¸; â‡‰0, Ïƒ^2
â‡‰I).$
~~~
Then
$$\begin{aligned}
â†’Î¸_\mathrm{MAP} &= \argmax\nolimits_{â†’Î¸} p(ğ•; â†’Î¸)p(â†’Î¸) \\
                &= \argmax\nolimits_{â†’Î¸} âˆ\nolimits_{i=1}^m p(â†’x^{(i)}; â†’Î¸)p(â†’Î¸) \\
                &= \argmin\nolimits_{â†’Î¸} âˆ‘\nolimits_{i=1}^m \Big(-\log p(â†’x^{(i)}; â†’Î¸) - \log p(â†’Î¸)\Big).
\end{aligned}$$

~~~
By substituting the probability of the Gaussian prior, we get
$$â†’Î¸_\mathrm{MAP} = \argmin_{â†’Î¸} âˆ‘_{i=1}^m \Big(-\log p(â†’x^{(i)}; â†’Î¸) {\color{gray} + \frac{|â†’Î¸|}{2} \log(2Ï€Ïƒ^2)} + \frac{\|â†’Î¸\|_2^2}{2Ïƒ^2}\Big).$$

---
# L2 Regularization

The resulting parameter update during SGD with $L^2$-regularization is
$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚E}{âˆ‚Î¸_i} - Î±Î»Î¸_i,\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸ - Î±âˆ‡_{â†’Î¸}E(â†’Î¸) - Î±Î»â†’Î¸.$$

~~~
This update can be rewritten to
$$Î¸_i â† Î¸_i (1 - Î±Î») - Î±\frac{âˆ‚E}{âˆ‚Î¸_i},\textrm{~~or in vector notation,~~}â†’Î¸ â† â†’Î¸(1 - Î±Î») - Î±âˆ‡_{â†’Î¸}E(â†’Î¸).$$

~~~
Termilogically, the update of weights in these two formulas is called _weight
decay_, because the weights are multiplied by a factor $1 - Î±Î» < 1$, while
adding the $L^2$-norm of the parameters to the loss is called _$L^2$-regularization_.

For SGD, they are equivalent â€“ but once you add momentum or normalization by the
estimated second moment (RMSProp, Adam), weight decay and $L^2$-regularization
are different.

---
# L2 Regularization â€“ AdamW

It has taken more than three years to realize that using Adam with
$L^2$-regularization does not work well. At the end of 2017, **AdamW**
was proposed, which is Adam with weight decay.

## <span style="color: #d4d">Adam with $L^2$-regularization</span>, <span style="color: #2c2">AdamW</span>

<div class="algorithm">

- $â†’s â† â†’0$, $â†’r â† â†’0$, $t â† 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(â†’x^{(i)}, y^{(i)})$
    - $â†’g â† \frac{1}{m} âˆ‘_i âˆ‡_{â†’Î¸} \big(L(f(â†’x^{(i)}; â†’Î¸), y^{(i)}) \textcolor{#d4d}{+ \frac{Î»}{2}\|â†’Î¸\|^2} \big)$
    - $t â† t + 1$
    - $â†’s â† Î²_1â†’s + (1-Î²_1)â†’g$
    - $â†’r â† Î²_2â†’r + (1-Î²_2)â†’g^2$
    - $â†’sÌ‚ â† â†’s / (1 - Î²_1^t)$, $â†’rÌ‚ â† â†’r / (1 - Î²_2^t)$
    - $â†’Î¸ â† â†’Î¸ - \frac{Î±_t}{\sqrt{â†’rÌ‚} + Îµ}â†’sÌ‚ \textcolor{#2c2}{-Î±_t Î» â†’Î¸}$
</div>

---
# L2 Regularization â€“ AdamW

$$â†’Î¸ â† â†’Î¸ - \frac{Î±_t}{\sqrt{â†’rÌ‚} + Îµ}â†’sÌ‚ \textcolor{#2c2}{-Î±_t Î» â†’Î¸}$$

In some variants of the algorithm (notably in the original AdamW paper), the
authors proposed not to use the learning rate in the weight decay (to decouple
the influence of the learning rate on the weight decay).

~~~
However, this would mean that if you utilize learning rate decay, you would need
to apply it manually also on the weight decay. So currently, both the
implementation of `tf.optimizers.experimental.AdamW` (it will move to
`tf.optimizers.AdamW` in TF 2.12) and `torch.optim.AdamW` multiplies the
(possibly decaying) learning rate and the (constant) weight decay in the update.

---
# L1 Regularization

Similar to $L^2$-regularization, but could prefer low $L^1$ metric of parameters. We
could therefore minimize
$$EÌƒ(â†’Î¸; ğ•) = E(â†’Î¸; ğ•) + Î» \|â†’Î¸\|_1.$$

The corresponding SGD update is then
$$Î¸_i â† Î¸_i - Î±\frac{âˆ‚EJ}{âˆ‚Î¸_i} - \min\big(Î±Î», |Î¸_i|\big)\operatorname{sign}(Î¸_i).$$

~~~
Empirically, $L^1$-regularization does not work well with deep neural networks
and is essentially never used, as far as I know.

---
# Regularization â€“ Dataset Augmentation

For some data, it is cheap to generate slightly modified examples.

- Image processing: translations, horizontal flips, scaling, rotations, color
  adjustments, â€¦

~~~
  - Mixup (appeared in 2017)

  ![w=25%,h=center](mixup.svgz)

~~~
- Speech recognition: noise, frequency change, â€¦

~~~
- More difficult for discrete domains like text.

---
# Regularization â€“ Ensembling

**Ensembling** (also called **model averaging** or in some contexts _bagging_) is a general technique
for reducing generalization error by combining several models. The models are
usually combined by averaging their outputs (either distributions or output
values in case of a regression).

~~~
The main idea behind ensembling it that if models have uncorrelated
(independent) errors, then by averaging model outputs the errors will cancel
out. If we denote the prediction of the $i^\textrm{th}$ model on a training example $(â†’x, y)$ as
$y_i(â†’x) = y + Îµ_i(â†’x)$, so that $Îµ_i(â†’x)$ is the model error on example $â†’x$,
the mean square error of the model is $ğ”¼\big[(y_i(â†’x) - y)^2\big] = ğ”¼\big[Îµ_i^2(â†’x)\big].$

~~~
Because for uncorrelated identically distributed random values $â‡x_i$ we have
$$\Var\left(âˆ‘ â‡x_i\right) = âˆ‘ \Var(â‡x_i),~~~~\Var(a â‹… â‡x) = a^2 \Var(â‡x),$$
~~~
we get that $\Var\big(\frac{1}{n}âˆ‘_i â‡Îµ_i\big) = \frac{1}{n} \big(âˆ‘_i \frac{1}{n} \Var(â‡Îµ_i)\big)$, so
the errors should decrease with the increasing number of models.

---
# Regularization â€“ Ensembling Visualization

Consider ensembling predictions generated uniformly on a planar disc:
![w=100%](ensemble_visualization-r1.svgz)
~~~
![w=100%](ensemble_visualization-r1b.svgz)
~~~
![w=100%](ensemble_visualization-r42.svgz)

---
# Regularization â€“ Ensembling

There are many possibilities how to train the models to ensemble:

- For neural network models, training models with independent random
  initialization is usually enough, given that the loss has many local minima,
  so the models tend to be quite independent just when using different random
  initialization.

~~~
![w=34%,f=right](ensembling.svgz)

- Algorithms with convex loss functions usually converge to the same optimum
  independent of randomization. In that case, we can use **bagging** (bootstrap
  aggregation), where we generate different training data for each model by
  sampling with replacement.

~~~
- Average models from last hours/days of training.

~~~
However, ensembling usually has high performance requirements.

---
section: Dropout
# Regularization â€“ Dropout

How to design good universal features?

- In reproduction, evolution is achieved using gene swapping. The genes must not
  be just good with combination with other genes, they need to be universally
  good.

~~~
Idea of **dropout** by (Srivastava et al., 2014), in preprint since 2012.

~~~
When applying dropout to a layer, we drop each neuron independently with
a probability of $p$ (usually called **dropout rate**). To the rest of the
network, the dropped neurons have value of zero.

~~~
![w=60%,h=center](network_with_dropout.png)

---
# Regularization â€“ Dropout

Dropout is performed only when training, during inference no nodes are
dropped. However, in that case we need to _scale the activations down_
by a factor of $1-p$ to account for more neurons than usual.

![w=75%,h=center,mh=80%,v=middle](dropout_inference.svgz)

---
# Regularization â€“ Dropout

Alternatively, we might _scale the activations up_ during training by a factor
of $1/(1-p)$.

![w=75%,h=center,mh=90%,v=middle](dropout_inference_2.svgz)

---
# Regularization â€“ Dropout as Ensembling

![w=50%,h=center](dropout_ensembling.svgz)

---
# Regularization â€“ Dropout Implementation

```python
def dropout(inputs, rate=0.5, training=False):
    def do_inference():
        return inputs

    def do_train():
        random_noise = tf.random.uniform(tf.shape(inputs))
        mask = tf.cast(random_noise >= rate, tf.float32)
        return inputs * mask / (1 - rate)

    if training:
        return do_train()
    else:
        return do_inference()
```

---
# Regularization â€“ Dropout Effect
![w=85%,h=center](dropout_features.svgz)

---
section: LabelSmoothing
# Regularization â€“ Label Smoothing

Problem with softmax MLE loss is that it is _never satisfied_, always pushing
the gold label probability higher (but it saturates near 1).

~~~
This behaviour can be responsible for overfitting, because the network is always
commanded to respond more strongly to the training examples, not respecting
similarity of different training examples.

~~~
Ideally, we would like a full (non-sparse) categorical distribution of classes
for training examples, but that is usually not available.

~~~
We can at least use a simple smoothing technique, called _label smoothing_, which
allocates some small probability volume $Î±$ uniformly for all possible classes.

~~~
The target distribution is then
$$(1-Î±)â†’1_\textit{gold} + Î± \frac{â†’1}{\textrm{number~of~classes}}.$$

---
# Regularization â€“ Label Smoothing

![w=100%,v=middle](label_smoothing.svgz)

---
# Regularization â€“ Good Defaults

When you need to regularize (your model is overfitting), then a good default strategy is to:

~~~
- use data augmentation if possible;

~~~
- use dropout on all hidden dense layers (not on the output layer), good default
  dropout rate is 0.5 (or use 0.3-0.1 if the model is underfitting);
~~~
- use weight decay (AdamW) for convolutional networks;
~~~
- use label smoothing (start with 0.1);
~~~
- if you require best performance and have a lot of resources, also
  perform ensembling.

---
section: Convergence
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

~~~
A major issue of convergence of deep networks is to make sure that the gradient
with respect to all parameters is reasonable at all times, i.e., it does not
decrease or increase too much with depth or in different batches.

~~~
There are _many_ factors influencing the gradient, convergence and its speed, we
now mention three of them:
- saturating nonlinearities,
- parameter initialization strategies,
- gradient clipping.

---
class: middle
# Convergence â€“ Saturating Non-linearities

![w=55%,f=left](sigmoid.svgz)
![w=55%,f=left](tanh.svgz)
~~~

![w=44%](relu.svgz)

---
# Convergence â€“ Parameter Initialization

Neural networks usually need random initialization to _break symmetry_.

- Biases are usually initialized to a constant value, usually 0.

~~~

- Weights are usually initialized to small random values, either with uniform or
  normal distribution.
   - The scale matters for deep networks!

~~~
   - Originally, people used $U\left[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}\right]$ distribution.

~~~
   - Xavier Glorot and Yoshua Bengio, 2010:
     _Understanding the difficulty of training deep feedforward neural networks_.

     The authors theoretically and experimentally show that a suitable way to
     initialize a $â„^{nÃ—m}$ matrix is
     $$U\left[-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right].$$

---
# Convergence â€“ Parameter Initialization

![w=62%,h=center](glorot_activations.svgz)

---
# Convergence â€“ Parameter Initialization

![w=63%,h=center](glorot_gradients.svgz)

---
# Convergence â€“ Gradient Clipping

![w=100%](exploding_gradient.svgz)

---
# Convergence â€“ Gradient Clipping

![w=50%,h=center](gradient_clipping.svgz)

Using a given maximum norm, we may _clip_ the gradient.

~~~
$$â†’g â† \begin{cases}
  â†’g & \textrm{ if }\|â†’g\| â‰¤ c, \\
  c \frac{â†’g}{\|â†’g\|} & \textrm{ if }\|â†’g\| > c.
\end{cases}$$

~~~
Clipping can be performed per weight (parameter `clipvalue` of
`tf.optimizers.Optimizer`), **per variable** (`clipnorm`) or for the gradient as
a whole (`global_clipnorm`).
