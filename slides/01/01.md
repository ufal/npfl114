title: NPFL114, Lecture 1
class: title, langtech, cc-by-nc-sa
# Introduction to Deep Learning

## Milan Straka

### February 24, 2020

---
# Deep Learning Highlights
- Image recognition

~~~ ~
# Deep Learning Highlights
![w=60%,h=center](imagenet_recognition.jpg)

~~~ ~~
- Object detection
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](object_detection.pdf)

~~~ ~~
- Image segmentation,
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](image_segmentation.pdf)

~~~ ~~
- Human pose estimation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](human_pose_estimation.pdf)

~~~ ~~
- Image labeling
~~~ ~
# Deep Learning Highlights
![w=75%,h=center](image_labeling.pdf)

~~~ ~~
- Visual question answering
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](vqa.pdf)

~~~ ~~
- Speech recognition and generation
~~~ ~
# Deep Learning Highlights
![w=100%](tacotron_comparison.pdf)

<audio controls="" style="width: 49%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/gan_or_vae.wav"></audio>
<audio controls="" style="width: 49%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/fox_question.wav"></audio>

~~~ ~~
- Lip reading
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](lrw_showcase.pdf)
![w=70%,h=center](lipnet_saliency.pdf)

~~~ ~~
- Machine translation
~~~ ~
# Deep Learning Highlights
![w=44%,h=center](attention_visualization.pdf)

~~~ ~~
- Machine translation without parallel data
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](umt_ideas.pdf)
![w=30%,h=center](umt_comparison.pdf)

~~~ ~~
- Chess, Go and Shogi
~~~ ~
# Deep Learning Highlights
![w=95%,h=center](a0_results.pdf)

~~~ ~~
- Multiplayer Capture the flag
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](ctf_overview.pdf)

~~~ ~~

---
section: Notation
# Notation

- $a$, $‚Üía$, $‚áâA$, $‚á∂A$: scalar (integer or real), vector, matrix, tensor

- $‚Åáa$, $‚Åá‚Üía$, $‚Åá‚áâA$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{‚àÇf}{‚àÇx}$: partial derivative of $f$ with respect to $x$

~~~
- $‚àá_‚Üíx f$: gradient of $f$ with respect to $‚Üíx$, i.e.,
  $\left(\frac{‚àÇf(‚Üíx)}{‚àÇx_1}, \frac{‚àÇf(‚Üíx)}{‚àÇx_2}, \ldots, \frac{‚àÇf(‚Üíx)}{‚àÇx_n}\right)$

---
section: Random Variables
# Random Variables
A random variable $‚Åáx$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $‚Åáx ‚àº P$ stands for a random variable $‚Åáx$ having a distribution $P$.

~~~
For discrete variables, the probability that $‚Åáx$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(‚Åáx = x)$.

~~~
For continuous variables, the probability that the value of $‚Åáx$ lies in the interval
$[a, b]$ is given by $‚à´_a^b p(x)\d x$.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$ùîº_{‚Åáx ‚àº P}[f(x)] ‚âù ‚àë_x P(x)f(x)$$

For continuous variables it is computed as:
$$ùîº_{‚Åáx ‚àº p}[f(x)] ‚âù ‚à´_x p(x)f(x)\d x$$

~~~
If the random variable is obvious from context, we can write only $ùîº_P[x]$
of even $ùîº[x]$.

~~~
Expectation is linear, i.e.,
$$ùîº_‚Åáx [Œ±f(x) + Œ≤g(x)] = Œ±ùîº_‚Åáx [f(x)] + Œ≤ùîº_‚Åáx [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $Œº = ùîº[x]$.

$$\begin{aligned}
  \Var(x) &‚âù ùîº\left[\big(x - ùîº[x]\big)^2\right]\textrm{, or more generally} \\
  \Var(f(x)) &‚âù ùîº\left[\big(f(x) - ùîº[f(x)]\big)^2\right]
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ùîº\left[x^2 - 2xùîº[x] + \big(ùîº[x]\big)^2\right] = ùîº\left[x^2\right] - \big(ùîº[x]\big)^2.$$

~~~
Variance is connected to $E[x^2]$, a _second moment_ of a random
variable ‚Äì it is in fact a _centered_ second moment.

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $œÜ ‚àà [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= œÜ^x (1-œÜ)^{1-x} \\
  ùîº[x] &= œÜ \\
  \Var(x) &= œÜ(1-œÜ)
\end{aligned}$$

~~~
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $k$ different
discrete outcomes. It is parametrized by $‚Üíp ‚àà [0, 1]^k$ such that $‚àë_{i=1}^{k} p_{i} = 1$.
$$\begin{aligned}
  P(‚Üíx) &= ‚àè\nolimits_i^k p_i^{x_i} \\
  ùîº[x_i] &= p_i, \Var(x_i) = p_i(1-p_i) \\
\end{aligned}$$

---
section: Information Theory
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have _additive_ information.

~~~
$$I(x) ‚âù -\log P(x) = \log \frac{1}{P(x)}$$

~~~
## Entropy

Amount of _surprise_ in the whole distribution.
$$H(P) ‚âù ùîº_{‚Åáx‚àºP}[I(x)] = -ùîº_{‚Åáx‚àºP}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -‚àë_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -‚à´ P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ‚âù -ùîº_{‚Åáx‚àºP}[\log Q(x)]$$

~~~
- Gibbs inequality
    - $H(P, Q) ‚â• H(P)$
    - $H(P) = H(P, Q) ‚áî P = Q$
~~~
    - Proof: Using Jensen's inequality, we get
      $$‚àë_x P(x) \log \frac{Q(x)}{P(x)} ‚â§ \log ‚àë_x P(x) \frac{Q(x)}{P(x)} = \log ‚àë_x Q(x) = 0.$$
~~~
    - Corollary: For a categorical distribution with $n$ outcomes, $H(P) ‚â§ \log n$,
    because for $Q(x) = 1/n$ we get $H(P) ‚â§ H(P, Q) = -‚àë_x P(x) \log Q(x) = \log n.$
~~~
- generally $H(P, Q) ‚â† H(Q, P)$

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D_\textrm{KL}(P || Q) ‚âù H(P, Q) - H(P) = ùîº_{‚Åáx‚àºP}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P || Q) ‚â• 0$
- generally $D_\textrm{KL}(P || Q) ‚â† D_\textrm{KL}(Q || P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Œº$ and variance $œÉ^2$:
$$ùìù(x; Œº, œÉ^2) = \sqrt{\frac{1}{2œÄœÉ^2}} \exp \left(-\frac{(x - Œº)^2}{2œÉ^2}\right)$$

For standard values $Œº=0$ and $œÉ^2=1$ we get $ùìù(x; 0, 1) = \sqrt{\frac{1}{2œÄ}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions with a given mean and variance, it can be proven
(using variational inference) that such a distribution with _maximal entropy_
is exactly the normal distribution.

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $x‚àà‚Ñù$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, ‚Ä¶
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, ‚Ä¶)
    - _reinforcement learning_, _semi-supervised learning_, ‚Ä¶
- Measure P
    - _accuracy_, _error rate_, _F-score_, ‚Ä¶

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [MNIST](http://yann.lecun.com/exdb/mnist/) | Images (28x28, grayscale) of handwritten digits. | 60k |
| [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 10 classes of objects. | 50k |
| [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 100 classes of objects (with 20 defined superclasses). | 50k |
| [ImageNet](http://image-net.org/) | Labeled object image database (labeled objects, some with bounding boxes). | 14.2M |
| [ImageNet-ILSVRC](http://image-net.org/challenges/LSVRC/) | Subset of ImageNet for Large Scale Visual Recognition Challenge, annotated with 1000 object classes and their bounding boxes. | 1.2M |
| [COCO](http://cocodataset.org/) | _Common Objects in Context_: Complex everyday scenes with descriptions (5) and highlighting of objects (91 types). | 2.5M |

---
class: middle
# Well-known Datasets
## ImageNet-ILSVRC
![w=70%,mw=50%,h=center](imagenet_recognition.jpg)![w=50%](imagenet_object_detection.jpg)

---
class: middle
# Well-known Datasets
## COCO
![w=100%,h=center](coco_segmentation.jpg)

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [IAM-OnDB](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database) | Pen tip movements of handwritten English from 221 writers. | 86k words |
| [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1) | Recordings of 630 speakers of 8 dialects of American English. | 6.3k sents |
| [CommonVoice](https://voice.mozilla.org/data) | 400k recordings from 20k people, around 500 hours of speech. | 400k
| [PTB](https://catalog.ldc.upenn.edu/LDC99T42) | _Penn Treebank_: 2500 stories from Wall Street Journal, with POS tags and parsed into trees. | 1M words |
| [PDT](https://ufal.mff.cuni.cz/prague-dependency-treebank) | _Prague Dependency Treebank_: Czech sentences annotated on 4 layers (word, morphological, analytical, tectogrammatical). | 1.9M words |
| [UD](http://universaldependencies.org/) | _Universal Dependencies_: Treebanks of 76 languages with consistent annotation of lemmas, POS tags, morphology and syntax. | 129 treebanks |
| [WMT](http://statmt.org/) | Aligned parallel sentences for machine translation. | gigawords |

---
# ILSVRC Image Recognition Error Rates

![w=80%,h=center](ilsvrc.svg)

~~~ ~
# ILSVRC Image Recognition Error Rates

![w=80%,h=center](ilsvrc-complete.svg)

---
# ILSVRC Image Recognition Error Rates

In summer 2017, a paper came out describing automatic generation of
neural architectures using reinforcement learning.

![w=100%](nas_net.pdf)

---
# ILSVRC Image Recognition Error Rates

The current state-of-the-art to my best knowledge is EfficientNet,
which combines automatic architecture discovery, multidimensional scaling
and elaborate dataset augmentation methods

![w=50%](efficientnet_flops.pdf)![w=50%](efficientnet_size.pdf)

---
class: wide
# Introduction to Machine Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
# How Good is Current Deep Learning

- DL has seen amazing progress in the last ten years.

~~~
- Is it enough to get a bigger brain (datasets, models, computer power)?

~~~
![w=50%,f=right](talosian.jpg)

~~~
- Problems compared to Human learning:
  - Sample efficiency
  - Human-provided labels
  - Robustness do data distribution
  - Stupid errors

---
# How Good is Current Deep Learning

![w=32%,f=right](thinking_fast_and_slow.jpg)

- Thinking fast and slow

~~~
  - System 1

    - intuitive
    - fast
    - automatic
    - frequent
    - unconscious
~~~

    Current DL
~~~
  - System 2

    - logical
    - slow
    - effortful
    - infrequent
    - conscious
~~~

    Future DL

---
# Curse of Dimensionality

![w=100%,v=middle](curse_of_dimensionality.png)

---
# Machine and Representation Learning

![w=35%,h=center](machine_learning.pdf)

---
section: Neural Nets '80s
# Neural Network Architecture √† la '80s

![w=45%,h=center](neural_network.svg)

---
# Neural Network Architecture

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(‚àë_j w_{i,j} x_j\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$‚Üíh = f\left(‚áâW ‚Üíx\right)$$

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

~~~
- $œÉ$ (sigmoid; logistic regression if there are no hidden layers)
  $$œÉ(x) ‚âù \frac{1}{1 + e^{-x}}$$

~~~
- $\softmax$ (maximum entropy model if there are no hidden layers)
  $$\softmax(‚Üíx) ‚àù e^‚Üíx$$
  $$\softmax(‚Üíx)_i ‚âù \frac{e^{x_i}}{‚àë_j e^{x_j}}$$

---
# Neural Network Activation Functions
## Hidden Layers
- none (does not help, composition of linear mapping is a linear mapping)

~~~
- $œÉ$ (but works badly ‚Äì nonsymmetrical, $\frac{dœÉ}{dx}(0) = 1/4$)

~~~
- $\tanh$
    - result of making $œÉ$ symmetrical and making derivation in zero 1
    - $\tanh(x) = 2œÉ(2x) - 1$
  ![w=45%,h=center](sigmoid_tanh.png)

~~~
- ReLU
    - $\max(0, x)$


---
# Universal Approximation Theorem '89


Let $œÜ(x)$ be a nonconstant, bounded and nondecreasing continuous function.
<br>(Later a proof was given also for $œÜ = \ReLU$.)

Then for any $Œµ > 0$ and any continuous function $f$ on $[0, 1]^m$ there exists
an $N ‚àà ‚Ñï, v_i ‚àà ‚Ñù, b_i ‚àà ‚Ñù$ and $‚Üí{w_i} ‚àà ‚Ñù^m$, such that if we denote
$$F(‚Üíx) = ‚àë_{i=1}^N v_i œÜ(‚Üí{w_i} \cdot ‚Üíx + b_i)$$
then for all $x ‚àà [0, 1]^m$
$$|F(‚Üíx) - f(‚Üíx)| < Œµ.$$

---
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=35%,h=center](relu_approx.svg)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units ‚Äì on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tanget and the tangent of the approximation until this point.

---
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-0.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-1.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-2.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-3.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-4.svg)

---
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $œÜ(x)$ (i.e., nonconstant, bounded and
nondecreasing continuous function like sigmoid):

~~~
- We can prove $œÜ$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)

---
section: ML Basics
# Estimators and Bias

An _estimator_ is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
_Bias_ of an estimator is the difference of the expected value of the estimator
and the true value being estimated.

~~~
If the bias is zero, we call the estimator _unbiased_, otherwise we call it
_biased_.

~~~
If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $‚Åáx_1,
\ldots, ‚Åáx_n$ idenpendent and identically distributed random variables, we might
estimate mean and variance as
$$ŒºÃÇ = \frac{1}{n} ‚àë\nolimits_i x_i,~~~œÉÃÇ_2 = \frac{1}{n} ‚àë\nolimits_i (x_i - ŒºÃÇ)^2.$$
~~~
Such estimate is biased, because $ùîº[œÉÃÇ^2] = (1 - \frac{1}{n})œÉ^2$, but the bias
converges to zero with increasing $n$.

~~~
Also, an unbiased estimator does not necessarily have small variance ‚Äì in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the main goal of _machine learning_ is to perform well on _previously
unseen_ data, so called **generalization error** or **test error**. We typically
estimate the generalization error using a **test set** of examples independent
of the training set, but generated by the same data generating distribution.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.pdf)

---
# Machine Learning Basics

We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.pdf)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than others.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $||‚ÜíŒ∏||^2$).

![w=70%,h=center](regularization.pdf)

---
# Machine Learning Basics

_Hyperparameters_ are not adapted by learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](double_descent.pdf)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent.pdf)

---
# Why do Neural Networks Generalize so Well

![w=90s,h=center](deep_double_descent_width.pdf)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent_size_epochs.pdf)
