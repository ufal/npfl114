class: title
## NPFL114, Lecture 01

# Introduction to Deep Learning

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Notation
- $a$, $‚Üía$, $‚áâA$, $‚á∂A$: scalar (integer or real), vector, matrix, tensor

- $‚Åáa$, $‚Åá‚Üía$, $‚Åá‚áâA$: scalar, vector, matrix random variable

--
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{‚àÇf}{‚àÇx}$: partial derivative of $f$ with respect to $x$

--
- $‚àá\_‚Üíx f$: gradient of $f$ with respect to $‚Üíx$, i.e.,
  $\left(\frac{‚àÇf(‚Üíx)}{‚àÇx\_1}, \frac{‚àÇf(‚Üíx)}{‚àÇx\_2}, \ldots, \frac{‚àÇf(‚Üíx)}{‚àÇx\_n}\right)$

---
# Random Variables
A random variable $‚Åáx$ is a result of a random process. It can be discrete or
continuous.

--
## Probability Distribution
Probability distribution describes how likely are individual values a random
variable can take.

The $‚Åáx ‚àº P$ denotes a random variable $‚Åáx$ has distribution $P$.

For discrete variables, probability that $‚Åáx$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(‚Åáx = x)$.

For discrete variables, probability that value of $‚Åáx$ lies in the interval
$[a, b]$ is given by $‚à´\_a^b p(x)\d x$.

---
# Random Variables

## Expectation
An expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$ùîº\_{‚Åáx ‚àº P}[f(x)] ‚âù ‚àë\_x P(x)f(x)$$

For continuous variables it is computed as:
$$ùîº\_{‚Åáx ‚àº p}[f(x)] ‚âù ‚à´\_x p(x)f(x)\d x$$

Expectation is linear, i.e.,
$$ùîº\_‚Åáx [Œ±f(x) + Œ≤g(x)] = Œ±ùîº\_‚Åáx [f(x)] + Œ≤ùîº\_‚Åáx [g(x)]$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from the
expectation.
$$\Var(f(x)) ‚âù ùîº\left[(f(x) - ùîº[f(x)])^2\right]$$

---
# Common Probability Distributions
## Bernoulli Distribution
Bernoulli distribution is a distribution over a binary random variable.
It has one parameter $œÜ ‚àà [0, 1]$, which specifies the probability of the random
variable being equal to 1.

--
## Categorical Distribution
Extension of Bernoulli distribution to random variables taking $k$ different
variables. It is parametrized by a $p ‚àà [0, 1]^k$, such that $‚àëp(i) = 1$.

---
# Information Theory

## Self Information

Amount of _surprise_ when a random variable is sampled.
- Should be zero for events with probability 1.
- Less likely events are more surprising.
- Independent events should have additive information.

$$I(x) ‚âù -\log P(x) = \log \frac{1}{P(x)}$$

--
## Entropy

Amount of _surprise_ in the whole distribution.

$$H(P) ‚âù ùîº\_{‚Åáx‚àºP}[I(x)] = -ùîº\_{‚Åáx‚àºP}[\log P(x)]$$
- for discrete $P$: $H(P) = -‚àë\_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -‚à´ P(x) \log P(x)\,\mathrm dx$

---
# Information Theory

## Cross-Entropy

$$H(P, Q) ‚âù -ùîº\_{‚Åáx‚àºP}[\log Q(x)]$$

- Gibbs inequality
    - $H(P, Q) ‚â• H(P)$
    - $H(P) = H(P, Q) ‚áî P = Q$
- generally $H(P, Q) ‚â† H(Q, P)$

--

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called _relative entropy_.

$$D\_\textrm{KL}(P || Q) ‚âù H(P, Q) - H(P) = ùîº\_{‚Åáx‚àºP}[\log P(x) - \log Q(x)]$$

- consequence of Gibbs inequality: $D\_\textrm{KL}(P || Q) ‚â• 0$
- generally $D\_\textrm{KL}(P || Q) ‚â† D\_\textrm{KL}(Q || P)$

---
class: middle
# Nonsymmetry of KL Divergence

![:pdf 100%](kl_nonsymmetry.pdf)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Œº$ and variance $œÉ^2$:
$$\N(x; Œº, œÉ^2) = \sqrt{\frac{1}{2œÄœÉ^2}} \exp \left(-\frac{(x - Œº)^2}{2œÉ^2}\right)$$

For standard values $Œº=0$ and $œÉ^2=1$ we get $\N(x; 0, 1) = \sqrt{\frac{1}{2œÄ}} e^{-\frac{x^2}{2}}$.

![:pdf 70%,center](normal_distribution.pdf)

---
# Why Normal Distribution

## Central Limit Theorem
A sum of independent identically distributed random variables with a limited
variance converges to normal distribution.

--
## Distribution with Maximal Entropy
Consider distributions with a given mean and variance. It can be proven
(using variational inference) that such distribution with _maximal entropy_
is exactly the normal distribution.

A distribution with maximal entropy can be considered the most general one,
containing as little additional assumptions as possible.

---
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

--

- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $x‚àà‚Ñù$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, ‚Ä¶
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, ‚Ä¶)
    - _reinforcement learning_, _semi-supervised learning_, ‚Ä¶
- Measure P
    - _accuracy_, _error rate_, _F-score_, ‚Ä¶

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [MNIST](http://yann.lecun.com/exdb/mnist/) | Images (28x28, grayscale) of handwritten digits. | 60k |
| [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 10 classes of objects. | 50k |
| [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 100 classes of objects (with 20 defined superclasses). | 50k |
| [ImageNet](http://image-net.org/) | Labeled object image database (labeled objects, some with bounding boxes). | 14.2M |
| [ImageNet-ILSVRC](http://image-net.org/challenges/LSVRC/) | Subset of ImageNet for Large Scale Visual Recognition Challenge, annotated with 1000 object classes and their bounding boxes. | 1.2M |
| [COCO](http://cocodataset.org/) | _Common Objects in Context_: Complex everyday scenes with descriptions (5) and highlighting of objects (91 types). | 2.5M |

---
# Well-known Datasets
## ImageNet-ILSVRC
![:image 40%](image_recognition.jpg)
![:image 54%](object_detection.jpg)

## COCO
![:image 100%,center](image_segmentation.jpg)

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [IAM-OnDB](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database) | Pen tip movements of handwritten English from 221 writers. | 86k words |
| [TIMIT](https://catalog.ldc.upenn.edu/ldc93s1) | Recordings of 630 speakers (10 sentences each) of 8 major dialects of American English. | 6.3k sentences |
| [CommonVoice](https://voice.mozilla.org/data) | Nearly 400,000 recordings from 20,000 different people, around 500 hours of speech. | 400k
| [PTB](https://catalog.ldc.upenn.edu/ldc99t42) | _Penn Treebank_: 2500 stories from Wall Street Journal, with POS tags and parsed into trees. | 1M words |
| [PDT](https://ufal.mff.cuni.cz/prague-dependency-treebank) | _Prague Dependency Treebank_: Czech sentences annotated on 4 layers (word, morphological, analytical, tectogrammatical). | 1.9M words |
| [UD](http://universaldependencies.org/) | _Universal Dependencies_: Treebanks of 60 languages with consistent annotation of lemmas, POS tags, morphology and syntax. | 102 treebanks |
| [WMT](http://statmt.org/) | Aligned parallel sentences for machine translation. | gigawords |

---
# ILSVRC Image Recognition Accuracies

![:plotly 100%,100%,center]({"data":[
  {"csv":{"file":"ilsvrc.csv","x":"Date","y":"Score"},"type":"bar","name":"Neural Networks"},
  {"csv":{"file":"ilsvrc.csv","x":"Date","y":"Human"},"type":"scatter","name":"Human Performance"}
],"layout":{
  "xaxis":{"type":"category"},"legend":{"orientation":"h"}
}})

---
# ILSVRC Image Recognition Accuracies

Recently (summer 2017), a paper came out describing automatic generation of
neural architectures using reinforcement learning.

![:pdf 100%,,margin-top:70px](nas_net.pdf)

---
class: middle, full
# Introduction to Machine Learning History

![:image 100%](figure1_ANN_history.jpg)

---
class: center, middle
# Curse of Dimensionality

![:image 100%](curse_of_dimensionality.png)

---
class: center, middle
# Machine and Representation Learning

![:pdf 50%](machine_learning.pdf)

---
class: center, middle
# Neural Network Architecture √† la '80s

![:dot 65%](digraph G { rankdir=LR; splines=line;
  x3 [ label=<x<sub><font point-size="10">3</font></sub>>]
  x4 [ label=<x<sub><font point-size="10">4</font></sub>>]
  x1 [ label=<x<sub><font point-size="10">1</font></sub>>]
  x2 [ label=<x<sub><font point-size="10">2</font></sub>>]
  h3 [ label=<h<sub><font point-size="10">3</font></sub>>]
  h4 [ label=<h<sub><font point-size="10">4</font></sub>>]
  h1 [ label=<h<sub><font point-size="10">1</font></sub>>]
  h2 [ label=<h<sub><font point-size="10">2</font></sub>>]
  y1 [ label=<y<sub><font point-size="10">1</font></sub>>]
  y2 [ label=<y<sub><font point-size="10">2</font></sub>>]
  {x1, x2, x3, x4} -> {h1, h2, h3, h4} -> {y1, y2}
  lx [ label=<Input<br/>layer>; shape=none ]
  la [ label=<Hidden<br/>layer>; shape=none ]
  ly [ label=<Output<br/>layer>; shape=none ]
  lx -> la -> ly [ style=invis ]
})

---
# Neural Network Architecture

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on output layer.
$$h\_i = f\left(‚àë\_j w\_{i,j} x\_j\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$‚Üíh = f\left(‚áâW ‚Üíx\right)$$

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

- $œÉ$ (sigmoid; logistic regression if there are no hidden layers)
  $$œÉ(x) ‚âù \frac{1}{1 + e^{-x}}$$

- $\softmax$ (maximum entropy markov model if there are no hidden layers)
  $$\softmax(‚Üíx) ‚àù e^‚Üíx$$
  $$\softmax(‚Üíx)\_i ‚âù \frac{e^{x\_i}}{‚àë\_j e^{x\_j}}$$

---
# Neural Network Activation Functions
## Hidden Layers
- none (does not help, composition of linear mapping is a linear mapping)

- $œÉ$ (but works badly ‚Äì nonsymmetrical, $\frac{dœÉ}{dx}(0) = 1/4$)

- $\tanh$
    - result of making $œÉ$ symmetrical and making derivation in zero 1
    - $\tanh(x) = 2œÉ(2x) - 1$

- ReLU
    - $\max(0, x)$


---
# Universal Approximation Theorem '89

Let $œÜ(x)$ be nonconstant, bounded and monotonically increasing continuous function.

Then for any $Œµ > 0$ and any continuous function $f$ on $[0, 1]^m$ there exists
an $N ‚àà ‚Ñï, v\_i ‚àà ‚Ñù, b\_i ‚àà ‚Ñù$ and $‚Üí{w\_i} ‚àà ‚Ñù^m$, such that if we denote
$$F(‚Üíx) = ‚àë\_{i=1}^N v\_i œÜ(‚Üí{w\_i} \cdot ‚Üíx + b\_i)$$
then for all $x ‚àà [0, 1]^m$ $$|F(‚Üíx) - f(‚Üíx)| < Œµ.$$

---
# Evolving ReLU Approximation

![:plotly 100%,100%,center]({"data":[
  {"csv":{"file":"relu_approximation_epoch1.csv","x":"X","y":"Y"},"type":"scatter","mode":"lines+markers","name":"Epoch 1","opacity":0.3,
   "marker":{"size":8,"symbol":"cross","color":"#2b2"},"line":{"color":"#22b"}},
  {"csv":{"file":"relu_approximation_epoch2.csv","x":"X","y":"Y"},"type":"scatter","mode":"lines+markers","name":"Epoch 2","opacity":0.45,
   "marker":{"size":8,"symbol":"cross","color":"#2b2"},"line":{"color":"#22b"}},
  {"csv":{"file":"relu_approximation_epoch3.csv","x":"X","y":"Y"},"type":"scatter","mode":"lines+markers","name":"Epoch 3","opacity":0.65,
   "marker":{"size":8,"symbol":"cross","color":"#2b2"},"line":{"color":"#22b"}},
  {"csv":{"file":"relu_approximation_epoch4.csv","x":"X","y":"Y"},"type":"scatter","mode":"lines+markers","name":"Epoch 4","opacity":1,
   "marker":{"size":8,"symbol":"cross","color":"#2b2"},"line":{"color":"#22b"}},
  {"csv":{"file":"relu_approximation_original.csv","x":"X","y":"Y"},"type":"scatter","mode":"lines","name":"Original Function",
   "line":{"shape":"spline","color":"#e33"}}
],"layout":{"xaxis":{"range":[-1,1]},"legend":{"orientation":"h"}}})
