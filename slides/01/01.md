title: NPFL114, Lecture 1
class: title, langtech, cc-by-nc-sa
# Introduction to Deep Learning

## Milan Straka

### February 14, 2022

---
# What is Deep Learning

![w=38%,h=center](deep_learning.jpg)

---
# Deep Learning Highlights
- Image recognition

~~~ ~
# Deep Learning Highlights
![w=60%,h=center](imagenet_recognition.jpg)

~~~ ~~
- Object detection
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](object_detection.svgz)

~~~ ~~
- Image segmentation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](image_segmentation.svgz)

~~~ ~~
- Human pose estimation
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](human_pose_estimation.jpg)

~~~ ~~
- Image labeling
~~~ ~
# Deep Learning Highlights
![w=75%,h=center](image_labeling.svgz)

~~~ ~~
- Visual question answering
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](vqa.svgz)

~~~ ~~
- Speech recognition and generation
~~~ ~
# Deep Learning Highlights

<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/washington_gen.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/fox_question.wav"></audio>
<audio controls style="width: 32%"><source src="https://google.github.io/tacotron/publications/tacotron2/demos/gan_or_vae.wav"></audio>

![w=100%](tacotron_comparison.svgz)

~~~ ~~
- Lip reading
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](lrw_showcase.svgz)
![w=70%,h=center](lipnet_saliency.svgz)

~~~ ~~
- Machine translation
~~~ ~
# Deep Learning Highlights
![w=44%,h=center](attention_visualization.svgz)

~~~ ~~
- Machine translation without parallel data
~~~ ~
# Deep Learning Highlights
![w=70%,h=center](umt_ideas.png)
![w=30%,h=center](umt_comparison.svgz)

~~~ ~~
- Chess, Go and Shogi
~~~ ~
# Deep Learning Highlights
![w=95%,h=center](a0_results.svgz)

~~~ ~~
- Multiplayer Capture the flag
~~~ ~
# Deep Learning Highlights
![w=100%,v=middle](ctf_overview.svgz)

~~~ ~~
- StarCraft II
~~~ ~
# Deep Learning Highlights
![w=71%,h=center](alphastar_results.svgz)
~~~ ~~

---
section: Organization

# Organization

**Course Website:** https://ufal.mff.cuni.cz/courses/npfl114
~~~
  - Slides, recordings, assignments, Exam questions
~~~

**Course Repository:** https://github.com/ufal/npfl114
- Templates for the assignments, slide sources.

~~~
## Piazza

- Piazza will be used as a communication platform.

  You can post questions or notes,
  - privately to the instructors, or
~~~
  - to everyone (signed or anonymously).
~~~

  Students can answer other student's questions too, which allows you to get
  faster response. Please do not send complete solutions to other students,
  only excerpts of the source files.

~~~
- Please use Piazza for **all communication** with the instructors.
~~~
- You will get the invite link after the first lecture.

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you will be able to create an account
  using your CAS credentials and should automatically see the right group.
~~~
- Otherwise, there will be **instructions** on **Piazza** how to get
  ReCodEx account (generally you will need to send me a message with several
  pieces of information and I will send it to ReCodEx administrators in
  batches).

---
# Course Requirements

## Practicals
~~~

- There will be 2-3 assignments a week, each with a 2-week deadline.
~~~
  - There is also another week-long second deadline, but for less points.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the practicals, you need to get 80 non-bonus points. There will be
  assignments for at least 120 non-bonus points.
~~~
- If you get more than 80 points (be it bonus or non-bonus), they will be
  all transferred to the exam. Additionally, if you solve all the assignments,
  you pass the exam with grade 1.

~~~
## Lecture

You need to pass a written exam (or solve all the assignments).
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus the surplus
  points from the practicals and plus at most 10 surplus points for **community
  work** (improving slides, â€¦).
~~~
- You need 60/75/90 points to pass with grade 3/2/1; 75 points for PhD students.

---
section: Notation
# Notation

- $a$, $â†’a$, $â‡‰A$, $â‡¶A$: scalar (integer or real), vector, matrix, tensor

~~~
  - all vectors are always **column** vectors
~~~
  - transposition changes a column vector into a row vector, so $â†’a^T$ is a row vector
~~~
  - we denote **scalar product** between vectors $â†’a$ and $â†’b$ as $â†’a^T â†’b$
    - we understand it as matrix multiplication
~~~

- $â‡a$, $â‡â†’a$, $â‡â‡‰A$: scalar, vector, matrix random variable

~~~
- $\frac{df}{dx}$: derivative of $f$ with respect to $x$

- $\frac{âˆ‚f}{âˆ‚x}$: partial derivative of $f$ with respect to $x$

~~~
- $âˆ‡_{â†’x} f(â†’x)$: gradient of $f$ with respect to $â†’x$, i.e.,
  $\left(\frac{âˆ‚f(â†’x)}{âˆ‚x_1}, \frac{âˆ‚f(â†’x)}{âˆ‚x_2}, \ldots, \frac{âˆ‚f(â†’x)}{âˆ‚x_n}\right)$

---
section: Random Variables
# Random Variables
A random variable $â‡x$ is a result of a random process. It can be discrete or
continuous.

~~~
## Probability Distribution
A probability distribution describes how likely are individual values a random
variable can take.

The notation $â‡x âˆ¼ P$ stands for a random variable $â‡x$ having a distribution $P$.

~~~
For discrete variables, the probability that $â‡x$ takes a value $x$ is denoted as
$P(x)$ or explicitly as $P(â‡x = x)$. All probabilities are non-negative and sum
of probabilities of all possible values of $â‡x$ is $âˆ‘_x P(â‡x=x) = 1$.

~~~
For continuous variables, the probability that the value of $â‡x$ lies in the interval
$[a, b]$ is given by $âˆ«_a^b p(x)\d x$, where $p(x)$ is the _probability density
function_, which is always non-negative and integrates to 1 over the range of
all values of $â‡x$.

---
style: .katex-display { margin: 0.8em 0 }
# Joint, Conditional, Marginal Probability

![w=44%,f=right](joint_probability.svgz)

For two random variables, **joint probability distribution** is a distribution
of all possible pairs of outputs (and analogously for more than two):

$$P(â‡x = x_2, â‡y = y_1).$$

~~~
**Marginal distribution** is a distribution of one (orÂ aÂ subset) of the random
variables and can be obtained by summing over the other variable(s):
$$P(â‡x=x_2) = {\small âˆ‘\nolimits}_y P(â‡x = x_2, â‡y = y).$$

~~~
**Conditional distribution** is a distribution of one (or a subset) of the
random variables, given that another event has already occurred:
$$P(â‡x=x_2 | â‡y=y_1) = P(â‡x = x_2, â‡y = y_1) / P(â‡y = y_1).$$

~~~
If $P(â‡x, â‡y) = P(â‡x) â‹… P(â‡y)$ or $P(â‡x | â‡y) = P(â‡x)$, random variables $â‡x$ and $â‡y$ are **independent**.

---
# Random Variables

## Expectation
The expectation of a function $f(x)$ with respect to discrete probability
distribution $P(x)$ is defined as:
$$ğ”¼_{â‡x âˆ¼ P}[f(x)] â‰ âˆ‘_x P(x)f(x).$$

For continuous variables it is computed as:
$$ğ”¼_{â‡x âˆ¼ p}[f(x)] â‰ âˆ«_x p(x)f(x)\d x.$$

~~~
If the random variable is obvious from context, we can write only $ğ”¼_P[x]$
or even $ğ”¼[x]$.

~~~
Expectation is linear, i.e.,
$$ğ”¼_â‡x [Î±f(x) + Î²g(x)] = Î±ğ”¼_â‡x [f(x)] + Î²ğ”¼_â‡x [g(x)].$$

---
# Random Variables

## Variance
Variance measures how much the values of a random variable differ from its
mean $Î¼ = ğ”¼[x]$.

$$\begin{aligned}
  \Var(x) &â‰ ğ”¼\left[\big(x - ğ”¼[x]\big)^2\right]\textrm{, or more generally,} \\
  \Var(f(x)) &â‰ ğ”¼\left[\big(f(x) - ğ”¼[f(x)]\big)^2\right].
\end{aligned}$$

~~~
It is easy to see that
$$\Var(x) = ğ”¼\left[x^2 - 2xğ”¼[x] + \big(ğ”¼[x]\big)^2\right] = ğ”¼\left[x^2\right] - \big(ğ”¼[x]\big)^2,$$
because $ğ”¼\big[2xğ”¼[x]\big] = 2(ğ”¼[x])^2$.

~~~
Variance is connected to $ğ”¼[x^2]$, the **second moment** of a random
variable â€“ it is in fact a **centered** second moment.

---
# Common Probability Distributions
## Bernoulli Distribution
The Bernoulli distribution is a distribution over a binary random variable.
It has a single parameter $Ï† âˆˆ [0, 1]$, which specifies the probability of the random
variable being equal to 1.

~~~
$$\begin{aligned}
  P(x) &= Ï†^x (1-Ï†)^{1-x} \\
  ğ”¼[x] &= Ï† \\
  \Var(x) &= Ï†(1-Ï†)
\end{aligned}$$

![w=60%,h=center](bernoulli_variance.svgz)

---
# Common Probability Distributions
## Categorical Distribution
Extension of the Bernoulli distribution to random variables taking one of $K$ different
discrete outcomes. It is parametrized by $â†’p âˆˆ [0, 1]^K$ such that $âˆ‘_{i=0}^{K-1} p_{i} = 1$.

~~~
We represent outcomes as vectors $âˆˆ \{0, 1\}^K$ in the **one-hot encoding**.
Therefore, an outcome $x âˆˆ \{0, 1, â€¦, K-1\}$ is represented as a vector
$$â†’1_x â‰ \big([i = x]\big)_{i=0}^{K-1} = \big(\underbrace{0, â€¦, 0}_{k}, 1, \underbrace{0, â€¦, 0}_{K-k-1}\big).$$

~~~
The outcome probability, mean and variance are very similar to the Bernoulli
distribution.
$$\begin{aligned}
  P(â†’x) &= âˆ\nolimits_{i=0}^{K-1} p_i^{x_i} \\
  ğ”¼[x_i] &= p_i \\
  \Var(x_i) &= p_i(1-p_i) \\
\end{aligned}$$

---
section: Information Theory
# Information Theory

## Self Information

Amount of **surprise** when a random variable is sampled.
~~~
- Should be zero for events with probability 1.
~~~
- Less likely events are more surprising.
~~~
- Independent events should have **additive** information.

~~~
$$I(x) â‰ -\log P(x) = \log \frac{1}{P(x)}$$

---
# Information Theory

## Entropy

Amount of **surprise** in the whole distribution.
$$H(P) â‰ ğ”¼_{â‡xâˆ¼P}[I(x)] = -ğ”¼_{â‡xâˆ¼P}[\log P(x)]$$

~~~
- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

~~~
Because $\lim_{x â†’ 0} x \log x = 0$, for $P(x) = 0$ we consider\
$P(x) \log P(x)$ to be zero.

~~~ ~~~
![w=40%,f=right](entropy_example.svgz)

- for discrete $P$: $H(P) = -âˆ‘_x P(x) \log P(x)$
- for continuous $P$: $H(P) = -âˆ« P(x) \log P(x)\,\mathrm dx$

Because $\lim_{x â†’ 0} x \log x = 0$, for $P(x) = 0$ we consider
$P(x) \log P(x)$ to be zero.

~~~
Note that in the continuous case, the continuous entropy (also called
_differential entropy_) has slightly different semantics, for example, it can be
negative.

~~~
From now on, all logarithms are _natural logarithms_ with base $e$.

---
# Information Theory

## Cross-Entropy

$$H(P, Q) â‰ -ğ”¼_{â‡xâˆ¼P}[\log Q(x)]$$

~~~
**Gibbs inequality** states that
- $H(P, Q) â‰¥ H(P)$
- $H(P) = H(P, Q) â‡” P = Q$
~~~
- Proof: Using the fact that $\log x â‰¤ (x-1)$ with equality only for $x=1$, we get
  $$âˆ‘_x P(x) \log \frac{Q(x)}{P(x)} â‰¤ âˆ‘_x P(x) \left(\frac{Q(x)}{P(x)}-1\right) = âˆ‘_x Q(x) - âˆ‘_x P(x) = 0.$$
~~~
- Corollary: For a categorical distribution with $n$ outcomes, $H(P) â‰¤ \log n$,
  because for $Q(x) = 1/n$ we get $H(P) â‰¤ H(P, Q) = -âˆ‘_x P(x) \log Q(x) = \log n.$
~~~

Note that generally $H(P, Q) â‰  H(Q, P)$.

---
# Information Theory

## Kullback-Leibler Divergence (KL Divergence)

Sometimes also called **relative entropy**.

$$D_\textrm{KL}(P \| Q) â‰ H(P, Q) - H(P) = ğ”¼_{â‡xâˆ¼P}[\log P(x) - \log Q(x)]$$

~~~
- consequence of Gibbs inequality: $D_\textrm{KL}(P \| Q) â‰¥ 0$, $D_\textrm{KL}(P \| Q) = 0$ iff $P = Q$
~~~
- generally $D_\textrm{KL}(P \| Q) â‰  D_\textrm{KL}(Q \| P)$

---
# Nonsymmetry of KL Divergence

![w=100%,v=middle](kl_nonsymmetry.svgz)

---
# Common Probability Distributions
## Normal (or Gaussian) Distribution
Distribution over real numbers, parametrized by a mean $Î¼$ and variance $Ïƒ^2$:
$$ğ“(x; Î¼, Ïƒ^2) = \sqrt{\frac{1}{2Ï€Ïƒ^2}} \exp \left(-\frac{(x - Î¼)^2}{2Ïƒ^2}\right)$$

~~~
For standard values $Î¼=0$ and $Ïƒ^2=1$ we get $ğ“(x; 0, 1) = \sqrt{\frac{1}{2Ï€}} e^{-\frac{x^2}{2}}$.

![w=45%,h=center](normal_distribution.svgz)

---
# Why Normal Distribution

## Central Limit Theorem
The sum of independent identically distributed random variables
with finite variance converges to normal distribution.

~~~
## Principle of Maximum Entropy
Given a set of constraints, a distribution with maximal entropy fulfilling the
constraints can be considered the most general one, containing as little
additional assumptions as possible.

~~~
Considering distributions on all real numbers with a given mean and variance, it
can be proven (using variational inference) that such a distribution with
**maximum entropy** is exactly the normal distribution.

---
section: Machine Learning
# Machine Learning

A possible definition of learning from Mitchell (1997):
>  A computer program is said to learn from experience E with respect to some
>  class of tasks T and performance measure P, if its performance at tasks in
>  T, as measured by P, improves with experience E.

~~~
- Task T
    - _classification_: assigning one of $k$ categories to a given input
    - _regression_: producing a number $xâˆˆâ„$ for a given input
    - _structured prediction_, _denoising_, _density estimation_, â€¦
~~~
- Measure P
    - _accuracy_, _error rate_, _F-score_, â€¦
~~~
- Experience E
    - _supervised_: usually a dataset with desired outcomes (_labels_ or
      _targets_)
    - _unsupervised_: usually data without any annotation (raw text, raw images, â€¦)
    - _reinforcement learning_, _semi-supervised learning_, â€¦

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [MNIST](http://yann.lecun.com/exdb/mnist/) | Images (28x28, grayscale) of handwritten digits. | 60k |
| [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 10 classes of objects. | 50k |
| [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) | Images (32x32, color) of 100 classes of objects (with 20 defined superclasses). | 50k |
| [ImageNet](http://image-net.org/) | Labeled object image database (labeled objects, some with bounding boxes). | 14.2M |
| [ImageNet-ILSVRC](http://image-net.org/challenges/LSVRC/) | Subset of ImageNet for Large Scale Visual Recognition Challenge, annotated with 1000 object classes and their bounding boxes. | 1.2M |
| [COCO](http://cocodataset.org/) | _Common Objects in Context_: Complex everyday scenes with descriptions (5) and highlighting of objects (91 types). | 2.5M |

---
class: middle
# Well-known Datasets
## ImageNet-ILSVRC
![w=70%,mw=50%,h=center](imagenet_recognition.jpg)![w=50%](imagenet_object_detection.jpg)

---
class: middle
# Well-known Datasets
## COCO
![w=100%,h=center](coco_segmentation.jpg)

---
# Well-known Datasets

| Name | Description | Instances |
| ------ | ------------- | ----------- |
| [IAM-OnDB](http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database) | Pen tip movements of handwritten English from 221 writers. | 86k words |
| [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1) | Recordings of 630 speakers of 8 dialects of American English. | 6.3k sents |
| [CommonVoice](https://voice.mozilla.org/data) | 400k recordings from 20k people, around 500 hours of speech. | 400k
| [PTB](https://catalog.ldc.upenn.edu/LDC99T42) | _Penn Treebank_: 2500 stories from Wall Street Journal, with POS tags and parsed into trees. | 1M words |
| [PDT](https://ufal.mff.cuni.cz/prague-dependency-treebank) | _Prague Dependency Treebank_: Czech sentences annotated on 4 layers (word, morphological, analytical, tectogrammatical). | 1.9M words |
| [UD](http://universaldependencies.org/) | _Universal Dependencies_: Treebanks of 104 languages with consistent annotation of lemmas, POS tags, morphology, syntax. | 183 treebanks |
| [WMT](http://statmt.org/) | Aligned parallel sentences for machine translation. | gigawords |

---
# ILSVRC Image Recognition Error Rates

![w=80%,h=center](ilsvrc.svg)

~~~ ~
# ILSVRC Image Recognition Error Rates

![w=80%,h=center](ilsvrc-complete.svg)

---
# ILSVRC Image Recognition Error Rates

In summer 2017, a paper came out describing automatic generation of
neural architectures using reinforcement learning.

![w=100%](nas_net.svgz)

---
# ILSVRC Image Recognition Error Rates

Currently, one of the best architectures is EfficientNet, which combines
automatic architecture discovery, multidimensional scaling and elaborate dataset
augmentation methods.

![w=50%](efficientnet_flops.svgz)![w=50%](efficientnet_size.svgz)

---
# ILSVRC Image Recognition Error Rates

EfficientNet was further improved by EfficientNetV2 two years later.

![w=100%](efficientnetv2.svgz)

---
# Machine Translation Improvements

To illustrate deep neural networks improvements in other domains, consider the
Englishâ†’Czech results of the international Workshop on Machine Translation.
Both the automatic BLEU metric and manual evaluation are presented.

![w=50%](nmt_bleu.svgz)![w=50%](nmt_relative.svgz)

- TectoMT parses the input, transfers to the other language, generates the sentence;
- RBMT is the PC-Translator software;
- SMT is statistical machine translation using the Moses system;
- Online is an online translation system (Google in 2009, `Online-B` since 2010);
- **NMT** is the neural machine translation using deep neural networks.

---
class: wide
# Introduction to Deep Learning History

![w=99%,h=center](figure1_ANN_history.jpg)

---
# How Good is Current Deep Learning

- DL has seen amazing progress in the last ten years.

~~~
- Is it enough to get a bigger brain (datasets, models, computer power)?

~~~
![w=50%,f=right](talosian.jpg)

~~~
- Problems compared to Human learning:
  - Sample efficiency
~~~
  - Human-provided labels
~~~
  - Robustness to data distribution change
~~~
  - Stupid errors

---
# How Good is Current Deep Learning

![w=50%,h=center](nn_consciousness.svgz)

~~~
![w=50%](nn_consciousness_reply1.svgz)![w=50%](nn_consciousness_reply2.svgz)

---
# How Good is Current Deep Learning

![w=32%,f=right](thinking_fast_and_slow.jpg)

- Thinking fast and slow

~~~
  - System 1

    - intuitive
    - fast
    - automatic
    - frequent
    - unconscious
~~~

    Current DL
~~~
  - System 2

    - logical
    - slow
    - effortful
    - infrequent
    - conscious
~~~

    Future DL

---
# Curse of Dimensionality

![w=100%,v=middle](curse_of_dimensionality.png)

---
# Machine and Representation Learning

![w=35%,h=center](machine_learning.svgz)

---
section: NNs '80s
# Neural Network Architecture Ã  la '80s

![w=45%,h=center](neural_network.svg)

---
# Neural Network Architecture

There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(âˆ‘_j w_{i,j} x_j + b_i\right)$$

~~~
If the network is composed of layers, we can use matrix notation and write
$$â†’h = f\left(â‡‰W â†’x + â†’b\right),$$
where $â‡‰W âˆˆ â„^{|\mathit{hidden~neurons}|Ã—|\mathit{input~neurons}|}$ is a matrix
of weights and $â†’b âˆˆ â„^{|\mathit{hidden~neurons}|}$ is a vector of biases.

---
# Neural Network Activation Functions
## Output Layers
- none (linear regression if there are no hidden layers)

~~~
- $Ïƒ$ (sigmoid; logistic regression if there are no hidden layers)
  $$Ïƒ(x) â‰ \frac{1}{1 + e^{-x}}$$
  is used to model a probability $p$ of a binary event; its input is called
  a **logit**, $\log \frac{p}{1-p}$

~~~
- $\softmax$ (maximum entropy model if there are no hidden layers)
  $$\begin{gathered}
    \softmax(â†’x) âˆ e^{â†’x} \\
    \softmax(â†’x)_i â‰ \frac{e^{x_i}}{âˆ‘_j e^{x_j}}
  \end{gathered}$$
  is used to model probability distribution $â†’p$; its input is called
  a **logit**, $\log(â†’p) + c$

---
# Neural Network Activation Functions
## Hidden Layers
- none: does not help, composition of linear mapping is a linear mapping

~~~
- $Ïƒ$: however, it works badly â€“ nonsymmetrical, repeated application converges to
  the fixed point $x = Ïƒ(x) â‰ˆ 0.659$, and $\frac{dÏƒ}{dx}(0) = 1/4$

~~~
- $\tanh$
    - result of making $Ïƒ$ symmetrical and making the derivative in zero 1
    - $\tanh(x) = 2Ïƒ(2x) - 1$
  ![w=45%,h=center](sigmoid_tanh.png)

~~~
- ReLU: $\max(0, x)$


---
# Universal Approximation Theorem '89

Let $Ï†(x):â„ â†’ â„$ be a nonconstant, bounded and nondecreasing continuous function. \
(Later a proof was given also for $Ï† = \ReLU$ and even for any nonpolynomial
function.)

For any $Îµ > 0$ and any continuous function $f: [0, 1]^D â†’ â„$, there exists
$H âˆˆ â„•$, $â†’v âˆˆ â„^H$, $â†’b âˆˆ â„^H$ and $â‡‰W âˆˆ â„^{DÃ—H}$, such that if we denote
$$F(â†’x) = â†’v^T Ï†(â†’x^T â‡‰W + â†’b) = âˆ‘_{i=1}^H v_i Ï†(â†’x^T â†’{W_{*, i}} + b_i),$$
where $Ï†$ is applied elementwise,
~~~
then for all $â†’x âˆˆ [0, 1]^D$:
$$|F(â†’x) - f(â†’x)| < Îµ.$$

---
# Universal Approximation Theorem for ReLUs

Sketch of the proof:

~~~
- If a function is continuous on a closed interval, it can be approximated by
  a sequence of lines to arbitrary precision.

![w=35%,h=center](relu_approx.svg)

~~~
- However, we can create a sequence of $k$ linear segments as a sum of $k$ ReLU
  units â€“ on every endpoint a new ReLU starts (i.e., the input ReLU value is
  zero at the endpoint), with a tangent which is the difference between the
  target tangent and the tangent of the approximation until this point.

---
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-0.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-1.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-2.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-3.svg)

~~~ ~
# Evolving ReLU Approximation
![w=75%,h=center](relu_approx-4.svg)

---
# Universal Approximation Theorem for Squashes

Sketch of the proof for a squashing function $Ï†(x)$ (i.e., nonconstant, bounded and
nondecreasing continuous function like sigmoid):

~~~
- We can prove $Ï†$ can be arbitrarily close to a hard threshold by compressing
  it horizontally.

![w=38%,h=center](universal_approximation_squash.png)

~~~
- Then we approximate the original function using a series of straight line
  segments

![w=38%,h=center](universal_approximation_rectangles.png)
