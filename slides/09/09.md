title: NPFL114, Lecture 9
class: title, langtech, cc-by-nc-sa

# Word2Vec, Seq2seq, NMT

## Milan Straka

### April 27, 2020

---
section: Word2vec
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

~~~

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

**Words that are used in the same contexts tend to have similar meanings**.

The distributional hypothesis is usually attributed to Firth (1957):
> _You shall know a word by a company it keeps._

---
# Word2Vec

![w=70%,h=center](word2vec.pdf)

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

---
# Word2Vec

![w=100%](word2vec_composability.pdf)

---
# Word2Vec â€“ SkipGram Model

![w=55%,h=center,mh=70%](word2vec.pdf)

Considering input word $w_i$ and output $w_o$, the Skip-gram model defines
$$p(w_o | w_i) â‰ \frac{e^{â‡‰W_{w_o}^\top â‡‰V_{w_i}}}{âˆ‘_w e^{â‡‰W_w^\top â‡‰V_{w_i}}}.$$

---
# Word2Vec â€“ Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n_1, n_2, \ldots, n_L$, we define
$$p_\textrm{HS}(w | w_i) â‰ âˆ_{j=1}^{L-1} Ïƒ(\textrm{[+1 if }n_{j+1}\textrm{  is right child else -1]} \cdot â‡‰W_{n_j}^\top â‡‰V_{w_i}).$$

---
# Word2Vec â€“ Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

We could also only sample the _negative examples_ instead of training all of
them.

This gives rise to the following _negative sampling_ objective:
$$l_\textrm{NEG}(w_o, w_i) â‰ \log Ïƒ(â‡‰W_{w_o}^\top â‡‰V_{w_i}) + âˆ‘_{j=1}^k ğ”¼_{w_j âˆ¼ P(w)} \log \big(1 - Ïƒ(â‡‰W_{w_j}^\top â‡‰V_{w_i})\big).$$

~~~

For $P(w)$, both uniform and unigram distribution $U(w)$ work, but
$$U(w)^{3/4}$$
outperforms them significantly (this fact has been reported in several papers by
different authors).

---
section: Subword Embeddings
# Recurrent Character-level WEs

![w=80%,h=center](cle_rnn_examples.pdf)

---
# Convolutional Character-level WEs

![w=100%](cle_cnn_examples.pdf)
---
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

~~~
The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept;
~~~
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K âˆ¼ 10^6$ is used).

---
# Charagram WEs

![w=100%,v=middle](cle_charagram_examples.pdf)

---
# Charagram WEs

![w=48%,h=center](cle_charagram_ngrams.pdf)

---
section: Seq2seq
class: middle, center
# Sequence-to-Sequence Architecture

# Sequence-to-Sequence Architecture

---
# Sequence-to-Sequence Architecture

![w=100%,v=middle](seq2seq.pdf)

---
# Sequence-to-Sequence Architecture

![w=45%,h=center](encoder_decoder.pdf)

---
# Sequence-to-Sequence Architecture

## Training

![w=50%,f=right](../07/sequence_prediction_training.pdf)

The so-called _teacher forcing_ is used during training â€“ the gold outputs are
used as inputs during training.

~~~
## Inference

![w=50%,f=right](../07/sequence_prediction_inference.pdf)

During inference, the network processes its own predictions.

Usually, the generated logits are processed by an $\argmax$, the chosen word
embedded and used as next input.

---
# Tying Word Embeddings

![w=26%,h=center](tying_embeddings.pdf)

---
section: Attention
# Attention

![w=35%,f=right](attention.pdf)

As another input during decoding, we add _context vector_ $c_i$:
$$â†’s_i = f(â†’s_{i-1}, â†’y_{i-1}, â†’c_i).$$

~~~
We compute the context vector as a weighted combination of source sentence
encoded outputs:
$$â†’c_i = âˆ‘_j Î±_{ij} â†’h_j$$

~~~
The weights $Î±_{ij}$ are softmax of $e_{ij}$ over $j$,
$$â†’Î±_i = \softmax(â†’e_i),$$
with $e_{ij}$ being
$$e_{ij} = â†’v^\top \tanh(â‡‰Vâ†’h_j + â‡‰Wâ†’s_{i-1} + â†’b) .$$

---
# Attention

![w=45%,h=center](attention_visualization.pdf)

---
section: SubWords
# Subword Units

Translate _subword units_ instead of words. The subword units can be generated
in several ways, the most commonly used are:

~~~
- **BPE**:
  Using the _byte pair encoding_ algorithm. Start with individual characters plus
  a special end-of-word symbol $â‹…$. Then, merge the most occurring symbol pair
  $A, B$ by a new symbol $AB$, with the symbol pair never crossing word boundary
  (so that the end-of-word symbol cannot be inside a subword).

~~~
  Considering a dictionary with words _low, lowest, newer, wider_, a possible
  sequence of merges:

  $$\begin{aligned}
    r \,\,\, â‹… & â†’ râ‹… \\
    l \,\,\, o & â†’ lo \\
    lo \,\,\, w & â†’ low \\
    e \,\,\, râ‹… & â†’ erâ‹… \\
  \end{aligned}$$

---
# Subword Units

- **Wordpieces**:
  Given a text divided into subwords, we can compute unigram probability of
  every subword, and then get the likelihood of the text under a unigram language
  model by multiplying the probabilities of the subwords in the text.

~~~
  When we have only a text and a subword dictionary, we divide the text in
  a greedy fashion, iteratively choosing the longest existing subword.

~~~
  When constructing the subwords, we again start with individual characters, and
  then repeatedly join such a pair of subwords, which increases the unigram
  language model likelihood the most.

~~~
Both approaches give very similar results; a biggest difference is that during
the inference:
- for BPE, the sequence of merges must be performed in the same order as during
  the construction of the BPE;
~~~
- for Wordpieces, it is enough to find longest matches from the subword
  dictionary.

~~~
Usually quite little subword units are used (32k-64k), often generated on the
union of the two vocabularies (the so-called _joint BPE_ or _shared
wordpieces_).

---
section: GNMT
# Google NMT

![w=95%,h=center](gnmt_overview.png)

---
# Google NMT

![w=60%,h=center](gnmt_training.pdf)

---
# Google NMT

![w=80%,h=center](gnmt_rating.png)

---
# Beyond one Language Pair

![w=75%,h=center](../01/image_labeling.pdf)

---
# Beyond one Language Pair

![w=70%,h=center](../01/vqa.pdf)

---
# Multilingual and Unsupervised Translation

Many attempts at multilingual translation.

- Individual encoders and decoders, shared attention.

- Shared encoders and decoders.

~~~
Surprisingly, even unsupervised translation is attempted lately.
By unsupervised we understand settings where we have access to large
monolingual corpora, but no parallel data.

~~~
In 2019, the best unsupervised systems were on par with the best 2014 supervised
systems.

![w=90%,h=center](umt_results.pdf)
