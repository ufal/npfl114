title: NPFL114, Lecture 9
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Word2Vec

## Milan Straka

### April 27, 2020

---
section: Word2vec
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

~~~

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

**Words that are used in the same contexts tend to have similar meanings**.

The distributional hypothesis is usually attributed to Firth (1957):
> _You shall know a word by a company it keeps._

---
# Word2Vec

![w=70%,h=center](word2vec.pdf)

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

---
# Word2Vec

![w=100%](word2vec_composability.pdf)

---
# Word2Vec â€“ SkipGram Model

![w=55%,h=center,mh=70%](word2vec.pdf)

Considering input word $w_i$ and output $w_o$, the Skip-gram model defines
$$p(w_o | w_i) â‰ \frac{e^{â‡‰W_{w_o}^\top â‡‰V_{w_i}}}{âˆ‘_w e^{â‡‰W_w^\top â‡‰V_{w_i}}}.$$

---
# Word2Vec â€“ Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n_1, n_2, \ldots, n_L$, we define
$$p_\textrm{HS}(w | w_i) â‰ âˆ_{j=1}^{L-1} Ïƒ(\textrm{[+1 if }n_{j+1}\textrm{  is right child else -1]} \cdot â‡‰W_{n_j}^\top â‡‰V_{w_i}).$$

---
# Word2Vec â€“ Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

We could also only sample the _negative examples_ instead of training all of
them.

This gives rise to the following _negative sampling_ objective:
$$l_\textrm{NEG}(w_o, w_i) â‰ \log Ïƒ(â‡‰W_{w_o}^\top â‡‰V_{w_i}) + âˆ‘_{j=1}^k ğ”¼_{w_j âˆ¼ P(w)} \log \big(1 - Ïƒ(â‡‰W_{w_j}^\top â‡‰V_{w_i})\big).$$

~~~

For $P(w)$, both uniform and unigram distribution $U(w)$ work, but
$$U(w)^{3/4}$$
outperforms them significantly (this fact has been reported in several papers by
different authors).

---
section: Subword Embeddings
# Recurrent Character-level WEs

![w=80%,h=center](cle_rnn_examples.pdf)

---
# Convolutional Character-level WEs

![w=100%](cle_cnn_examples.pdf)
---
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

~~~
The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept;
~~~
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K âˆ¼ 10^6$ is used).

---
# Charagram WEs

![w=100%,v=middle](cle_charagram_examples.pdf)

---
# Charagram WEs

![w=48%,h=center](cle_charagram_ngrams.pdf)

