title: NPFL114, Lecture 9
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }

# Recurrent Neural Networks III

## Milan Straka

### April 29, 2019

---
section: Refresh
# Recurrent Neural Networks

## Single RNN cell

![w=17%,h=center](../06/rnn_cell.pdf)

## Unrolled RNN cells

![w=60%,h=center](../06/rnn_cell_unrolled.pdf)

---
# Basic RNN Cell

![w=50%,h=center,mh=55%](../06/rnn_cell_basic.pdf)

Given an input $â†’x^{(t)}$ and previous state $â†’s^{(t-1)}$, the new state is computed as
$$â†’s^{(t)} = f(â†’s^{(t-1)}, â†’x^{(t)}; â†’Î¸).$$

One of the simplest possibilities is
$$â†’s^{(t)} = \tanh(â‡‰Uâ†’s^{(t-1)} + â‡‰Vâ†’x^{(t)} + â†’b).$$

---
# Basic RNN Cell

Basic RNN cells suffer a lot from vanishing/exploding gradients (_the challenge
of long-term dependencies_).

If we simplify the recurrence of states to
$$â†’s^{(t)} = â‡‰Uâ†’s^{(t-1)},$$
we get
$$â†’s^{(t)} = â‡‰U^tâ†’s^{(0)}.$$

If $U$ has eigenvalue decomposition of $â‡‰U = â‡‰Q â‡‰Î› â‡‰Q^{-1}$, we get
$$â†’s^{(t)} = â‡‰Q â‡‰Î›^t â‡‰Q^{-1} â†’s^{(0)}.$$
The main problem is that the _same_ function is iteratively applied many times.

Several more complex RNN cell variants have been proposed, which alleviate
this issue to some degree, namely **LSTM** and **GRU**.

---
# Long Short-Term Memory

Later in Gers, Schmidhuber & Cummins (1999) a possibility to _forget_
information from memory cell $â†’c_t$ was added.

![w=40%,f=right](../06/lstm_input_output_forget_gates.pdf)

$$\begin{aligned}
  â†’i_t & â† Ïƒ(â‡‰W^i â†’x_t + â‡‰V^i â†’h_{t-1} + â†’b^i) \\
  â†’f_t & â† Ïƒ(â‡‰W^f â†’x_t + â‡‰V^f â†’h_{t-1} + â†’b^f) \\
  â†’o_t & â† Ïƒ(â‡‰W^o â†’x_t + â‡‰V^o â†’h_{t-1} + â†’b^o) \\
  â†’c_t & â† â†’f_t \cdot â†’c_{t-1} + â†’i_t \cdot \tanh(â‡‰W^y â†’x_t + â‡‰V^y â†’h_{t-1} + â†’b^y) \\
  â†’h_t & â† â†’o_t \cdot \tanh(â†’c_t)
\end{aligned}$$

---
# Long Short-Term Memory
![w=100%,v=middle](../06/LSTM3-SimpleRNN.png)

---
# Gated Recurrent Unit

![w=50%,h=center](../07/gru.pdf)

$$\begin{aligned}
  â†’r_t & â† Ïƒ(â‡‰W^r â†’x_t + â‡‰V^r â†’h_{t-1} + â†’b^r) \\
  â†’u_t & â† Ïƒ(â‡‰W^u â†’x_t + â‡‰V^u â†’h_{t-1} + â†’b^u) \\
  â†’hÌ‚_t & â† \tanh(â‡‰W^h â†’x_t + â‡‰V^h (â†’r_t \cdot â†’h_{t-1}) + â†’b^h) \\
  â†’h_t & â† â†’u_t \cdot â†’h_{t-1} + (1 - â†’u_t) \cdot â†’hÌ‚_t
\end{aligned}$$

---
# Gated Recurrent Unit
![w=100%,v=middle](../07/LSTM3-var-GRU.png)

---
# Word Embeddings

One-hot encoding considers all words to be independent of each other.

However, words are not independent â€“ some are more similar than others.

Ideally, we would like some kind of similarity in the space of the word
representations.

## Distributed Representation
The idea behind distributed representation is that objects can
be represented using a set of common underlying factors.

We therefore represent words as fixed-size _embeddings_ into $â„^d$ space,
with the vector elements playing role of the common underlying factors.

---
# Word Embeddings

The word embedding layer is in fact just a fully connected layer on top of
one-hot encoding. However, it is important that this layer is _shared_ across
the whole network.

~~~

![w=37%](../07/words_onehot.pdf)
~~~
![w=60%](../07/words_embeddings.pdf)

---
# Word Embeddings for Unknown Words

## Recurrent Character-level WEs

![w=40%,h=center](../07/cle_rnn.pdf)

---
# Word Embeddings for Unknown Words

# Convolutional Character-level WEs

![w=49%,h=center](../07/cle_cnn.pdf)

---
# Basic RNN Applications

## Sequence Element Classification

Use outputs for individual elements.

![w=70%,h=center](../06/rnn_cell_unrolled.pdf)

## Sequence Representation

Use state after processing the whole sequence (alternatively, take output of the
last element).

---
# Structured Prediction

Consider generating a sequence of $y_1, \ldots, y_N âˆˆ Y^N$ given input
$â†’x_1, \ldots, â†’x_N$.

Predicting each sequence element independently models the distribution $P(y_i | â‡‰X)$.

However, there may be dependencies among the $y_i$ themselves, which
is difficult to capture by independent element classification.

---
# Linear-Chain Conditional Random Fields (CRF)

Linear-chain Conditional Random Fields, usually abbreviated only to CRF, acts as
an output layer. It can be considered an extension of a softmax â€“ instead of
a sequence of independent softmaxes, CRF is a sentence-level softmax, with
additional weights for neighboring sequence elements.

$$s(â‡‰X, â†’y; â†’Î¸, â‡‰A) = âˆ‘_{i=1}^N \big(â‡‰A_{y_{i-1}, y_i} + f_â†’Î¸(y_i | â‡‰X)\big)$$

$$p(â†’y | â‡‰X) = \softmax_{â†’z âˆˆ Y^N}\big(s(â‡‰X, â†’z)\big)_{â†’z}$$

$$\log p(â†’y | â‡‰X) = s(â‡‰X, â†’y) - \operatorname{logadd}_{â†’z âˆˆ Y^N}(s(â‡‰X, â†’z))$$

---
# Linear-Chain Conditional Random Fields (CRF)

## Computation

We can compute $p(â†’y | â‡‰X)$ efficiently using dynamic programming. If we denote
$Î±_t(k)$ as probability of all sentences with $t$ elements with the last $y$
being $k$.

The core idea is the following:

![w=40%,h=center](crf_composability.pdf)

$$Î±_t(k) = f_â†’Î¸(y_t=k | â‡‰X) + \operatorname{logadd}_{jâˆˆY} (Î±_{t-1}(j) + â‡‰A_{j, k}).$$

For efficient implementation, we use the fact that
$$\ln(a+b) = \ln a + \ln (1 + e^{\ln b - \ln a}).$$


---
# Conditional Random Fields (CRF)

## Decoding

We can perform optimal decoding, by using the same algorithm, only replacing
$\operatorname{logadd}$ with $\max$ and tracking where the maximum was attained.

## Applications

CRF output layers are useful for _span labeling_ tasks, like
- named entity recognition
- dialog slot filling

---
section: CTC
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y_1, \ldots, y_M$ given input
$â†’x_1, \ldots, â†’x_N$, but this time $M â‰¤ N$ and there is no explicit alignment
of $â†’x$ and $y$ in the gold data.

~~~
![w=100%,mh=90%,v=middle](ctc_example.pdf)

---
# Connectionist Temporal Classification

We enlarge the set of output labels by a â€“ (_blank_) and perform a classification for every
input element to produce an _extended labeling_. We then post-process it by the
following rules (denoted $ğ“‘$):
1. We remove neighboring symbols.
2. We remove the â€“.

~~~
Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

~~~
Denoting the probability of label $l$ at time $t$ as $p_l^t$, we define
$$Î±^t(s) â‰ âˆ‘_{\textrm{labeling }â†’Ï€: ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'}.$$

---
# CRF and CTC Comparison

In CRF, we normalize the whole sentences, therefore we need to compute
unnormalized probabilities for all the (exponentially many) sentences. Decoding
can be performed optimally.

~~~
In CTC, we normalize per each label. However, because we do not have explicit
alignment, we compute probability of a labeling by summing probabilities
of (generally exponentially many) extended labelings.

---
# Connectionist Temporal Classification

## Computation

When aligning an extended labeling to a regular one, we need to consider
whether the extended labeling ends by a _blank_ or not. We therefore define
$$\begin{aligned}
  Î±_-^t(s) &â‰ âˆ‘_{\textrm{labeling }â†’Ï€: ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}, Ï€_t=-} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'} \\
  Î±_*^t(s) &â‰ âˆ‘_{\textrm{labeling }â†’Ï€: ğ“‘(â†’Ï€_{1:t}) = â†’y_{1:s}, Ï€_tâ‰ -} âˆ_{t'=1}^t p_{â†’Ï€_{t'}}^{t'}
\end{aligned}$$
and compute $Î±^t(s)$ as $Î±_-^t(s) + Î±_*^t(s)$.

---
# Connectionist Temporal Classification

## Computation

![w=25%,f=right](ctc_computation.pdf)

We initialize $Î±$s as follows:
- $Î±_-^1(0) â† p_-^1$
- $Î±_*^1(1) â† p_{y_1}^1$

~~~
We then proceed recurrently according to:
- $Î±_-^t(s) â† p_-^t (Î±_-^{t-1}(s) + Î±_*^{t-1}(s))$
~~~
- $Î±_*^t(s) â† \begin{cases}
  p_{y_s}^t(Î±_*^{t-1}(s) + Î±_*^{t-1}(s-1) + a_-^{t-1}(s-1))\textrm{, if }y_sâ‰ y_{s-1}\\
  p_{y_s}^t(Î±_*^{t-1}(s) + a_-^{t-1}(s-1))\textrm{, if }y_s=y_{s-1}\\
\end{cases}$

---
# CTC Decoding

Unlike CRF, we cannot perform the decoding optimally. The key
observation is that while an optimal extended labeling can be extended
into an optimal labeling of a larger length, the same does not apply to
regular (non-extended) labeling. The problem is that regular labeling coresponds
to many extended labelings, which are modified each in a different way
during an extension of the regular labeling.

![w=80%,h=center](ctc_decoding.pdf)
---
# CTC Decoding

## Beam Search

To perform beam search, we keep $k$ best regular labelings for each prefix of
the extended labelings. For each regular labeling we keep both $Î±_-$ and
$a_*$ and by _best_ we mean such regular labelings with maximum $Î±_- + Î±_*$.

To compute best regular labelings for longer prefix of extended labelings,
for each regular labeling in the beam we consider the following cases:
- adding a _blank_ symbol, i.e., updating both $Î±_-$ and $Î±_*$;
- adding any non-blank symbol, i.e., updating $Î±_*$.

Finally, we merge the resulting candidates according to their regular labeling and
keep only the $k$ best.

---
section: Word2vec
# Unsupervised Word Embeddings

The embeddings can be trained for each task separately.

~~~

However, a method of precomputing word embeddings have been proposed, based on
_distributional hypothesis_:

**Words that are used in the same contexts tend to have similar meanings**.

The distributional hypothesis is usually attributed to Firth (1957).

---
# Word2Vec

![w=70%,h=center](word2vec.pdf)

Mikolov et al. (2013) proposed two very simple architectures for precomputing
word embeddings, together with a C multi-threaded implementation `word2vec`.

---
# Word2Vec

![w=100%](word2vec_composability.pdf)

---
# Word2Vec â€“ SkipGram Model

![w=55%,h=center,mh=70%](word2vec.pdf)

Considering input word $w_i$ and output $w_o$, the Skip-gram model defines
$$p(w_o | w_i) â‰ \frac{e^{â‡‰W_{w_o}^\top â‡‰V_{w_i}}}{âˆ‘_w e^{â‡‰W_w^\top â‡‰V_{w_i}}}.$$

---
# Word2Vec â€“ Hierarchical Softmax

Instead of a large softmax, we construct a binary tree over the words, with
a sigmoid classifier for each node.

If word $w$ corresponds to a path $n_1, n_2, \ldots, n_L$, we define
$$p_\textrm{HS}(w | w_i) â‰ âˆ_{j=1}^{L-1} Ïƒ(\textrm{[+1 if }n_{j+1}\textrm{  is right child else -1]} \cdot â‡‰W_{n_j}^\top â‡‰V_{w_i}).$$

---
# Word2Vec â€“ Negative Sampling

Instead of a large softmax, we could train individual sigmoids for all words.

We could also only sample the _negative examples_ instead of training all of
them.

This gives rise to the following _negative sampling_ objective:
$$l_\textrm{NEG}(w_o, w_i) â‰ \log Ïƒ(â‡‰W_{w_o}^\top â‡‰V_{w_i}) + âˆ‘_{j=1}^k ğ”¼_{w_j âˆ¼ P(w)} \log \big(1 - Ïƒ(â‡‰W_{w_j}^\top â‡‰V_{w_i})\big).$$

~~~

For $P(w)$, both uniform and unigram distribution $U(w)$ work, but
$$U(w)^{3/4}$$
outperforms them significantly (this fact has been reported in several papers by
different authors).

---
section: Subword Embeddings
# Recurrent Character-level WEs

![w=80%,h=center](cle_rnn_examples.pdf)

---
# Convolutional Character-level WEs

![w=100%](cle_cnn_examples.pdf)
---
# Character N-grams

Another simple idea appeared simultaneously in three nearly simultaneous
publications as [Charagram](https://arxiv.org/abs/1607.02789), [Subword Information](https://arxiv.org/abs/1607.04606) or [SubGram](http://link.springer.com/chapter/10.1007/978-3-319-45510-5_21).

A word embedding is a sum of the word embedding plus embeddings of its character
_n_-grams. Such embedding can be pretrained using same algorithms as `word2vec`.

~~~
The implementation can be
- dictionary based: only some number of frequent character _n_-grams is kept;
~~~
- hash-based: character _n_-grams are hashed into $K$ buckets
  (usually $K âˆ¼ 10^6$ is used).

---
# Charagram WEs

![w=100%,v=middle](cle_charagram_examples.pdf)

---
# Charagram WEs

![w=48%,h=center](cle_charagram_ngrams.pdf)

---
section: Seq2seq
class: middle, center
# Sequence-to-Sequence Architecture

# Sequence-to-Sequence Architecture

---
# Sequence-to-Sequence Architecture

![w=100%,v=middle](seq2seq.pdf)

---
# Sequence-to-Sequence Architecture

![w=45%,h=center](encoder_decoder.pdf)

---
# Sequence-to-Sequence Architecture

## Training

![w=50%,f=right](../06/sequence_prediction_training.pdf)

The so-called _teacher forcing_ is used during training â€“ the gold outputs are
used as inputs during training.

~~~
## Inference

![w=50%,f=right](../06/sequence_prediction_inference.pdf)

During inference, the network processes its own predictions.

Usually, the generated logits are processed by an $\argmax$, the chosen word
embedded and used as next input.

---
# Tying Word Embeddings

![w=26%,h=center](tying_embeddings.pdf)

---
section: Attention
# Attention

![w=35%,f=right](attention.pdf)

As another input during decoding, we add _context vector_ $c_i$:
$$â†’s_i = f(â†’s_{i-1}, â†’y_{i-1}, â†’c_i).$$

~~~
We compute the context vector as a weighted combination of source sentence
encoded outputs:
$$â†’c_i = âˆ‘_j Î±_{ij} â†’h_j$$

~~~
The weights $Î±_{ij}$ are softmax of $e_{ij}$ over $j$,
$$â†’Î±_i = \softmax(â†’e_i),$$
with $e_{ij}$ being
$$e_{ij} = â†’v^\top \tanh(â‡‰Vâ†’h_j + â‡‰Wâ†’s_{i-1} + â†’b) .$$

---
# Attention

![w=45%,h=center](attention_visualization.pdf)

---
section: NMT
# Subword Units

Translate _subword units_ instead of words. The subword units can be generated
in several ways, the most commonly used are

~~~
- BPE â€“ Using the _byte pair encoding_ algorithm. Start with characters plus
  a special end-of-word symbol $\cdot$. Then, merge the most occurring symbol
  pair $A, B$ by a new symbol $AB$, with the symbol pair never crossing word
  boundary.

~~~
  Considering a dictionary with words _low, lowest, newer, wider_:
  $$\begin{aligned}
    r \,\,\, \cdot & â†’ r\cdot \\
    l \,\,\, o & â†’ lo \\
    lo \,\,\, w & â†’ low \\
    e \,\,\, r\cdot & â†’ er\cdot \\
  \end{aligned}$$

~~~
- Wordpieces â€“ Joining neighboring symbols to maximize unigram language
  model likelihood.

~~~
Usually quite little subword units are used (32k-64k), often generated on the
union of the two vocabularies (the so-called _joint BPE_ or _shared
wordpieces_).

---
# Google NMT

![w=95%,h=center](gnmt_overview.png)

---
# Google NMT

![w=60%,h=center](gnmt_training.pdf)

---
# Google NMT

![w=80%,h=center](gnmt_rating.png)

---
# Beyond one Language Pair

![w=75%,h=center](../01/image_labeling.pdf)

---
# Beyond one Language Pair

![w=70%,h=center](../01/vqa.pdf)

---
# Multilingual Translation

Many attempts at multilingual translation.

- Individual encoders and decoders, shared attention.

- Shared encoders and decoders.

---
section: Transformer
# Attention is All You Need

![w=55%,h=center](transformer.pdf)

---
# Attention is All You Need

The attention module is defined as:

$$\textrm{Attention}(â‡‰Q, â‡‰K, â‡‰V) = \softmax\left(\frac{â‡‰Q â‡‰K^\top}{\sqrt{d_k}}\right)â‡‰V.$$

~~~
Multihead attention is used in practice.

![w=60%,h=center](transformer_multihead.pdf)

---
# Attention is All You Need

## Positional Embeddings

We need to encode positional information (which was implicit in RNNs).

~~~
- Learned embeddings for every position.

~~~
- Sinusoids of different frequencies:
  $$\begin{aligned}
    \textrm{PE}_{(\textit{pos}, 2i)} & = \sin\left(\textit{pos} / 10000^{2i/d}\right) \\
    \textrm{PE}_{(\textit{pos}, 2i + 1)} & = \cos\left(\textit{pos} / 10000^{2i/d}\right)
  \end{aligned}$$

  This choice of functions should allow the model to attend to relative
  positions, since for any fixed $k$, $\textrm{PE}_{\textit{pos} + k}$ is
  a linear function of $\textrm{PE}_\textit{pos}$.

---
# Why Attention

![w=100%,v=middle](transformer_attentions.pdf)

---
# Transformers Results

![w=100%,v=middle](transformer_results.pdf)
