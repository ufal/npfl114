title: NPFL114, Lecture 12
class: title, langtech, cc-by-nc-sa

# NASNet, Speech Synthesis,<br>External Memory Networks

## Milan Straka

### May 18, 2020

---
section: NASNet
# Neural Architecture Search (NASNet) â€“ 2017

- We can design neural network architectures using reinforcement learning.

~~~
- The designed network is encoded as a sequence of elements, and is generated
  using an _RNN controller_, which is trained using the REINFORCE with baseline
  algorithm.

![w=55%,h=center](nasnet_overview.pdf)

~~~
- For every generated sequence, the corresponding network is trained on CIFAR-10
  and the development accuracy is used as a return.

---
# Neural Architecture Search (NASNet) â€“ 2017

The overall architecture of the designed network is fixed and only the Normal
Cells and Reduction Cells are generated by the controller.

![w=29%,h=center](nasnet_overall.pdf)

---
# Neural Architecture Search (NASNet) â€“ 2017

- Each cell is composed of $B$ blocks ($B=5$ is used in NASNet).
~~~
- Each block is designed by a RNN controller generating 5 parameters.

![w=80%,h=center](nasnet_rnn_controller.pdf)

![w=60%,mw=50%,h=center](nasnet_block_steps.pdf)![w=80%,mw=50%,h=center](nasnet_operations.pdf)

- Every block is designed by a RNN controller generating individual operations.

---
# Neural Architecture Search (NASNet) â€“ 2017

The final proposed Normal Cell and Reduction Cell:

![w=77%,h=center](nasnet_blocks.pdf)

---
# EfficientNet Search

EfficientNet changes the search in two ways.

~~~
- Computational requirements are part of the return. Notably, the goal is to
  find an architecture $m$ maximizing
  $$\operatorname{DevelopmentAccuracy}(m) â‹… \left(\frac{\textrm{TargetFLOPS=400M}}{\operatorname{FLOPS}(m)}\right)^{0.07}$$
  where the constant $0.07$ balances the accuracy and FLOPS.

~~~
- Using a different search space, which allows to control kernel sizes and
  channels in different parts of the overall architecture (compared to using the
  same cell everywhere as in NASNet).

---
# EfficientNet Search

![w=100%](mnasnet_overall.pdf)

![w=30%,f=right](mnasnet_parameters.pdf)

The overall architecture consists of 7 blocks, each described by 6 parameters
â€“ 42 parameters in total, compared to 50 parameters of NASNet search space.

---
# EfficientNet-B0 Baseline Network

![w=100%](../05/efficientnet_architecture.pdf)

---
section: WaveNet
# WaveNet

Our goal is to model speech, using a auto-regressive model
$$P(â†’x) = âˆ_t P(x_t | x_{t-1}, â€¦, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.pdf)

---
# WaveNet

![w=100%,v=middle](wavenet_dilated_convolutions.pdf)

---
# WaveNet

## Output Distribution
The raw audio is usually stored in 16-bit samples. However, classification
into $65\,536$ classes would not be tractable, and instead WaveNet adopts
$Î¼$-law transformation and quantize the samples into 256 values using
$$\operatorname{sign}(x)\frac{\ln(1 + 255|x|)}{\ln(1 + 255)}.$$

~~~
## Gated Activation
To allow greater flexibility, the outputs of the dilated convolutions are passed
through the gated activation units
$$â†’z = \tanh(â‡‰W_f * â†’x) â‹… Ïƒ(â‡‰W_g * â†’x).$$

---
# WaveNet

![w=80%,h=center](wavenet_block.pdf)

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $â†’h$,
changing the gated activation function to
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_fâ†’h) â‹… Ïƒ(â‡‰W_g * â†’x + â‡‰V_gâ†’h).$$

~~~
## Local Conditioning
For local conditioning, we are given a time series $h_t$, possibly with a lower
sampling frequency. We first use transposed convolutions $â†’y = f(â†’h)$ to match resolution
and then compute analogously to global conditioning
$$â†’z = \tanh(â‡‰W_f * â†’x + â‡‰V_f * â†’y) â‹… Ïƒ(â‡‰W_g * â†’x + â‡‰V_g * â†’y).$$

---
# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 3
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1Ã—1$ output convolution produces 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of $0.0002$

---
# WaveNet

![w=85%,h=center](wavenet_results.pdf)

---
section: ParallelWaveNet
# Parallel WaveNet

The output distribution was changed from 256 $Î¼$-law values to a Mixture of
Logistic (suggested in another paper â€“ PixelCNN++, but reused in other architectures since):
$$Î½ âˆ¼ âˆ‘_i Ï€_i \operatorname{logistic}(Î¼_i, s_i).$$

~~~
The logistic distribution is a distribution with a $Ïƒ$ as cumulative density function
(where the mean and scale is parametrized by $Î¼$ and $s$). Therefore, we can
write
$$P(x | â†’Ï€, â†’Î¼, â†’s) = âˆ‘_i Ï€_i \big[Ïƒ((x + 0.5 - Î¼_i) / s_i) - Ïƒ((x - 0.5 - Î¼_i) / s_i)\big],$$
where we replace -0.5 and 0.5 in the edge cases by $-âˆ$ and $âˆ$.

~~~
In Parallel WaveNet, 10 mixture components are used.

---
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we use a following trick. We will model $P(x_t)$ as $P(x_t | â†’z_{<t})$
for a _random_ $â†’z$ drawn from a logistic distribution. Then, we compute
$$x^1_t = z_t â‹… s^1(â†’z_{< t}) + Î¼^1(â†’z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
â€“ 4 iterations were used by the authors. In further iterations,
$$x^i_t = x^{i-1}_t â‹… s^i(â†’x^{i-1}_{< t}) + Î¼^i(â†’x^{i-1}_{< t}).$$

~~~
After $N$ iterations, $P(â†’x^N_t | â†’z_{â‰¤t})$ is a logistic distribution
with location $â†’Î¼_\textrm{tot}$ and scale $â†’s_\textrm{tot}$ with
$$â†’Î¼_\textrm{tot} = âˆ‘_i^N Î¼^i(â†’x^{i-1}_{< t}) \cdot \left(âˆ\nolimits_{j>i}^N s^j(â†’x^{j-1}_{< t})\right)
\textrm{~~and~~}
â†’s_\textrm{tot} = âˆ‘_i^N s^i(â†’x^{i-1}_{< t}).$$


---
# Parallel WaveNet

The network is trained using a _probability density distillation_ using
a teacher WaveNet, using KL-divergence as loss.

![w=75%,h=center](parallel_wavenet_distillation.pdf)

---
# Parallel WaveNet

Denoting the teacher distribution as $P_T$ and the student distribution
as $P_S$, the loss is specifically
$$D_\textrm{KL}(P_S || P_T) = H(P_S, P_T) - H(P_S).$$

~~~
Therefore, we do not only minimize cross-entropy, but we also try to keep the
entropy of the student as high as possible. That is crucial not to match
just the mode of the teacher. (Consider a teacher generating white noise,
where every sample comes from $ğ“(0, 1)$ â€“ in this case, the cross-entropy
loss of a constant $â†’0$, complete silence, would be maximal.)

~~~
In a sense, probability density distillation is similar to GANs. However,
the teacher is kept fixed and the student does not attempt to fool it
but to match its distribution instead.

---
# Parallel WaveNet

With the 4 iterations, the Parallel WaveNet generates over 500k samples per
second, compared to ~170 samples per second of a regular WaveNet â€“ more than
a 1000 times speedup.

![w=80%,mh=80%,v=bottom,h=center](parallel_wavenet_results.pdf)

---
section: Tacotron
# Tacotron

![w=65%,h=center](tacotron.pdf)

---
# Tacotron

![w=65%,h=center,v=middle](tacotron_results.pdf)

---
# Tacotron

![w=100%,v=middle](tacotron_comparison.pdf)

---
section: NTM
# Neural Turing Machines

So far, all input information was stored either directly in network weights, or
in a state of a recurrent network.

~~~
However, mammal brains seem to operate with a _working memory_ â€“ a capacity for
short-term storage of information and its rule-based manipulation.

~~~
We can therefore try to introduce an external memory to a neural network. The
memory $â‡‰M$ will be a matrix, where rows correspond to memory cells.

---
# Neural Turing Machines

The network will control the memory using a controller which reads from the
memory and writes to is. Although the original paper also considered
a feed-forward (non-recurrent) controller, usually the controller is a recurrent
LSTM network.

![w=55%,h=center](ntm_architecture.pdf)

---
# Neural Turing Machine

## Reading

To read the memory in a differentiable way, the controller at time $t$ emits
a read distribution $â†’w_t$ over memory locations, and the returned read vector $â†’r_t$
is then
$$â†’r_t = âˆ‘_i w_t(i) â‹… â†’M_t(i).$$

## Writing

Writing is performed in two steps â€“ an _erase_ followed by an _add_. The
controller at time $t$ emits a write distribution $â†’w_t$ over memory locations,
and also an _erase vector_ $â†’e_t$ and an _add vector_ $â†’a_t$. The memory is then
updates as
$$â†’M_t(i) = â†’M_{t-1}(i)\big[1 - w_t(i)â†’e_t] + w_t(i) â†’a_t.$$

---
# Neural Turing Machine

The addressing mechanism is designed to allow both
- content addressing, and
- location addressing.

![w=90%,h=center](ntm_addressing.pdf)

---
# Neural Turing Machine

## Content Addressing

Content addressing starts by the controller emitting the _key vector_ $â†’k_t$,
which is compared to all memory locations $â†’M_t(i)$, generating a distribution
using a $\softmax$ with temperature $Î²_t$.
$$w_t^c(i) = \frac{\exp(Î²_t â‹… \operatorname{distance}(â†’k_t, â†’M_t(i))}{âˆ‘_j \exp(Î²_t â‹… \operatorname{distance}(â†’k_t, â†’M_t(j))}$$

The $\operatorname{distance}$ measure is usually the cosine similarity
$$\operatorname{distance}(â†’a, â†’b) = \frac{â†’a â‹… â†’b}{||â†’a|| â‹… ||â†’b||}.$$

---
# Neural Turing Machine

## Location-Based Addressing

To allow iterative access to memory, the controller might decide to reuse the
memory location from the previous timestep. Specifically, the controller emits
_interpolation gate_ $g_t$ and defines
$$â†’w_t^g = g_t â†’w_t^c + (1 - g_t) â†’w_{t-1}.$$

Then, the current weighting may be shifted, i.e., the controller might decide to
â€œrotateâ€ the weights by a small integer. For a given range (the simplest case
are only shifts $\{-1, 0, 1\}$), the network emits $\softmax$ distribution over
the shifts, and the weights are then defined using a circular convolution
$$wÌƒ_t(i) = âˆ‘_j w_t^g(j) s_t(i - j).$$

Finally, not to lose precision over time, the controller emits
a _sharpening factor_ $Î³_t$ and the final memory location weights are
$w_t(i) = {wÌƒ_t(i)^{Î³_t}} / {âˆ‘_j wÌƒ_t(j)^{Î³_t}}.$

---
# Neural Turing Machine

## Overall Execution

Even if not specified in the original paper, following the DNC paper, the LSTM
controller can be implemented as a (potentially deep) LSTM. Assuming $R$ read
heads and one write head, the input is $â†’x_t$ and $R$ read
vectors $â†’r_{t-1}^1, â€¦, â†’r_{t-1}^R$ from the previous time step, the output of the
controller are vectors $(â†’Î½_t, â†’Î¾_t)$, and the final output is
$â†’y_t = â†’Î½_t + W_r\big[â†’r_t^1, â€¦, â†’r_t^R\big]$. The $â†’Î¾_t$ is a concatenation of
$$â†’k_t^1, Î²_t^1, g_t^1, â†’s_t^1, Î³_t^1, â†’k_t^2, Î²_t^2, g_t^2, â†’s_t^2, Î³_t^2, â€¦, â†’k_t^w, Î²_t^w, g_t^w, â†’s_t^w, Î³_t^w, â†’e_t^w, â†’a_t^w.$$

---
# Neural Turing Machines

## Copy Task

Repeat the same sequence as given on input. Trained with sequences of length up
to 20.

![w=70%,h=center](ntm_copy_training.pdf)

---
# Neural Turing Machines

![w=84%,h=center](ntm_copy_generalization.pdf)

---
# Neural Turing Machines

![w=95%,h=center](ntm_copy_generalization_lstm.pdf)

---
# Neural Turing Machines

![w=65%,h=center](ntm_copy_memory.pdf)

---
# Neural Turing Machines

## Associative Recall

In associative recall, a sequence is given on input, consisting of subsequences
of length 3. Then a randomly chosen subsequence is presented on input and the
goal is to produce the following subsequence.

![w=65%,h=center](ntm_associative_recall_training.pdf)

---
# Neural Turing Machines

![w=83%,h=center](ntm_associative_recall_generalization.pdf)

---
# Neural Turing Machines

![w=53%,h=center](ntm_associative_recall_memory.pdf)

---
section: DNC
# Differentiable Neural Computer

NTM was later extended to a Differentiable Neural Computer.

![w=82%,h=center](dnc_architecture.pdf)

---
# Differentiable Neural Computer

The DNC contains multiple read heads and one write head.

~~~
The controller is a deep LSTM network, with input at time $t$ being the current
input $â†’x_t$ and $R$ read vectors $â†’r_{t-1}^1, â€¦, â†’r_{t-1}^R$ from previous time
step. The output of the controller are vectors $(â†’Î½_t, â†’Î¾_t)$, and the final
output is $â†’y_t = â†’Î½_t + W_r\big[â†’r_t^1, â€¦, â†’r_t^R\big]$. The $â†’Î¾_t$ is
a concatenation of parameters for read and write heads (keys, gates, sharpening
parameters, â€¦).

~~~
In DNC, the usage of every memory location is tracked, which enables performing
dynamic allocation â€“ at each time step, a cell with least usage can be allocated.

~~~
Furthermore, for every memory location, we track which memory location
was written to previously ($â†’b_t$) and subsequently ($â†’f_t$), allowing
to recover sequences in the order in which it was written, independently on the
real indices used.

~~~
The write weighting is defined as a weighted combination of the allocation
weighting and write content weighting, and read weighting is computed as a weighted
combination of read content weighting, previous write weighting, and subsequent
write weighting.


---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks.pdf)

---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks_traversal.pdf)

---
section: MANN
# Memory-augmented Neural Networks

External memory can be also utilized for _learning to learn_. Consider a network,
which should learn classification into a user-defined hierarchy by observing
ideally a small number of samples.

~~~
Apart from finetuning the model and storing the information in the _weights_,
an alternative is to store the samples in _external memory_. Therefore, the
model learns how to store the data and access it efficiently, which allows
it to learn without changing its weights.

![w=100%](mann_overview.pdf)

---
# Memory-augmented NNs

![w=90%,mw=62%](mann_reading.pdf)![w=38%](mann_writing.pdf)

---
# Memory-augmented NNs

![w=60%,h=center](mann_results.pdf)
