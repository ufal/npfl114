title: NPFL114, Lecture 12
class: title, langtech, cc-by-sa

# Deep Reinforcement Learning, VAE

## Milan Straka

### May 2, 2023

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# Reinforcement Learning

**Reinforcement learning** is a machine learning paradigm, different from
_supervised_ and _unsupervised learning_.

~~~
The essence of reinforcement learning is to learn from _interactions_ with the
environment to maximize a numeric _reward_ signal.
~~~
The learner is not told which actions to take, and the actions may affect not
just the immediate reward, but also all following rewards.

~~~
![w=50%,h=center](robots.png)

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s â€“ Richard Bellman

~~~
- Trial and error learning â€“ since 1850s
  - Law and effect â€“ Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, â€¦ â€“ 1950s and 1960s
  - Tsetlin, Holland, Klopf â€“ 1970s
  - Sutton, Barto â€“ since 1980s

---
# Reinforcement Learning Successes

![w=23%,f=right](atari_games.png)

- Human-level video game playing (_DQN_) â€“ 2013 (2015 Nature), Mnih. et al, Deepmind.

~~~
  - After 7 years of development, the _Agent57_ beats humans on all 57
    AtariÂ 2600 games, achieving a mean score of 4766% compared to human players.

~~~

- _AlphaGo_ beat 9-dan professional player Lee Sedol in Go in Mar 2016.
~~~
  - After two years of development, _AlphaZero_ achieved best performance
    in Go, chess, shogi, being trained using self-play only.
  ![w=38%,h=center](a0_results.svgz)

~~~
- Impressive performance in Dota2, Capture the flag FPS, StarCraft II, â€¦

---
# Reinforcement Learning Successes

- Neural Architecture Search â€“ since 2017

  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
  - _NasNet_, _EfficientNet_, _EfficientNetV2_, â€¦
~~~
  - AutoML: automatically discovering
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - â€¦

~~~
- Optimize nondifferentiable loss
~~~
  - improved translation quality in 2016

~~~
- Discovering discrete latent structures
~~~
- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction
~~~
- Reinforcement learning from human feedback (_RLHF_) is used during
  ChatGPT training.

---
section: MABandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, â€¦

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ğ”¼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) â‰ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t â‰ \argmax_a Q_t(a).$$

---
# $Îµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$Îµ$-greedy_ method follows the greedy action with probability $1-Îµ$, and
chooses a uniformly random action with probability $Îµ$.

---
# $Îµ$-greedy Method

![w=52%,h=center,v=middle](e_greedy.svgz)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,
where:
- $ğ“¢$ is a set of states,
~~~
- $ğ“$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a **reward** $r âˆˆ â„$,
~~~
- $Î³ âˆˆ [0, 1]$ is a **discount factor** (we always use $Î³=1$ and finite episodes in this course).

~~~
Let a **return** $G_t$ be $G_t â‰ âˆ‘_{k=0}^âˆ Î³^k R_{t + 1 + k}$. The goal is to optimize $ğ”¼[G_0]$.

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $Î³=1$,
because the return $G â‰ âˆ‘_{t=0}^H Î³^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Î³$ needs
to be sharply smaller than 1.


---
# Policy

A **policy** $Ï€$ computes a distribution of actions in a given state, i.e.,
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
We will model a policy using a neural network with parameters $â†’Î¸$:
$$Ï€(a | s; â†’Î¸).$$

~~~
If the number of actions is finite, we consider the policy to be a categorical
distribution and utilize the $\softmax$ output activation as in supervised
classification.

---
# (State-)Value and Action-Value Functions

To evaluate a quality of a policy, we define **value function** $v_Ï€(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_Ï€(s) & â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s\right] \\
         & = ğ”¼_{A_t âˆ¼ Ï€(s)} ğ”¼_{S_{t+1},R_{t+1} âˆ¼ p(s,A_t)} \big[R_{t+1}
           + Î³ ğ”¼_{A_{t+1} âˆ¼ Ï€(S_{t+1})} ğ”¼_{S_{t+2},R_{t+2} âˆ¼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + â€¦ \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $Ï€$ is defined analogously as
$$q_Ï€(s, a) â‰ ğ”¼_Ï€\left[G_t \middle| S_t = s, A_t = a\right] = ğ”¼_Ï€\left[âˆ‘\nolimits_{k=0}^âˆ Î³^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and the state-value function can be easily expressed using one another:
$$\begin{aligned}
  v_Ï€(s) &= ğ”¼_{aâˆ¼Ï€}\big[q_Ï€(s, a)\big], \\
  q_Ï€(s, a) &= ğ”¼_{s', r âˆ¼ p}\big[r + Î³v_Ï€(s')\big]. \\
\end{aligned}$$

---
# Optimal Value Functions

**Optimal state-value function** is defined as
$$v_*(s) â‰ \max_Ï€ v_Ï€(s),$$
~~~
and **optimal action-value function** is defined analogously as
$$q_*(s, a) â‰ \max_Ï€ q_Ï€(s, a).$$

~~~
Any policy $Ï€_*$ with $v_{Ï€_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $Ï€_*(s) â‰ \argmax_a q_*(s, a) = \argmax_a ğ”¼[R_{t+1} + Î³v_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $Î³ < 1$, there always exists a unique optimal
state-value function, aÂ unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: REINFORCE
# Policy Gradient Methods

We train the policy
$$Ï€(a | s; â†’Î¸)$$
by maximizing the expected return $v_Ï€(s)$.

~~~
To that account, we need to compute its **gradient** $âˆ‡_{â†’Î¸} v_Ï€(s)$.

---
# Policy Gradient Theorem

Assume that $ğ“¢$ and $ğ“$ are finite, $Î³=1$, and that maximum episode length $H$ is also finite.

Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.
Let also $J(â†’Î¸) â‰ ğ”¼_{sâˆ¼h} v_Ï€(s)$.

~~~
Then
$$âˆ‡_{â†’Î¸} v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸)$$
and
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸),$$

~~~
where $P(s â†’ â€¦ â†’ s'|Ï€)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, â€¦ steps.


---
# Proof of Policy Gradient Theorem

$\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \Big[ âˆ‘\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ q_Ï€(s, a) âˆ‡ Ï€(a|s; â†’Î¸) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ q_Ï€(s, a) âˆ‡ Ï€(a|s; â†’Î¸) + Ï€(a|s; â†’Î¸) âˆ‡ \big(âˆ‘\nolimits_{s', r} p(s', r|s, a)(r + v_Ï€(s'))\big) \Big]$

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ q_Ï€(s, a) âˆ‡ Ï€(a|s; â†’Î¸) + Ï€(a|s; â†’Î¸) \big(âˆ‘\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\big) \Big]$

~~~
_We now expand $v_Ï€(s')$._

~~~
$\displaystyle \phantom{âˆ‡v_Ï€(s)} = âˆ‘\nolimits_a \Big[ q_Ï€(s, a) âˆ‡ Ï€(a|s; â†’Î¸) + Ï€(a|s; â†’Î¸) \Big(âˆ‘\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad âˆ‘\nolimits_{a'} \Big[ q_Ï€(s', a') âˆ‡ Ï€(a'|s'; â†’Î¸) + Ï€(a'|s'; â†’Î¸) \big(âˆ‘\nolimits_{s''} p(s''|s', a') âˆ‡ v_Ï€(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_Ï€(s'')$, we obtain the following:_

$\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} âˆ‘_{k=0}^H P(s â†’ s'\textrm{~in~}k\textrm{~steps~}|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, it is enough to realize that
$$âˆ‘\nolimits_{k=0}^H P(s â†’ s'\textrm{~in~}k\textrm{~steps~}|Ï€) âˆ P(s â†’ â€¦ â†’ s'|Ï€).$$

~~~
For the second part, we know that
$$âˆ‡_{â†’Î¸} J(â†’Î¸) = ğ”¼_{s âˆ¼ h} âˆ‡_{â†’Î¸} v_Ï€(s) âˆ ğ”¼_{s âˆ¼ h} âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_{â†’Î¸} Ï€(a | s'; â†’Î¸),$$
~~~
therefore using the fact that $Î¼(s') = ğ”¼_{s âˆ¼ h} P(s â†’ â€¦ â†’ s'|Ï€)$ we get
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
Finally, note that the theorem can be proven with infinite $ğ“¢$ and $ğ“$; and
also for infinite episodes when discount factor $Î³<1$.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(â†’Î¸) â‰ -ğ”¼_{sâˆ¼h} v_Ï€(s)$. The loss gradient is then
$$âˆ‡_{â†’Î¸} -J(â†’Î¸) âˆ -âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = -ğ”¼_{s âˆ¼ Î¼} âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$âˆ‡_{â†’Î¸} -J(â†’Î¸) âˆ ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_{â†’Î¸} -\log Ï€(a | s; â†’Î¸),$$
where we used the fact that
$$âˆ‡_{â†’Î¸} \log Ï€(a | s; â†’Î¸) = \frac{1}{Ï€(a | s; â†’Î¸)} âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss $-J(â†’Î¸)$ with gradient
$$ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_{â†’Î¸} -\log Ï€(a | s; â†’Î¸),$$
where we estimate the $q_Ï€(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
# REINFORCE Algorithm Example Performance

![w=30%,v=middle](stochastic_policy_example.svgz)![w=69%,v=middle](reinforce_performance.svgz)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary â€“ better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$âˆ‡_{â†’Î¸} J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \big(q_Ï€(s, a) - b(s)\big) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$âˆ‘_a b(s) âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = b(s) âˆ‘_a âˆ‡_{â†’Î¸} Ï€(a | s; â†’Î¸) = b(s) âˆ‡_{â†’Î¸} âˆ‘_a Ï€(a | s; â†’Î¸) = b(s) âˆ‡_{â†’Î¸} 1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize the
variance of the gradient estimator. Such baseline reminds centering of the
returns, given that
$$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_Ï€(s, a) - v_Ï€(s)$ function is also called the **advantage** function
$$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s).$$

~~~
Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks
are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline Example Performance


![w=40%,h=center,mh=48%](stochastic_policy_example.svgz)

![w=48%](reinforce_performance.svgz)![w=52%](reinforce_with_baseline_comparison.svgz)

---
section: NAS
# Neural Architecture Search (NASNet) â€“ 2017

- We can design neural network architectures using reinforcement learning.

~~~
- The designed network is encoded as a sequence of elements, and is generated
  using an **RNN controller**, which is trained using the REINFORCE with baseline
  algorithm.

![w=55%,h=center](nasnet_overview.svgz)

~~~
- For every generated sequence, the corresponding network is trained on CIFAR-10
  and the development accuracy is used as a return.

---
# Neural Architecture Search (NASNet) â€“ 2017

The overall architecture of the designed network is fixed and only the Normal
Cells and Reduction Cells are generated by the controller.

![w=29%,h=center](nasnet_overall.svgz)

---
# Neural Architecture Search (NASNet) â€“ 2017

- Each cell is composed of $B$ blocks ($B=5$ is used in NASNet).
~~~
- Each block is designed by a RNN controller generating 5 parameters.

![w=80%,h=center](nasnet_rnn_controller.svgz)

![w=60%,mw=50%,h=center](nasnet_block_steps.svgz)![w=80%,mw=50%,h=center](nasnet_operations.svgz)

- Every block is designed by a RNN controller generating individual operations.

---
# Neural Architecture Search (NASNet) â€“ 2017

The final Normal Cell and Reduction Cell chosen from 20k architectures
(500GPUs, 4days).

![w=77%,h=center](nasnet_blocks.svgz)

---
# EfficientNet Search

EfficientNet changes the search in three ways.

~~~
- Computational requirements are part of the return. Notably, the goal is to
  find an architecture $m$ maximizing
  $$\operatorname{DevelopmentAccuracy}(m) â‹… \left(\frac{\textrm{TargetFLOPS=400M}}{\operatorname{FLOPS}(m)}\right)^{0.07},$$
~~~
  where the constant $0.07$ balances the accuracy and FLOPS (_the constant comes
  from an empirical observation that doubling the FLOPS brings about 5% relative
  accuracy gain, and $1.05 = 2^Î²$_ gives $Î² â‰ˆ 0.0704$).

~~~
- It uses a different search space allowing to control kernel sizes and
  channels in different parts of the architecture (compared to using the same
  cell everywhere as in NASNet).

~~~
- Training directly on ImageNet, but only for 5 epochs.

~~~
In total, 8k model architectures are sampled, and PPO algorithm is used
instead of the REINFORCE with baseline.

---
# EfficientNet Search

![w=100%](mnasnet_overall.svgz)

![w=30%,f=right](mnasnet_parameters.svgz)

The overall architecture consists of 7 blocks, each described by 6 parameters
â€“ 42 parameters in total, compared to 50 parameters of the NASNet search space.

---
# EfficientNet-B0 Baseline Network

![w=100%](../05/efficientnet_architecture.svgz)
---
section: RLWhatNext
# What Next

If you liked the introduction to the deep reinforcement learning, I have
a whole course **NPFL122 â€“ Deep Reinforcement Learning**.

~~~
- It covers a range of reinforcement learning algorithms, from the basic
  ones to more advanced algorithms utilizing deep neural networks.

~~~
- Previously it was in winter semester, but it will be in the summer semester
  starting from the next year.

~~~
- This year it was 2/2 C+Ex, but I want to lengthen it to 3/2 C+Ex, like
  the Deep learning.

~~~
- An elective (povinnÄ› volitelnÃ½) course in the programs:
  - Artificial Intelligence,
  - Language Technologies and Computational Linguistics.

---
section: GenerativeModels
class: center, middle
# Generative Models

# Generative Models

---
# Generative Models

![w=76%,h=center](stable_diffusion.jpg)

---
# Generative Models

![w=50%](hands_v4.jpg)
~~~
![w=47%](hands_v5.jpg)

---
# Generative Models

Generative models are given a set of realizations of a random variable $â‡â†’x$ and
their goal is to estimate $P(â†’x)$.

~~~
Usually the goal is to be able to sample from $P(â‡â†’x)$, but sometimes an
explicit calculation of $P(â†’x)$ is also possible.

---
# Deep Generative Models

![w=25%,h=center](generative_model.svgz)

One possible approach to estimate $P(â†’x)$ is to assume that the random variable
$â‡â†’x$ depends on a **latent variable** $â‡â†’z$:
$$P(â†’x) = âˆ‘_{â†’z} P(â†’z) P(â†’x | â†’z) = ğ”¼_{â†’z âˆ¼ P(â‡â†’z)} P(â†’x | â†’z).$$

~~~
We use neural networks to estimate the conditional probability
$P_{â†’Î¸}(â†’x | â†’z)$.

---
# AutoEncoders

![w=50%,h=center](ae.svgz)

- Autoencoders are useful for unsupervised feature extraction, especially when
  performing input compression (i.e., when the dimensionality of the latent
  space $â†’z$ is smaller than the dimensionality of the input).

~~~
- When $â†’x + â†’Îµ$ is used as input, autoencoders can perform denoising.

~~~
- However, the latent space $â†’z$ does not need to be fully covered, so
  a randomly chosen $â†’z$ does not need to produce a valid $â†’x$.

---
# AutoEncoders

![w=100%,v=middle](ae_latent_space.png)

---
section: VAE
# Variational AutoEncoders

We assume $P(â‡â†’z)$ is fixed and independent on $â‡â†’x$.

We approximate $P(â†’x | â†’z)$ using $P_{â†’Î¸}(â†’x | â†’z)$. However, in order
to train an autoencoder, we need to know the posterior $P_{â†’Î¸}(â†’z | â†’x)$, which is usually
intractable.

~~~
We therefore approximate $P_{â†’Î¸}(â†’z | â†’x)$ by a trainable $Q_{â†’Ï†}(â†’z | â†’x)$.

---
style: .katex-display { margin: .65em 0 }
# Jensen's Inequality

To derive a loss for training variational autoencoders, we first formulate
the Jensen's inequality.

~~~
![w=95%,mw=37%,h=right,f=right](../02/convex_2d.svgz)

Recall that convex functions by definition fulfil that for $â†’u, â†’v$ and real $0
â‰¤ t â‰¤ 1$,
$$f(tâ†’u + (1-t)â†’v) â‰¤ tf(â†’u) + (1-t)f(â†’v).$$

~~~
The **Jensen's inequality** generalizes the above property to any _convex_
combination of points: if we have $â†’u_i âˆˆ â„^D$ and weights $w_i âˆˆ â„^+$ such
that $âˆ‘_i w_i = 1$, it holds that

![w=95%,mw=37%,h=right,f=right](jensens_inequality.png)

$$f\big(âˆ‘_i w_i â†’u_i\big) â‰¤ âˆ‘_i w_i f\big(â†’u_i\big).$$

~~~
The Jensen's inequality can be formulated also for probability distributions
(whose expectation can be considered an infinite convex combination):

$$f\big(ğ”¼[â‡â†’u]\big) â‰¤ ğ”¼_{â‡â†’u} \big[f(â‡â†’u)\big].$$

---
# VAE â€“ Loss Function Derivation

Our goal will be to maximize the log-likelihood as usual, but we need to express
it using the latent variable $â†’z$:
$$\log P_{â†’Î¸}(â†’x) = \log ğ”¼_{P(â†’z)} \big[P_{â†’Î¸}(â†’x | â†’z)\big].$$

~~~
However, approximating the expectation using a single sample has monstrous
variance, because for most $â†’z$, $P_{â†’Î¸}(â†’x | â†’z)$ will be nearly zero.

~~~
We therefore turn to our _encoder_, which is able for a given $â†’x$ to generate
â€œitsâ€ $â†’z$:

~~~
$\displaystyle \kern10em\mathllap{\log P_{â†’Î¸}(â†’x)} = \log ğ”¼_{P(â†’z)} \big[P_{â†’Î¸}(â†’x | â†’z)\big]$

$\displaystyle \kern10em{} = \log ğ”¼_{Q_{â†’Ï†}(â†’z|â†’x)} \bigg[P_{â†’Î¸}(â†’x | â†’z) â‹… \frac{P(â†’z)}{Q_{â†’Ï†}(â†’z|â†’x)}\bigg]$

~~~
$\displaystyle \kern10em{} â‰¥ ğ”¼_{Q_{â†’Ï†}(â†’z|â†’x)} \bigg[\log P_{â†’Î¸}(â†’x | â†’z) + \log\frac{P(â†’z)}{Q_{â†’Ï†}(â†’z|â†’x)}\bigg]$

~~~
$\displaystyle \kern10em{} = ğ”¼_{Q_{â†’Ï†}(â†’z|â†’x)} \big[\log P_{â†’Î¸}(â†’x | â†’z)\big] - D_\textrm{KL}\big(Q_{â†’Ï†}(â†’z|â†’x) \| P(â†’z)\big).$

---
# VAE â€“ Variational (or Evidence) Lower Bound

The resulting **variational lower bound** or **evidence lower bound** (ELBO),
denoted $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$, can be also defined explicitly as:
$$ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = \log P_{â†’Î¸}(â†’x) - D_\textrm{KL}\big(Q_{â†’Ï†}(â†’z | â†’x) \| P_{â†’Î¸}(â†’z | â†’x)\big).$$

~~~
Because KL-divergence is nonnegative, $ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) â‰¤ \log P_{â†’Î¸}(â†’x).$

~~~
By using simple properties of conditional and joint probability, we get that

~~~
$\displaystyle \kern9em\mathllap{ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)} = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[\log P_{â†’Î¸}(â†’x) + \log P_{â†’Î¸}(â†’z | â†’x) - \log Q_{â†’Ï†}(â†’z | â†’x)\big]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[\log P_{â†’Î¸}(â†’x, â†’z) - \log Q_{â†’Ï†}(â†’z | â†’x)\big]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[\log P_{â†’Î¸}(â†’x | â†’z) + \log P(â†’z) - \log Q_{â†’Ï†}(â†’z | â†’x)\big]$

~~~
$\displaystyle \kern9em{} = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[\log P_{â†’Î¸}(â†’x | â†’z)\big] - D_\textrm{KL}\big(Q_{â†’Ï†}(â†’z | â†’x) \| P(â†’z)\big).$

---
# Variational AutoEncoders Training

$$-ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[-\log P_{â†’Î¸}(â†’x | â†’z)\big] + D_\textrm{KL}\big(Q_{â†’Ï†}(â†’z | â†’x) \| P(â†’z)\big)$$

- We train a VAE by minimizing the $-ğ“›(â†’Î¸, â†’Ï†;â‡â†’x)$.

~~~
- The $ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)}$ is estimated using a single sample.
~~~
- The distribution $Q_{â†’Ï†}(â†’z | â†’x)$ is parametrized as a normal distribution
  $ğ“(â†’z | â†’Î¼, â†’Ïƒ^2)$, with the model predicting $â†’Î¼$ and $â†’Ïƒ$ given $â†’x$.
~~~
  - In order for $â†’Ïƒ$ to be positive, we can use $\exp$ activation function
    (so that the network predicts $\log â†’Ïƒ$ before the activation), or for
    example a $\operatorname{softplus}$ activation function.
~~~
  - The normal distribution is used, because we can sample from it efficiently,
    we can backpropagate through it and we can compute $D_\textrm{KL}$
    analytically; furthermore, if we decide to parametrize $Q_{â†’Ï†}(â†’z | â†’x)$ using
    mean and variance, the maximum entropy principle suggests we should use the
    normal distribution.
~~~
- We use a prior $P(â†’z) = ğ“(â†’0, â†’I)$.

---
# Variational AutoEncoders Training

$$-ğ“›(â†’Î¸, â†’Ï†;â‡â†’x) = ğ”¼_{Q_{â†’Ï†}(â†’z | â†’x)} \big[-\log P_{â†’Î¸}(â†’x | â†’z)\big] + D_\textrm{KL}\big(Q_{â†’Ï†}(â†’z | â†’x) \| P(â†’z)\big)$$

![w=50%,h=center](vae_architecture.svgz)

Note that the loss has 2 intuitive components:
- **reconstruction loss** â€“ starting with $â†’x$, passing though $Q_{â†’Ï†}$, sampling
  $â†’z$ and then passing through $P_{â†’Î¸}$ should arrive back at $â†’x$;
~~~
- **latent loss** â€“ over all $â†’x$, the distribution of $Q_{â†’Ï†}(â†’z | â†’x)$ should be as close as
  possible to the prior $P(â†’z) = ğ“(â†’0, â†’I)$, which is independent on $â†’x$.

---
# Variational AutoEncoders â€“ Reparametrization Trick

In order to backpropagate through $â†’zâˆ¼Q_{â†’Ï†}(â†’z | â†’x)$, note that if
$$â†’z âˆ¼ ğ“(â†’Î¼, â†’Ïƒ^2),$$

~~~
we can write $â†’z$ as
$$â†’z âˆ¼ â†’Î¼ + â†’Ïƒ âŠ™ ğ“(â†’0, â†’I).$$

~~~
Such formulation then allows differentiating $â†’z$ with respect to
$â†’Î¼$ and $â†’Ïƒ$ and is called a **reparametrization trick** (Kingma and Welling, 2013).

---
# Variational AutoEncoders â€“ Reparametrization Trick

![w=100%,v=middle](reparametrization_trick.png)

---
# Variational AutoEncoders â€“ Reparametrization Trick

![w=100%,h=center](vae_architecture_reparametrized.svgz)

---
# Variational AutoEncoders

![w=80%,h=center](vae_manifold.svgz)

---
# Variational AutoEncoders

![w=100%,v=middle](vae_dimensionality.svgz)

---
# Variational AutoEncoders

![w=100%,v=middle](latent_space.png)

---
# Variational AutoEncoders â€“ Too High Latent Loss

![w=50%,h=center](vae_high_latent_loss.png)

---
# Variational AutoEncoders â€“ Too High Reconstruction Loss

![w=50%,h=center](vae_high_reconstruction_loss.png)
