class: title
## NPFL114, Lecture 12

# Sequence Prediction II,<br> Reinforcement Learning II

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
class: middle, center
# Structured Prediction

# Structured Prediction

---
# Conditional Random Fields (CRF)

## Motivation
- Label-bias problem

--

  - not a concrete definition
  - the inability of the model to assign individual weights to different
    sequence elements

--

- Global decoding

--

  - _ÄŒesko Slovensko porazilo_.

---
# Conditional Random Fields (CRF)

## Proposed Solution

- Performm sentence-level softmax, and add weights for neighboring sequence
  outputs.

--

$$s(â‡‰X, â†’y; â†’Î¸, â‡‰A) = âˆ‘\_{i=1}^N \big(â‡‰A\_{y\_{i-1}, y\_i} + f\_â†’Î¸(y\_i | â‡‰X)\big)$$

--

$$p(â†’y | â‡‰X) = \softmax\_{â†’z âˆˆ Y^N}\big(s(â‡‰X, â†’z)\big)\_{â†’z}$$

--

$$\log p(â†’y | â‡‰X) = s(â‡‰X, â†’y) - \operatorname{logadd}\_{â†’z âˆˆ Y^N}(s(â‡‰X, â†’z))$$

---
# Conditional Random Fields (CRF)

## Computation

We can compute $p(â†’y | â‡‰X)$ efficiently using dynamic programming. If we denote
$Î±\_t(k)$ as probability of all sentences with $t$ elements with the last $y$
being $k$.

--

The core idea is the following:

![:pdf 40%,center](crf_composability.pdf)

$$Î±\_t(k) = f\_â†’Î¸(y\_t=k | â‡‰X) + \operatorname{logadd}\_{jâˆˆY} (Î±\_{t-1}(j) + â‡‰A\_{j, k}).$$

---
# Conditional Random Fields (CRF)

## Computation

.algorithm[
**Inputs**: Network computing $f\_â†’Î¸(y\_t = k | â‡‰X)$, an unnormalized probability
of output sequence element probability being $k$ in time $t$.<br>
**Inputs**: Transition matrix $â‡‰A âˆˆ â„^{YÃ—Y}$.<br>
**Inputs**: Input sequence $â‡‰X$ of length $N$, gold labeling $â†’y âˆˆ Y^N$.<br>
**Outputs**: Value of $\log p(â†’y | â‡‰X)$.<br>
**Complexity**: $\mathcal O(N \cdot Y^2)$.

- For $k = 1, \ldots, Y$:
  - $Î±\_0(k) â† 0$
- For $t = 1, \ldots, N$:
  - For $k = 1, \ldots, Y:$
      - $Î±\_t(k) â† 0$
      - For $j = 1, \ldots, Y$:
          - $Î±\_t(k) â† \operatorname{logadd}(Î±\_t(k), Î±\_{t-1}(j) + â‡‰A\_{j, k})$
        - $Î±\_t(k) â† Î±\_t(k) + f\_â†’Î¸(y\_t=k | â‡‰X)$
]

---
# Conditional Random Fields (CRF)

## Decoding

We can perform optimal decoding, by using the same algorithm, only replacing
$\operatorname{logadd}$ with $\max$ and tracking where the maximum was attained.

---
# Connectionist Temporal Classification

Let us again consider generating a sequence of $y\_1, \ldots, y\_M$ given input
$â†’x\_1, \ldots, â†’x\_N$, but this time $M â‰¤ N$ and there is no explicit alignment
of $â†’x$ and $y$ in the gold data.

---
# Connectionist Temporal Classification

We enlarge the set of output labels by a â€“ (_blank_) and perform a classification for every
input element to produce an _extended labeling_. We then post-process it by the
following rules (denoted $\mathcal B$):
1. We remove neighboring symbols.
2. We remove the â€“.

--

Because the explicit alignment of inputs and labels is not known, we consider
_all possible_ alignments.

--

Denoting the probability of label $l$ at time $t$ as $p\_l^t$, we define
$$Î±^t(s) â‰ âˆ‘\_{\textrm{labeling }â†’Ï€: \mathcal B(â†’Ï€\_{1:t}) = â†’y\_{1:s}} âˆ\_{t'=1}^t p\_{â†’Ï€\_{t'}}^{t'}.$$

---
# CRF and CTC Comparison

In CRF, we normalize the whole sentences, therefore we need to compute
unnormalized probabilities for all the (exponentially many) sentences. Decoding
can be performed optimally.

--

In CTC, we normalize per each label. However, because we do not have explicit
alignment, we compute probability of a labeling by summing probabilities
of (generally exponentially many) extended labelings.

---
# Connectionist Temporal Classification

## Computation

When aligning an extended labeling to a regular one, we need to consider
whether the extended labeling ends by a _blank_ or not. We therefore define
$$\begin{aligned}
  Î±\_-^t(s) &â‰ âˆ‘\_{\textrm{labeling }â†’Ï€: \mathcal B(â†’Ï€\_{1:t}) = â†’y\_{1:s}, Ï€\_t=-} âˆ\_{t'=1}^t p\_{â†’Ï€\_{t'}}^{t'} \\\
  Î±\_\*^t(s) &â‰ âˆ‘\_{\textrm{labeling }â†’Ï€: \mathcal B(â†’Ï€\_{1:t}) = â†’y\_{1:s}, Ï€\_tâ‰ -} âˆ\_{t'=1}^t p\_{â†’Ï€\_{t'}}^{t'}
\end{aligned}$$
and compute $Î±^t(s)$ as $Î±\_-^t(s) + Î±\_\*^t(s)$.

---
# Connectionist Temporal Classification

## Computation

We initialize $Î±$s as follows:
- $Î±\_-^1(0) â† p\_-^1$
- $Î±\_\*^1(1) â† p\_{y\_1}^1$

--

We then proceed recurrently according to:
- $Î±\_-^t(s) â† p\_-^t (Î±\_-^{t-1}(s) + Î±\_\*^{t-1}(s))$
- $Î±\_\*^t(s) â† \begin{cases}
  p\_{y\_s}^t(Î±\_\*^{t-1}(s) + Î±\_\*^{t-1}(s-1) + a\_-^{t-1}(s-1))\textrm{, if }y\_sâ‰ y\_{s-1}\\\
  p\_{y\_s}^t(Î±\_\*^{t-1}(s) + a\_-^{t-1}(s-1))\textrm{, if }y\_s=y\_{s-1}\\\
\end{cases}$

---
# CTC Decoding

Unlike CRF, we cannot perform the decoding optimally. The key
observation is that while an optimal extended labeling can be extended
into an optimal labeling of a larger length, the same does not apply to
regular (non-extended) labeling. The problem is that regular labeling coresponds
to many extended labelings, which are modified each in a different way
during an extension of the regular labeling.

![:pdf 90%,center](../11/ctc_decoding.pdf)
---
# CTC Decoding

## Beam Search

To perform beam search, we keep $k$ best regular labelings for each prefix of
the extended labelings. For each regular labeling we keep both $Î±\_-$ and
$a\_\*$ and by _best_ we mean such regular labelings with maximum $Î±\_- + Î±\_\*$.

To compute best regular labelings for longer prefix of extended labelings,
for each regular labeling in the beam we consider the following cases:
- adding a _blank_ symbol, i.e., updating both $Î±\_-$ and $Î±\_\*$;
- adding any non-blank symbol, i.e., updating $Î±\_\*$.

Then, we merge the resulting candidates according to their regular labeling and
keep only the $k$ best.


---
class: middle, center
# Reinforcement Learning

# Reinforcement Learning

---
# Reinforcement Learning

![:pdf 85%, center](../11/diagram.pdf)

--

A _Markov decision process_ is a quadruple $(\mathcal S, \mathcal A, P, Î³)$,
where:
- $\mathcal S$ is a set of states,
- $\mathcal A$ is a set of actions,
- $P(S\_{t+1} = s', R\_{t+1} = r | S\_t = s, A\_t = a)$ is a probability that
  action $a âˆˆ \mathcal A$ will lead from state $s âˆˆ \mathcal S$ to $s'
  âˆˆ \mathcal S$, producing a _reward_ $r âˆˆ â„$,
- $Î³ âˆˆ [0, 1]$ is a _discount factor_.

--

Let a _return_ $G\_t$ be $G\_t â‰ âˆ‘\_{k=0}^\infty Î³^k R\_{t + 1 + k}$.

---
# Policies

A _policy_ $Ï€$ computes a distribution of actions in a given state, i.e.,<br>
$Ï€(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

--

## Value Function
To evaluate a quality of policy, we define _value function_ $v\_Ï€(s)$, or more
explicitly _state-value function_, as
$$v\_Ï€(s) â‰ ğ”¼\_Ï€[G\_t | S\_t = s].$$

--
An _action-value function_ for policy $Ï€$ is defined analogously as
$$q\_Ï€(s, a) â‰ ğ”¼\_Ï€[G\_t | S\_t = s, A\_t = a].$$

--
It follows that
$$q\_Ï€(s, a) = ğ”¼\_Ï€[R\_{t+1} + Î³v\_Ï€(S_{t+1}) | S\_t = s, A\_t = a].$$

---
# Optimal Policy

As value functions define an partial ordering of policies ($Ï€' â‰¥ Ï€$ if
and only if for all states $s$, $v\_{Ï€'}(s) â‰¥ v\_Ï€(s)$), it can be proven
that there always exists an _optimal policy_ $Ï€\_\*$, which is better or equal
to all other policies.

Intuitively, $Ï€\_\*(s) = \argmax\nolimits\_a q\_\*(s, a)$.

--
## Policy Improvement Theorem

Let $Ï€$ and $Ï€'$ be any pair of policies (both deterministic or stochastic), such that
$q\_Ï€(s, Ï€'(s)) â‰¥ v\_Ï€(s)$. Then $Ï€' â‰¥ Ï€$, i.e., for all states $s$, $v\_{Ï€'}(s) â‰¥ v\_Ï€(s)$.

---
class: middle
# Monte Carlo Control

![:pdf 100%](../11/monte_carlo.pdf)

---
# Temporal Difference Methods

![:pdf 100%](../11/td_example.pdf)

--

![:pdf 100%](../11/td_example_update.pdf)

---
# Temporal Difference Methods

A straightforward modification of Monte Carlo algorithm with constant-step
update and temporal difference is given by
$$Q(S\_t, A\_T) â† Q(S\_t, A\_t) + Î±[R\_{t+1} + Î³ Q(S\_{t+1}, A\_{t+1}) -Q(S\_t, A\_t)]$$
and is called _Sarsa_ ($S\_t, A\_t, R\_{t+1}, S\_{t+1}, A\_{t+1}$).

--
![:pdf 100%](../11/sarsa.pdf)

---
# Q-learning

_Q-learning_ is another TD control algorithm by (Watkins, 1989), defined by
$$Q(S\_t, A\_T) â† Q(S\_t, A\_t) + Î±[R\_{t+1} + Î³ \max\_a Q(S\_{t+1}, a) -Q(S\_t, A\_t)].$$

--
![:pdf 100%](../11/q_learning.pdf)

---
class: center
# Sarsa vs Q-learning

![:pdf 65%](../11/sarsa_vs_q.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](../11/double_q_example.pdf)

---
class: middle
# Double Q-learning

![:pdf 100%](../11/double_q.pdf)

---
# Bridging Q-learning and Monte Carlo

Monte Carlo uses whole episode returns, while Q-learning uses single-step
rewards. We can connect these approaches by considering n-step returns:
$$\sum\_{k=1}^n Î³^{k-1} R\_{t+k}.$$
We can then approximate full returns as
$$G\_t â‰ˆ \sum\_{k=1}^n (Î³^{k-1} R\_{t+k}) + v\_Ï€(S\_{t+n}).$$

---
class: middle
# Deep Q Networks

![:image 100%](atari.png)

---
# Deep Q Networks

No proofs of convergence; the training can be extremely brittle. Several
improvements to increase stability of the training:

--

- experience replay,

--

- separate target network $\hat Q$,

--

- clipping $R\_{t+1} + Î³ \max\_a Q(S\_{t+1}, a) - Q(S\_t, A\_t)$ to $[-1, 1]$.

---
# Policy Gradient Methods

The main idea of _policy gradient method_ is to train the policy itself, instead
of basing it on action-value function $q$.

However, for that we need to be able to compute a derivation of state-value
function, i.e., $âˆ‡v\_Ï€(s)$.

Hopefully, a _policy gradient theorem_ comes to the rescue.

--

## Policy Gradient Theorem
Let $Ï€$ be a given policy. We denote the on-policy distribution under $Ï€$ as $Î¼(s)$.
Then
$$âˆ‡v\_Ï€(s) âˆ \sum\_{sâˆˆ\mathcal S} Î¼(s) \sum\_{aâˆˆ\mathcal A} q\_Ï€(s, a) âˆ‡ Ï€(a|s; â†’Î¸).$$

---
# Policy Gradient Theorem

![:pdf 90%,center](policy_gradient_theorem.pdf)

Finally, we obtain the required form by dividing the result by an average length
of an episode.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, approximating the expectation by a single sample.

![:pdf 100%](reinforce.pdf)

---
# REINFORCE with baseline Algorithm

The gradient estimation used in REINFORCE has high variance.

However, we can decrease the variance by considering a _baseline_ $b(s)$, which
as an arbitrary function not depending on action $a$, using the following
generalization of policy gradient theorem:
$$âˆ‡v\_Ï€(s) âˆ \sum\_{sâˆˆ\mathcal S} Î¼(s) \sum\_{aâˆˆ\mathcal A} (q\_Ï€(s, a) \boldsymbol{- b(s)}) âˆ‡ Ï€(a|s; â†’Î¸).$$

The introduction of the baseline is possible, because
$$\sum\_{aâˆˆ\mathcal A} b(s) âˆ‡ Ï€(a|s; â†’Î¸) = b(s) âˆ‡ \sum\_{aâˆˆ\mathcal A} Ï€(a|s; â†’Î¸) = b(s) âˆ‡1 = 0.$$
$$

---
# REINFORCE with baseline Algorithm

![:pdf 100%](reinforce_with_baseline.pdf)

---
# Actor Critic

A combination of Q-learning and REINFORCE is also possible and called Actor
Critic algorithm.

![:pdf 100%](actor_critic.pdf)
